<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Neural-Network Implementation</title>
      <link href="2021/08/24/Neural-Network-Implementation/"/>
      <url>2021/08/24/Neural-Network-Implementation/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#Initialize parameters for deep neural network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Argument:</span></span><br><span class="line"><span class="string">  layer_dims -- python array (list) containing the dimensions of each layer in network</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">  parameters -- python dictionary containing parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span></span><br><span class="line"><span class="string">  Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">  bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  parameters = &#123;&#125;</span><br><span class="line">  L = <span class="built_in">len</span>(layer_dims) <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">      parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l-<span class="number">1</span>]) * <span class="number">0.01</span></span><br><span class="line">      parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">      <span class="keyword">assert</span>(parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], layer_dims[l - <span class="number">1</span>]))</span><br><span class="line">      <span class="keyword">assert</span>(parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment">#Implement the linear part of a layer&#x27;s forward propagation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span>(<span class="params">A, W, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></span><br><span class="line"><span class="string">    cache -- a python tuple containing &quot;A&quot;, &quot;W&quot; and &quot;b&quot; ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span>(<span class="params">A_prev, W, b, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python tuple containing &quot;linear_cache&quot; and &quot;activation_cache&quot;;</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        </span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment">#Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="comment">#For other computation, we can change the for loop</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- activation value from the output (last) layer</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</span></span><br><span class="line">    <span class="comment"># The for loop starts at 1 because layer 0 is the input</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l)], parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l)], <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(L)], parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(L)], <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">          </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line">  </span><br><span class="line"><span class="comment">#Implement the cost function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">AL, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true &quot;label&quot; vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    cost = -<span class="number">1</span>/m*(np.dot(Y, np.log(AL).T) + np.dot(<span class="number">1</span>-Y, np.log(<span class="number">1</span>-AL).T))</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost&#x27;s shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line">  </span><br><span class="line"><span class="comment">#Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span>(<span class="params">dZ, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    dW = <span class="number">1</span>/m*np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m*np.<span class="built_in">sum</span>(dZ, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line">  </span><br><span class="line"><span class="comment">#Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span>(<span class="params">dA, cache, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"></span><br><span class="line"><span class="comment">#Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span>(<span class="params">AL, Y, caches</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with &quot;relu&quot; (it&#x27;s caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with &quot;sigmoid&quot; (it&#x27;s caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads[&quot;dA&quot; + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads[&quot;db&quot; + str(l)] = ... </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;dAL, current_cache&quot;. Outputs: &quot;grads[&quot;dAL-1&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span></span><br><span class="line">    current_cache = caches[L-<span class="number">1</span>]</span><br><span class="line">    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L-<span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">    grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)] = dW_temp</span><br><span class="line">    grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = db_temp</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop from l=L-2 to l=0</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L-<span class="number">1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: &quot;grads[&quot;dA&quot; + str(l + 1)], current_cache&quot;. Outputs: &quot;grads[&quot;dA&quot; + str(l)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line">  </span><br><span class="line"><span class="comment">#Update parameters using gradient descent</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span>(<span class="params">params, grads, learning_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters[&quot;W&quot; + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters[&quot;b&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    parameters = params.copy()</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] = parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] = parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span>(<span class="params">X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parameters initialization.</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Print the cost every 100 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> i == num_iterations - <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">&quot;Cost after iteration &#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> i == num_iterations:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters, costs</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度神经网络 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Advice for applying machine learning</title>
      <link href="2021/06/29/Advice-for-applying-machine-learning/"/>
      <url>2021/06/29/Advice-for-applying-machine-learning/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Evaluating-a-Learning-Algorithm"><a href="#1-Evaluating-a-Learning-Algorithm" class="headerlink" title="1. Evaluating a Learning Algorithm"></a>1. Evaluating a Learning Algorithm</h4><h5 id="1-1-Evaluating-a-Hypothesis"><a href="#1-1-Evaluating-a-Hypothesis" class="headerlink" title="1.1 Evaluating a Hypothesis"></a>1.1 Evaluating a Hypothesis</h5><ul><li><p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a <strong>training set</strong> and a <strong>test set</strong>. Typically, the training set consists of $70%$ of data and the test set is the remaining $30%$.</p></li><li><p>The new procedure using these two sets is then:</p><ul><li>Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set</li><li>Compute the test set error $J_{test}(\Theta)$</li></ul></li><li><p><strong>The test set error</strong></p><ul><li><p>For linear regression: $J_{test}(\Theta) = \frac{1}{2m_{test}}\Sigma_{i=1}^{m_{test}}(h_\Theta(x_{test}^{(i)})-y_{test}^{(i)})^2$</p></li><li><p>For classification ~ Misclassification error (aka $0/1$ misclassification error):<br>$$<br>err(h_\Theta(x), y) = \begin{cases}1\quad \text{if } h_\Theta \ge 0.5 \text{ and } y=0 \text{ or } h_\Theta \lt 0.5 \text{ and } y=1\ 0\quad \text{otherwise}\end{cases}<br>$$</p><p>$$<br>\text{Test Error} = \frac{1}{m_{test}}\Sigma_{i=1}^{m_{test}}err(h_\Theta(x_{test}^{(i)}), y_{test}^{(i)})<br>$$</p></li></ul></li></ul><h5 id="1-2-Model-Selection-and-Train-Validation-Test-Sets"><a href="#1-2-Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="1.2 Model Selection and Train/Validation/Test Sets"></a>1.2 Model Selection and Train/Validation/Test Sets</h5><ul><li><p>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could overfit and as a result the predication on the test set would be poor. The error of the hypothesis as measured on the dataset with which you trained the parameters will be lower than the error on any other data set.</p></li><li><p>Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of the hypothesis, test each degree of polynomial and look at the error result.</p></li><li><p>One way to break down the dataset into the three sets is:</p><ul><li>Training set: $60%$</li><li>Cross validation set: $20%$</li><li>Test set: $20%$</li></ul></li><li><p>Calculate three separate error values for the three different sets using the following method:</p><ol><li>Optimize the parameters in $\Theta$ using the training set for each polynomial degree.</li><li>Find the polynomial degree $d$ with the least error using the cross validation set.</li><li>Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);</li></ol></li><li><p>This way, the degree of the polynomial $d$ has not been trained using the test set.</p></li></ul><h4 id="2-Bias-vs-Variance"><a href="#2-Bias-vs-Variance" class="headerlink" title="2. Bias vs. Variance"></a>2. Bias vs. Variance</h4><h5 id="2-1-Diagnosing-Bias-vs-Variance"><a href="#2-1-Diagnosing-Bias-vs-Variance" class="headerlink" title="2.1 Diagnosing Bias vs. Variance"></a>2.1 Diagnosing Bias vs. Variance</h5><ul><li><p>Need to distinguish whether <strong>bias</strong> or <strong>variance</strong> is the problem contributing to bad predictions.</p></li><li><p>High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.</p></li><li><p>The training error will tend to <strong>decrease</strong> as we increase the degree $d$ of the polynomial.</p><p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase $d$ up to a point, and then it will <strong>increase</strong> as $d$ is increased, forming a convex curve.</p></li><li><p><strong>High bias (underfitting)</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$</p></li><li><p><strong>High variance (overfitting)</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$</p></li><li><p>Summurized in the figure below:</p><img src="/2021/06/29/Advice-for-applying-machine-learning/BiasAndVariance.png" style="zoom:50%;"></li></ul><h5 id="2-2-Regularization-and-Bias-Variance"><a href="#2-2-Regularization-and-Bias-Variance" class="headerlink" title="2.2 Regularization and Bias/Variance"></a>2.2 Regularization and Bias/Variance</h5><ul><li><img src="/2021/06/29/Advice-for-applying-machine-learning/RegularizationAndBias:Variance.png" style="zoom:50%;"></li><li><p>In the figure above, as $\lambda$ increases, the fit becomes more rigid. On the other hand, as $\lambda$ approaches $0$, the model tends to overfit the data.</p></li><li><p>In order to choose the model and the regularization term $\lambda$, need to:</p><ol><li>Create a list of lambdas (i.e. $\lambda \in {0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24}$);</li><li>Create a set of models with different degrees or any other variants.</li><li>Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$.</li><li>Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ <strong>without</strong> regularization or $\lambda = 0$.</li><li>Select the best combo that produces the lowest error on the cross validation set.</li><li>Using the best combo $\Theta$ and $\lambda$, apply in on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.</li></ol></li></ul><h5 id="2-3-Learning-Curves"><a href="#2-3-Learning-Curves" class="headerlink" title="2.3 Learning Curves"></a>2.3 Learning Curves</h5><ul><li>Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence<ul><li>As the training set gets larger, the error for a quadratic function increases.</li><li>The error value will plateau out after a certain $m$, or training set size.</li></ul></li><li><strong>Experiencing high bias</strong>:<ul><li><strong>Low training set size</strong>: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.</li><li><strong>Large traning set size</strong>: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta) \approx J_{CV}(\Theta)$.</li><li>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data will not <strong>(by itself)</strong> help much.</li></ul></li><li><strong>Experiencing high variance</strong>:<ul><li><strong>Low training set size</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.</li><li><strong>Large training set size</strong>: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) \lt J_{CV}(\Theta)$ but the difference between them remains significant.</li><li>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is likely to help.</li></ul></li></ul><h5 id="2-4-Deciding-what-to-do-next-revisited"><a href="#2-4-Deciding-what-to-do-next-revisited" class="headerlink" title="2.4 Deciding what to do next revisited"></a>2.4 Deciding what to do next revisited</h5><ul><li>The decision process can be broken down as follows:<ul><li><strong>Getting more training examples</strong>: Fixes high variance</li><li><strong>Trying smaller sets of features</strong>: Fixes high variance</li><li><strong>Adding features</strong>: Fixes high bias</li><li><strong>Adding polynomial features</strong>: Fixes high bias</li><li><strong>Decreasing $\lambda$</strong>: Fixes high bias</li><li><strong>Increasing $\lambda$</strong>: Fixes high variance</li></ul></li><li><strong>Diagnosing Neural Networks</strong><ul><li>A neural network with fewer parameters is <strong>prone to underfitting</strong>. It is also <strong>Computationally cheaper</strong>.</li><li>A large neural network with more parameters is <strong>prone to overfitting</strong>. It is also <strong>computationally expensive</strong>. In this case, use regularization (increase $\lambda$) to address overfitting.</li><li>Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using cross validation set. Then select the one that performs best.</li></ul></li><li><strong>Model Complexity Effects</strong>:<ul><li>Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</li><li>Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.</li><li>In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> self learning notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> bias/variance </tag>
            
            <tag> cross validation </tag>
            
            <tag> learning curves </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="2021/06/22/Support-Vector-Machine/"/>
      <url>2021/06/22/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Optimization-Objective"><a href="#1-Optimization-Objective" class="headerlink" title="1. Optimization Objective"></a>1. Optimization Objective</h4><ul><li><p>$$<br>\min_\theta C \sum_{i=1}^m\left[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})\right] + \frac{1}{2}\sum_{i=1}^n\theta_J^2<br>$$</p></li><li><p>$z = \theta^Tx$</p></li><li><p><img src="/2021/06/22/Support-Vector-Machine/cost1.png" style="zoom:50%;"> <img src="/2021/06/22/Support-Vector-Machine/cost0.png" style="zoom:50%;"></p></li><li><p>if $y = 1$, $\theta^Tx \ge 1$</p><p>if $y=0$, $\theta^Tx\le -1$</p></li></ul><h4 id="2-Decision-Boundary"><a href="#2-Decision-Boundary" class="headerlink" title="2. Decision Boundary"></a>2. Decision Boundary</h4><ul><li><p>$$<br>\begin{align*}<br>&amp; \min_\theta \frac{1}{2}\sum_{i=1}^n\theta_j^2\<br>&amp; s.t.\quad \theta^Tx^{(i)} \ge 1\quad \text{if}\ y^{(i)} = 1\<br>&amp; \quad \quad\ \ \ \theta^Tx^{(i)} \le -1\quad \text{if}\ y^{(i)} = 0<br>\end{align*}<br>$$</p></li><li><p>Linearly Separable case</p><img src="/2021/06/22/Support-Vector-Machine/LinearSeparable.png" style="zoom:50%;"></li><li><img src="/2021/06/22/Support-Vector-Machine/LinearSeparable1.png" style="zoom:50%;"></li></ul><h4 id="3-Kernel"><a href="#3-Kernel" class="headerlink" title="3. Kernel"></a>3. Kernel</h4><ul><li><p>Given $x$, compute new feature depending on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)}, \cdots$</p></li><li><p>$f_1 = similarity(x, l^{(1)}) = \exp\left(-\frac{\Vert x-l^{(1)}\Vert^2}{2\sigma^2}\right)$</p><p>if $x \approx l^{(1)}: f_1 \approx \exp\left(-\frac{0^2}{2\sigma^2}\right) \approx 1$</p><p>If $x$ is far from $l^{(1)}: f_1 = \exp\left(-\frac{(\text{large number})^2}{2\sigma^2}\right) \approx 0$</p></li><li><p><strong>SVM with Kernels</strong></p><ul><li><p>Given $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})$</p></li><li><p>Choose $l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, \cdots, l^{(m)} = x^{(m)}$</p></li><li><p>Given example $x$:</p><ul><li>$f_1 = similarity(x, l^{(1)})$</li><li>$f_2 = similarity(x, l^{(2)})$</li><li>$\cdots$</li></ul></li><li><p>For training example $(x^{(i)}, y^{(i)})$</p><ul><li>$x^{(i)}\rightarrow {f_1^{(i)} = sim(x^{(i)}, l^{(1)})\ \vdots \ f_m^{(i)} = sim(x^{(i)}), l^{(m)}}$</li><li>$f^{(i)} = x^{(i)}\f_0^{(i)} = 1$</li></ul></li><li><p>Hypothesis: Given $x$, compute features $f\in R^{m+1}$</p><ul><li><p>Predict “$y=1$” if $\theta^Tf \ge 0$</p></li><li><p>Training<br>$$<br>\min_\theta C \sum_{i=1}^m\left[y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)})\right] + \frac{1}{2}\sum_{i=1}^n\theta_J^2<br>$$</p></li></ul></li></ul></li></ul><h4 id="4-SVM-parameters"><a href="#4-SVM-parameters" class="headerlink" title="4. SVM parameters"></a>4. SVM parameters</h4><ul><li>$C(=\frac{1}{\lambda})$<ul><li>Large $C$: Lower bias, high variance.</li><li>Small $C$: Higher bias, low variance.</li></ul></li><li>$\sigma^2$<ul><li>Large $\sigma^2$: Features $f_i$ vary more smoothly. Higher bias, lower variance.</li><li>Small $\sigma^2$: Features $f_i$ vary less smoothly. Lower bias, higher variance.</li></ul></li></ul><h4 id="5-Logistic-regression-vs-SVMs"><a href="#5-Logistic-regression-vs-SVMs" class="headerlink" title="5. Logistic regression vs. SVMs"></a>5. Logistic regression vs. SVMs</h4><p>$n = \text{number of features }(x\in R^{n+1}), m = \text{number of training examples}$</p><ul><li>If $n$ is large (relative to $m$):<ul><li>Use logistic regression, or SVM without a kernel (“linear kernel”)</li></ul></li><li>If $n$ is small, $m$ is intermediate:<ul><li>Use SVM with Gaussian kernel</li></ul></li><li>If $n$ is small, $m$ is large:<ul><li>Create/add more features, then use logistic regression or SVM without a kernel</li></ul></li><li>Neural network likely to work well for most of these settings, but may be slower to train.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 支持向量机 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自学笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network</title>
      <link href="2021/06/16/Neural-Network/"/>
      <url>2021/06/16/Neural-Network/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Model-Representation"><a href="#1-Model-Representation" class="headerlink" title="1. Model Representation"></a>1. Model Representation</h4><ul><li><p>Input: features $x_1\cdots x_n$</p></li><li><p>Output: the result of the hypothesis function.</p></li><li><img src="/2021/06/16/Neural-Network/Neural Network Rep1.png" style="zoom:50%;"></li><li><p>$x_0$ input node is called the “bias unit”. It is always equal to $1$.</p></li><li><p>The same logistic function as in classification, $\frac{1}{1+e^{-\theta^Tx}}$, which is also called sigmoid (logistic) <strong>activation</strong> function.</p></li><li><p>“theta” parameters are called “weights”.</p></li><li><p>A simplistic representation:<br>$$<br>[x_0x_1x_2]\rightarrow [\ ]\rightarrow h_\theta(x)<br>$$</p></li><li><p>Inpur nodes (layer 1), known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p></li><li><p>The intermediate layers of nodes between the input and output layers called the “hidden layers”.</p></li><li><img src="/2021/06/16/Neural-Network/Neural Network Rep2.png" style="zoom:50%;"></li><li><p>We label these intermediate or “hidden” layer nodes $a_0^2\cdots a_n^2$ and call them “activation units”.</p></li><li><p>$$<br>\begin{align*}<br>&amp; a_i^{(j)} = \text{activation}\ \text{of unit}\ i\ \text{in layer}\ j\<br>&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer}\ j \text{ to layer}\ j+1<br>\end{align*}<br>$$</p></li><li><p>$$<br>[x_0x_1x_2x_3]\rightarrow \left[a_1^{(2)}a_2^{(2)}a_3^{(2)}\right]\rightarrow h_\theta(x)<br>$$</p></li><li><p>The values for each of the “activation” nodes is obtained as follows:<br>$$<br>\begin{align*}<br>&amp;&amp; a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_2 + \Theta_{12}^{(1)}x_1 + \Theta_{13}^{(1)}x_3)\<br>&amp;&amp; a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_2 + \Theta_{22}^{(1)}x_1 + \Theta_{23}^{(1)}x_3)\<br>&amp;&amp; a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_2 + \Theta_{32}^{(1)}x_1 + \Theta_{33}^{(1)}x_3)\<br>&amp;&amp; h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})<br>\end{align*}<br>$$</p></li><li><p>The dimensions of matrices of weights is determined as follows:</p><p><strong>If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1}\times (s_j+1)$</strong>.</p><p>The $+1$ comes from the addition in $\Theta^{(j)}$ of the “bias nodes”, $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will.</p></li></ul><h4 id="2-Vectorized-Representation"><a href="#2-Vectorized-Representation" class="headerlink" title="2. Vectorized Representation"></a>2. Vectorized Representation</h4><ul><li><p>Setting $x = a^{(1)}$</p><p>$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$</p></li><li><p>Multiply matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$ (where $s_j$ is the number of activation nodes) by vector $a^{(j-1)}$ with height $n+1$. This gives vector $z^{(j)}$ with height $s_j$. Now the vector of activation nodes for layer $j$ as follows:<br>$$<br>a^{(j)} = g(z^{(j)})<br>$$<br>where function $g$ can be applied element-wise to vector $z^{(j)}$</p></li><li><p>Add a bias unit (equal to $1$) to layer $j$ after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to $1$.</p></li><li><p>Final result $h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})$</p></li></ul><h4 id="3-Cost-Function"><a href="#3-Cost-Function" class="headerlink" title="3. Cost Function"></a>3. Cost Function</h4><ul><li><p>First define a few variables need to use:</p><ul><li>$L = \text{total number of layers in the network}$</li><li>$s_l = \text{number of units (not counting bias unit) in layer } l$</li><li>$K = \text{number of output units/classes}$</li></ul></li><li><p>In neural networks, we may have many output nodes. Denote $h_\theta(x)_k$ as being a hypothesis that results in the $k^{th}$ output.</p></li><li><p>The cost function for neural networks is going to be generalization of the one used for logistic regression.<br>$$<br>J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k-1}^{K} \left[y_{k}^{(i)} \log((h_\Theta(x^{(i)}))<em>k) + (1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))<em>k)\right] + \frac{\lambda}{2m} \sum</em>{l=1}^{L-1} \sum</em>{i=1}^{s_l} \sum_{j=1}^{s_l+1}(\Theta_{j, i}^{(i)})^2<br>$$<br>We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.</p></li><li><p>In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.</p></li><li><p>Note:</p><ul><li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer.</li><li>the triple sum simply adds up the squares of all the individual $\Theta$s in the entire network.</li><li>the $i$ in the triple sum does <strong>not</strong> refer to training example $i$.</li></ul></li></ul><h4 id="4-Backpropagation-Algorithm"><a href="#4-Backpropagation-Algorithm" class="headerlink" title="4. Backpropagation Algorithm"></a>4. Backpropagation Algorithm</h4><ul><li><p>“Backpropagation” is neural-network terminology for minimizing the cost function.</p></li><li><p>Compute the partial derivative of $J(\Theta)$:<br>$$<br>\frac{\partial}{\partial \Theta_{i, j}^{(l)}}J(\Theta)<br>$$</p></li><li><p>Procedure:</p><ul><li>Given training examples set ${(x^{(1)}, y^{(1)})\cdots(x^{(m)}, y^{(m)})}$</li><li>Set $\Delta_{ij}^{(l)} = 0$ for all $(l, i, j)$</li><li>For $i = 1$ to $m$:<ul><li>Set $a^{(1)} = x^{(i)}$</li><li>Perform forward propagation to compute $a^{(l)}$ for $l = 2,3,\cdots, L$</li><li>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$</li><li>Compute $\delta^{(L-1)}, \delta^{(L-2)}, \cdots, \delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T\delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1-a^{(l)})$<ul><li>The delta values of layer $l$ are calculated by multiplying the delta values in the next layer with the theta matrix of layer $l$. We then element-wise multiply that with a function called $g^{\prime}$, or g-prime, which is the derivative of the activation function $g$ evaluated with the input values given by $z^{(l)}$.</li></ul></li><li>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$</li></ul></li><li>$D_{ij}^{(l)} := \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)} \text{ if } j \ne 0$</li><li>$D_{ij}^{(l)} := \Delta_{ij}^{(l)} \text{ if } j = 0$</li></ul></li><li><p>Intuitively, $\delta_j^{(l)}$ is the “error” for $a_j^{(l)}$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function: $\delta_j^{(l)} = \frac{\partial}{\partial z_j^{(l)}}cost(t)$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 未完待续 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Overfitting</title>
      <link href="2021/06/09/Overfitting/"/>
      <url>2021/06/09/Overfitting/</url>
      
        <content type="html"><![CDATA[<h4 id="1-The-Problem-of-Overfitting"><a href="#1-The-Problem-of-Overfitting" class="headerlink" title="1. The Problem of Overfitting"></a>1. The Problem of Overfitting</h4><ul><li><strong>Underfitting</strong>, or high bias, is when the form of our hypothesis function $h$ maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.</li><li>At the other extreme, <strong>overfitting</strong>, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</li><li>Two main options to address the issue of overfitting:<ul><li>Reduce the number of features:<ul><li>Manually select which features to keep.</li><li>Use a model selection algorithm.</li></ul></li><li>Regularization<ul><li>Keep all the features, but reduce the magnitude of parameters $\theta_j$.</li><li>Regularization works well when we have a lot of slightly useful features.</li></ul></li></ul></li></ul><h4 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h4><ul><li><p>If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.</p></li><li><p>$$<br>\min_\theta\frac{1}{2m}\Sigma_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \Sigma_{j=1}^n \theta_j^2<br>$$</p></li><li><p>The $\lambda$, or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.</p></li><li><p>Using the above cost function with extra summation, we can smooth the output of our hypothesis function to reduce overfitting.</p></li><li><p>If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p></li></ul><h4 id="3-Regularized-Linear-Regression"><a href="#3-Regularized-Linear-Regression" class="headerlink" title="3. Regularized Linear Regression"></a>3. Regularized Linear Regression</h4><ul><li><p><strong>Gradient Descent</strong><br>$$<br>\begin{align*}<br>&amp; Repeat\ {\<br>&amp; \quad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\<br>&amp; \quad\theta_j := \theta_j - \alpha\left[ \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\right) + \frac{\lambda}{m}\theta_j\right] \quad\quad j\in{1, 2\cdots n}\<br>&amp; }<br>\end{align*}<br>$$</p></li><li><p>The term $\frac{\lambda}{m}\theta_j$ performs regularization. With some manipulation, the update rule can also be represented as:<br>$$<br>\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\Sigma_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$<br>The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than $1$. Intuitively reduce the value of $\theta_j$ by some amount on every update.</p></li><li><p><strong>Normal Equation</strong><br>$$<br>\theta = (X^TX+\lambda L)^{-1}X^Ty\quad<br>where\ L = \begin{bmatrix} 0 \ \ &amp; 1 \ \ &amp;\ &amp; 1 \ \ &amp; \ &amp; \ &amp; \ddots \ \ &amp; \ &amp; \ &amp;\ &amp; 1\end{bmatrix}<br>$$<br>$L$ is a matrix with $0$ at the top left and $1$’s down the diagonal, with $0$’s everywhere else. It should have dimension $(n+1)\times (n+1)$. Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number $\lambda$.</p></li><li><p>When we add the term $\lambda L$, then $X^TX + \lambda L$ becomes invertible.</p></li></ul><h4 id="4-Regularized-Logistic-Regression"><a href="#4-Regularized-Logistic-Regression" class="headerlink" title="4. Regularized Logistic Regression"></a>4. Regularized Logistic Regression</h4><ul><li><p>Cost function<br>$$<br>J(\theta) = -\frac{1}{m}\Sigma_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i )}))] + \frac{\lambda}{m}\Sigma_{j=1}^n\theta_j^2<br>$$<br>The second sum, $\Sigma_{j=1}^n\theta_j^2$ <strong>means to explicitly exclude</strong> the bias term $\theta_0$.</p></li><li><p><strong>Gradient Descent</strong><br>$$<br>\begin{align*}<br>&amp; Repeat\ {\<br>&amp; \quad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\<br>&amp; \quad\theta_j := \theta_j - \alpha\left[ \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\right) + \frac{\lambda}{m}\theta_j\right] \quad\quad j\in{1, 2\cdots n}\<br>&amp; }<br>\end{align*}<br>$$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 分类问题 </tag>
            
            <tag> 过拟合 </tag>
            
            <tag> 回归问题 </tag>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression -- Classification</title>
      <link href="2021/06/08/Logistic-Regression-Classification/"/>
      <url>2021/06/08/Logistic-Regression-Classification/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Classification"><a href="#1-Classification" class="headerlink" title="1. Classification"></a>1. Classification</h4><ul><li>The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.</li><li><strong>Binary classification problem</strong><ul><li>$y$ can take on only two values, $0$ and $1$. (Multi-class later)</li><li>$0$ is called the negative class, and $1$ the positive class, and they are sometimes also denoted by the symbols “-“ and “+”.</li><li>Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the <strong>label</strong> for the training example.</li></ul></li></ul><h4 id="2-Hypothesis-Representation"><a href="#2-Hypothesis-Representation" class="headerlink" title="2. Hypothesis Representation"></a>2. Hypothesis Representation</h4><ul><li><p>By using the “Sigmoid Function”, also called the “Logistic Function”:<br>$$<br>\begin{align*}<br>&amp; h_{\theta}(x) = g(\theta^Tx)\<br>&amp; z = \theta^Tx\<br>&amp; g(z) = \frac{1}{1+e^{-z}}<br>\end{align*}<br>$$</p></li><li><p>The function $g(z)$, maps any real number to the $(0,1)$ interval, making it useful for transforming an arbitrary-valued function into a function suited for classification.</p></li><li><p>$h_{\theta}(x)$ will give us the <strong>probability</strong> that our output is $1$. The probability that the prediction is $0$ is just the complement of the probability that it is $1$.<br>$$<br>\begin{align*}<br>&amp; h_{\theta}(x) = P(y=1\mid x;\theta) = 1 - P(y=0\mid x;\theta)\<br>&amp; P(y=0\mid x;\theta) + P(y=1\mid x;\theta) = 1<br>\end{align*}<br>$$</p></li></ul><h4 id="3-Decision-Boundary"><a href="#3-Decision-Boundary" class="headerlink" title="3. Decision Boundary"></a>3. Decision Boundary</h4><ul><li><p>$$<br>\theta^Tx\ge 0 \Rightarrow y=1\<br>\theta^Tx\lt 0 \Rightarrow y=0<br>$$</p></li><li><p>The <strong>decision boundary</strong> is the line that separates the area where $y = 1$ and where $y = 0$. It is created by the hypothesis function.</p></li><li><p>The input to the sigmoid function $g(z)$ (e.g. $\theta^TX$) doesn’t need to be linear, and could be a function that describes a circle or any shape to fit our data.</p></li></ul><h4 id="4-Cost-Function"><a href="#4-Cost-Function" class="headerlink" title="4. Cost Function"></a>4. Cost Function</h4><ul><li><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p></li><li><p>Instead, our cost function for logistic regression looks like:<br>$$<br>\begin{align*}<br>&amp; J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})\<br>&amp; Cost(h_\theta(x), y) = -\log(h_\theta(x))\quad\quad if\ y = 1\<br>&amp; Cost(h_\theta(x), y) = -\log(1-h_\theta(x))\ if\ y = 0<br>\end{align*}<br>$$</p></li><li><p>$$<br>\begin{align*}<br>&amp; Cost(h_{\theta}(x), y) = 0\ if\ h_\theta(x) = y\<br>&amp; Cost(h_\theta(x), y) \rightarrow \infin\ if\ y = 0\ and\ h_\theta(x)\rightarrow 1\<br>&amp; Cost(h_\theta(x), y) \rightarrow \infin\ if\ y = 1\ and\ h_\theta(x) \rightarrow 0<br>\end{align*}<br>$$</p></li><li><p>If the correct answer ‘$y$’ is $0$, then the cost function will be $0$ if our hypothesis function also ouputs $0$. If the hypothesis approaches $1$, then the cost function will approach infinity. If the correct answer ‘$y$’ is $1$, the case is reverse.</p></li><li><p>Not that writing the cost function in this way guarantees that $J(\theta)$ is convex for logistic regression.</p></li></ul><h4 id="5-Simplified-Cost-Function"><a href="#5-Simplified-Cost-Function" class="headerlink" title="5. Simplified Cost Function"></a>5. Simplified Cost Function</h4><ul><li><p>Compress the cost function’s two conditional cses into one case:<br>$$<br>Cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))<br>$$</p></li><li><p>Cost function<br>$$<br>J(\theta) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))]<br>$$</p></li><li><p>A vectorized implementation:<br>$$<br>h = g(X\theta)\<br>J(\theta) = \frac{1}{m}(-y^T\log(h) - (1-y)^T\log(1-h))<br>$$</p></li></ul><h4 id="6-Gradient-Descent"><a href="#6-Gradient-Descent" class="headerlink" title="6. Gradient Descent"></a>6. Gradient Descent</h4><ul><li><p>$$<br>\begin{align*}<br>&amp; Repeat\ { \<br>&amp; \theta_j := \theta_j - \frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\<br>&amp; }<br>\end{align*}<br>$$</p></li><li><p>A vectorized implementation:<br>$$<br>\theta := \theta - \frac{\alpha}{m}X^T(g(X\theta) - \vec{y})<br>$$</p></li></ul><h4 id="7-Advanced-Optimization"><a href="#7-Advanced-Optimization" class="headerlink" title="7. Advanced Optimization"></a>7. Advanced Optimization</h4><ul><li>“Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize $\theta$ that can be used instead of gradient descent.</li></ul><h4 id="8-Multiclass-Classification-One-vs-all"><a href="#8-Multiclass-Classification-One-vs-all" class="headerlink" title="8. Multiclass Classification: One-vs-all"></a>8. Multiclass Classification: One-vs-all</h4><ul><li><p>Instead of $y = {0, 1}$, we will expand the definition so that $y = {0, 1\cdots n}$.</p></li><li><p>Since $y = {0, 1\cdots n}$, we divide the problem into $n+1$ binary classification problems; in each one, predict the probability that ‘$y$’ is a member of one of our classes.</p></li><li><p>$$<br>\begin{align*}<br>&amp; y\in {0, 1\cdots n}\<br>&amp; h_\theta^{(0)}(x) = P(y = 0\mid x;\theta)\<br>&amp; h_\theta^{(1)}(x) = P(y = 1\mid x;\theta)\<br>&amp; \cdots\<br>&amp; h_\theta^{(n)}(x) = P(y = n\mid x;\theta)\<br>&amp; prediction = \max_{i}(h_\theta^{(i)}(x))<br>\end{align*}<br>$$</p></li><li><p>We are basically choosing one class and then lumping all the others into a single second class. Do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> 分类问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Priority Queue</title>
      <link href="2021/06/02/Priority-Queue/"/>
      <url>2021/06/02/Priority-Queue/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A priority queue is an abstract data type for storing a collection of prioritized elements that supports<ul><li>arbitrary element insertion,</li><li>removal of elements in order of priority (the element with the first priority can be removed at any time)</li></ul></li></ul><h4 id="2-Priority-Queue-ADT"><a href="#2-Priority-Queue-ADT" class="headerlink" title="2. Priority Queue ADT"></a>2. Priority Queue ADT</h4><ul><li>A priority queue stores a collection of entries</li><li>Each entry is a pair (key, value)</li><li><strong>Entry ADT</strong><ul><li>An <em>entry</em> in a priority queue is simply a key-value pair</li><li>Priority queues store entries to allow for efficient insertion and removal based on keys</li></ul></li></ul><h4 id="3-Sequence-based-Priority-Queue"><a href="#3-Sequence-based-Priority-Queue" class="headerlink" title="3. Sequence-based Priority Queue"></a>3. Sequence-based Priority Queue</h4><ul><li>Implementation with an unsorted list<ul><li><em>insert</em> takes $O(1)$ time since we can insert the item at the beginning or end of the sequence</li><li><em>removeMin</em> and <em>min</em> take $O(n)$ time since we have to traverse the entire sequence to find the smallest key</li></ul></li><li>Implementation with a sorted list<ul><li><em>insert</em> takes $O(n)$ time since we have to find the place where to insert the item</li><li><em>removeMin</em> and <em>min</em> take $O(1)$ time, since the smallest key is at the beginning</li></ul></li></ul><h4 id="4-Priority-Queue-Sorting"><a href="#4-Priority-Queue-Sorting" class="headerlink" title="4. Priority Queue Sorting"></a>4. Priority Queue Sorting</h4><ul><li>Use a priority queue to sort a list of comparable elements<ul><li>Insert the elements one by one with a series of <em>insert</em> operations</li><li>Remove the elements in sorted order with a series of <em>removeMin</em> operations</li></ul></li><li>Selection-Sort<ul><li>Selection-sort is the variation of PQ-sort where the prioroty queue is implemented with an unsorted sequence</li><li>Running time of Selection-sort:<ul><li>Inserting the elements into the priority queue with $n$ insert operations takes $O(n)$ time</li><li>Removing the elements in sorted order from the priority queue with $n$ <em>removeMin</em> operations takes time proportional to $1+2+\cdots + n$</li></ul></li><li>Selection-sort runs in $O(n^2)$ time</li></ul></li><li>Insertion-Sort<ul><li>Insertion-sort is the variation of PQ-sort where the priority queue is implemented with a sorted sequence</li><li>Running time of Insertion-sort:<ul><li>Inserting the elements into the priority queue with $n$ <em>insert</em> operations takes time proportional to $1+2+\cdots +n$</li><li>Removing the elements in sorted order from the priority queue with a series of $n$ <em>removeMin</em> operations takes $O(n)$ time</li></ul></li><li>Insertion-sort runs in $O(n^2)$ time</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 课堂笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="2021/06/01/Linear-Regression/"/>
      <url>2021/06/01/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h5 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h5><ul><li>$x^{(i)}$ to denote the “input” variables, also called input features.</li><li>$y^{(i)}$ to denote the “output” or target variable that we are trying to predict.</li><li>A pair $(x^{(i)}, y^{(i)})$ is called a training example.</li><li>The dataset that we’ll be using to learn - a list of $m$ training examples $(x^{(i)}, y^{(i)}); i = 1, \cdots, m$ - is called a training set.</li><li>Note that the superscript “$(i)$” in the notation is simply an index into the training set, and has nothing to do with exponentiation.</li><li>$X$ to denote the space of input values.</li><li>$Y$ to denote the space of output values.</li></ul><h4 id="1-Model-Representation"><a href="#1-Model-Representation" class="headerlink" title="1. Model Representation"></a>1. Model Representation</h4><ul><li>Given a training set, to learn a function $h: X\rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. $h$ is called a hypothesis.</li><li>When the target variable that we’re trying to predict is continuous, we call the learning problem a regression problem.</li></ul><h4 id="2-Cost-Function-Loss-Function"><a href="#2-Cost-Function-Loss-Function" class="headerlink" title="2. Cost Function (Loss Function)"></a>2. Cost Function (Loss Function)</h4><ul><li><p>This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from $x$’s and the actual output $y$’s.</p></li><li><p>$$<br>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y_i} - y_i)^2 = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2<br>$$</p></li><li><p>This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $(\frac{1}{2})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\frac{1}{2}$ term.</p></li><li><p>As a goal, we should try to minimize the cost function.</p></li></ul><h4 id="3-Gradient-Descent"><a href="#3-Gradient-Descent" class="headerlink" title="3. Gradient Descent"></a>3. Gradient Descent</h4><ul><li><p>The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter $\alpha$, which is called the learning rate.</p></li><li><p>A smaller $\alpha$ would result in a smaller step and a larger $\alpha$ results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J(\theta_0, \theta_1)$.</p></li><li><p>The gradient descent algorithm is:</p><ul><li><p>repeat until convergence:<br>$$<br>\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)<br>$$<br>where</p><p>$j = 0, 1$ represents the feature index number.</p></li></ul></li><li><p>At each iterarion $j$, one should simultaneously update the parameters $\theta_1, \theta_2, \cdots, \theta_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation.</p></li><li><p>We should adjust our parameter $\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p></li><li><p>Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed. As we approach a local optimum, gradient descent will automatically take smaller steps. So, no need to decrese $\alpha$ over time.</p></li></ul><h4 id="4-Multiple-Features"><a href="#4-Multiple-Features" class="headerlink" title="4. Multiple Features"></a>4. Multiple Features</h4><ul><li><p>Linear regression with multiple variables is also known as <strong>“multivariate linear regression”</strong>.</p></li><li><p>$x^{(i)}_j = $ value of feature $j$ in the $i^{th}$ training example</p></li><li><p>$x^{(i)} = $ the input (features) of the $i^{th}$ training example</p></li><li><p>$m = $ the number of training examples</p></li><li><p>$n = $ the number of features</p></li><li><p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:<br>$$<br>h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \cdots + \theta_nx_n<br>$$</p></li><li><p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:<br>$$<br>h_{\theta}(x) = \begin{bmatrix}\theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n\end{bmatrix} \begin{bmatrix}x_0\x_1\ \vdots\ x_n\end{bmatrix} = \theta^Tx<br>$$</p></li></ul><h4 id="5-Gradient-Descent-for-Multiple-Variables"><a href="#5-Gradient-Descent-for-Multiple-Variables" class="headerlink" title="5. Gradient Descent for Multiple Variables"></a>5. Gradient Descent for Multiple Variables</h4><ul><li>repeat until convergence:<br>$$<br>\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\times x_j^{(i)}\quad for\ j := 0\cdots n<br>$$</li></ul><h4 id="6-Gradient-Descent-in-Practice-Feature-Scaling"><a href="#6-Gradient-Descent-in-Practice-Feature-Scaling" class="headerlink" title="6. Gradient Descent in Practice - Feature Scaling"></a>6. Gradient Descent in Practice - Feature Scaling</h4><ul><li><p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p></li><li><p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally: $-1 \le x_{(i)} \le 1$ or $-0.5 \le x_{(i)} \le 0.5$. These aren’t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p></li><li><p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown this formula:<br>$$<br>x_i := \frac{x_i - \mu_i}{s_i}<br>$$<br>Where $\mu_i$ is the <strong>average</strong> of all the values for feature $(i)$ and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.</p></li></ul><h4 id="7-Gradient-Descent-in-Practice-Learning-Rate"><a href="#7-Gradient-Descent-in-Practice-Learning-Rate" class="headerlink" title="7. Gradient Descent in Practice - Learning Rate"></a>7. Gradient Descent in Practice - Learning Rate</h4><ul><li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent. If $J(\theta)$ ever increases, then you probably need to decrease $\alpha$.</li><li><strong>Automatic convergence test.</strong> Declare convergence if $J(\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it’s difficult to choose this threshold value.</li><li>It has been proven that if learning rate $\alpha$ is sufficiently small, then $J(\theta)$ will decrease on every iteration.</li><li>To summarize:<ul><li>If $\alpha$ is too small: slow convergence.</li><li>If $\alpha$ is too large: may not decrease on every iteration and thus may not converge.</li></ul></li></ul><h4 id="8-Features-and-Polynomial-Regression"><a href="#8-Features-and-Polynomial-Regression" class="headerlink" title="8. Features and Polynomial Regression"></a>8. Features and Polynomial Regression</h4><ul><li>We can improve our features and the form of our hypothesis function in a couple different ways.</li><li>We can <strong>combine</strong> multiple features into one.</li><li><strong>Polynomial Regression</strong><ul><li>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</li><li>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</li><li>For example, if our hypothesis function is $h_{\theta}(x) = \theta_0 + \theta_1 x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$.</li><li>One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.</li></ul></li></ul><h4 id="9-Normal-Equation"><a href="#9-Normal-Equation" class="headerlink" title="9. Normal Equation"></a>9. Normal Equation</h4><ul><li><p>A second way of minimizing $J$.</p></li><li><p>In the “Normal Equation” method, we will minimize $J$ by explicitly taking its derivatives with respect to the $\theta_j$’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$</p></li><li><p>There is <strong>no need</strong> to do feature scaling with the normal equation.</p></li><li><p>The comparison of gradient descent and the normal equation:</p><img src="/2021/06/01/Linear-Regression/Screen Shot 2021-06-04 at 10.02.03 PM.png" style="zoom:50%;"></li><li><p>With the normal equation, computing the inversion has complexity $O(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, when $n$ exceeds $10,000$ it might be a good time to go from a normal solution to an iterative process.</p></li><li><p><strong>Noninvertibility</strong></p><ul><li>If $X^TX$ is <strong>noninvertible</strong>, the common causes might be having:<ul><li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li><li>Too many features (e.g. $m\le n$). In this case, delete some features or use “regularization”.</li></ul></li><li>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Red-Black Tree</title>
      <link href="2021/05/26/Red-Black-Tree/"/>
      <url>2021/05/26/Red-Black-Tree/</url>
      
        <content type="html"><![CDATA[<h2 align="center">Red-Black Trees</h2><h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A red-black tree is a binary search tree that satisfies the following properties:<ul><li><strong>Root Property</strong>: the root is black</li><li><strong>External Property</strong>: every leaf is black</li><li><strong>Internal/Red Property</strong>: the children of a red node are black</li><li><strong>Depth Property</strong>: all the leaves have the same <em>black depth</em>, defined as the number of proper ancestors that are black</li></ul></li></ul><h4 id="2-From-Red-Black-to-2-4-Trees"><a href="#2-From-Red-Black-to-2-4-Trees" class="headerlink" title="2. From Red-Black to $(2,4)$ Trees"></a>2. From Red-Black to $(2,4)$ Trees</h4><ul><li>Given a red-black tree, we can construct a corresponding $(2,4)$ tree:<ul><li>merge every red node $w$ into its parent, storing the entry from $w$ at its parent</li><li>the children of $w$ become ordered children of the parent</li></ul></li><li>Depth Property:<ul><li>$(2,4)$ Tree: all the external nodes have the same depth</li><li>Red-Black Tree: all the leaves have the same <em>black depth</em></li></ul></li><li>A red-black tree is a representation of a $(2,4)$ tree by means of a binary tree whose nodes are colored red or black</li><li>In comparison with its associated $(2,4)$ tree, a red-black tree has<ul><li>same logarithmic time performance</li><li>simpler implementation with a single node type</li></ul></li></ul><h4 id="3-Height-of-a-Red-Black-Tree"><a href="#3-Height-of-a-Red-Black-Tree" class="headerlink" title="3. Height of a Red-Black Tree"></a>3. Height of a Red-Black Tree</h4><ul><li>Theorem: A red-black tree storing $n$ items has height $O(\log n)$<ul><li>Proof:<ul><li>The height of a red-black tree is at most twice the height of its associated $(2,4)$ tree, which is $O(\log n)$</li></ul></li></ul></li></ul><h4 id="4-Search"><a href="#4-Search" class="headerlink" title="4. Search"></a>4. Search</h4><ul><li>The search algorithm for a red-black tree is the same as that for a binary search tree</li><li>Searching in a red-black tree takes $O(\log n)$ time</li></ul><h4 id="5-Insertion"><a href="#5-Insertion" class="headerlink" title="5. Insertion"></a>5. Insertion</h4><ul><li><p>To insert $(k, o)$, we execute the insertion algorithm for binary search trees and color <strong>red</strong> the newly inserted node $z$ <em>unless it is the root</em></p><ul><li>We preserve the root, external, and depth properties</li><li>If the parent $v$ of $z$ is black, we also preserve the internal property and we are done</li><li>Else ($v$ is red) we have a <strong>double red</strong> (i.e., a violation of the internal property), which requires a reorganization of the tree</li></ul></li><li><p><strong>Remedying a Double Red</strong></p><ul><li>Consider a double red with child $z$ and parent $v$, and let $w$ be the sibling of $v$</li><li>Case1: $w$ is black<ul><li>The double red is an incorrect replacement of a 4-node</li><li><strong>Restructuring</strong>: we change the 4-node replacement</li></ul></li><li>Case2: $w$ is red<ul><li>The double red corresponds to an overflow</li><li><strong>Recoloring</strong>: we perform the equivalent of a <strong>split</strong></li></ul></li></ul></li><li><p><strong>Restructuring</strong></p><ul><li><p>A restructuring remedies a child-parent double red when the parent red node has a black sibling</p></li><li><p>It is equivalent to restoring the <em>correct replacement</em> of a 4-node</p></li><li><p>The internal property is restored and the other properties are preserved</p></li><li><p>There are four restructuring configrations depending on whether the double red nodes are left or right children</p><img src="/2021/05/26/Red-Black-Tree/Screen Shot 2021-05-10 at 7.22.25 PM.png" style="zoom:50%;"></li></ul></li><li><p><strong>Recoloring</strong></p><ul><li>A recoloring remedies a child-parent double red when the parent red node has a red sibling</li><li>The parent $v$ and its sibling $w$ become black and the grandparent $u$ becomes red, unless it is the root</li><li>It is equivalent to performing a split on a 5-node</li><li><em>The double red violation may propagate to the grangparent $u$</em></li></ul></li><li><p>Analysis of Insertion</p><ul><li><pre><code class="pseudocode">Algorithm insert(k, o)1. We search for key k to locate the insertion node z2. We add the new entry (k, o) at node z and color z red3. while doubleRed(z)        if isBlack(sibling(parent(z)))            z &lt;- restructure(z)            return        else sibling(parent(z)) is red            z &lt;- recolor(z)</code></pre></li><li><p>Recall that a red-black tree has $O(\log n)$ height</p></li><li><p>Step 1 takes $O(\log n)$ time because we visit $O(\log n)$ nodes</p></li><li><p>Step 2 takes $O(1)$ time</p></li><li><p>Step 3 takes $O(\log n)$ time because we perform</p><ul><li>$O(\log n)$ recoloring, each taking $O(1)$ time, and</li><li>at most one restructuring taking $O(1)$ time</li></ul></li><li><p>Thus, an insertion in a red-black tree takes $O(\log n)$ time</p></li></ul></li></ul><h4 id="6-Deletion"><a href="#6-Deletion" class="headerlink" title="6. Deletion"></a>6. Deletion</h4><ul><li>To perform operation $remove(k)$, we first execute the deletion algorithm for binary search trees</li><li>Let $v$ be the internal node removed, $w$ the external node removed, and $r$ the sibling of $w$<ul><li>If $v$ was red, the resulting tree remains a valid red-black tree</li><li>If $v$ was black and $r$ was red, we color $r$ black and we are done</li><li>Else ($v$ and $r$ were both black) we color $r$ <em><strong>double black</strong></em>, to preserve the depth property</li></ul></li><li><strong>Remedying a Double Black</strong><ul><li>The algorithm for remedying a double black node $r$ with sibling $y$ considers three cases</li><li>Case1: $y$ is black and has a red child<ul><li>We perform a <strong>restructuring</strong>, equivalent to a <strong>transfer</strong>, and we are done.</li></ul></li><li>Case2: sibling $y$ of $r$ is black and its children are both black<ul><li>We perform a <strong>recoloring</strong>, equivalent to a <strong>fusion</strong>, which may propagate up the double black violation</li></ul></li><li>Case3: $y$ is red<ul><li>We perform an <strong>adjustment</strong>, equivalent to choosing a different representation of a 3-node, after which either Case 1 or Case 2 applies</li></ul></li></ul></li><li>Deletion in a red-black tree takes $O(\log n)$ time</li></ul>]]></content>
      
      
      <categories>
          
          <category> 课堂笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 红黑树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>堆</title>
      <link href="2020/09/17/%E5%A0%86/"/>
      <url>2020/09/17/%E5%A0%86/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 课堂笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 未完待续 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树有关算法</title>
      <link href="2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/"/>
      <url>2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h4 id="1-二叉树定义-java"><a href="#1-二叉树定义-java" class="headerlink" title="1. 二叉树定义(java)"></a>1. 二叉树定义(java)</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Definition for a binary tree node.</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> val;</span><br><span class="line">  TreeNode left;</span><br><span class="line">  TreeNode right;</span><br><span class="line">  TreeNode(<span class="keyword">int</span> x) &#123; val = x; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-二叉树翻转"><a href="#2-二叉树翻转" class="headerlink" title="2. 二叉树翻转"></a>2. 二叉树翻转</h4><img src="/2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/截屏2020-09-16 下午4.04.19.png" style="zoom:50%;"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//简单递归即可实现</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> TreeNode <span class="title">invertTree</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    TreeNode left = invertTree(root.left);</span><br><span class="line">    TreeNode right = invertTree(root.right);</span><br><span class="line">    root.right = left;</span><br><span class="line">    root.left = right;</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 当然还可以用四种遍历来解决这道题</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 未完待续 </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 总结 </tag>
            
            <tag> java </tag>
            
            <tag> 二叉树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序方法汇总</title>
      <link href="2020/09/15/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/"/>
      <url>2020/09/15/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h4 id="1-冒泡排序"><a href="#1-冒泡排序" class="headerlink" title="1. 冒泡排序"></a>1. 冒泡排序</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sort</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123; <span class="comment">// 整数数组升序</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length - <span class="number">1</span> - i; ++j) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[j] &gt; nums[j+<span class="number">1</span>]) &#123;</span><br><span class="line">          <span class="keyword">int</span> temp = nums[j];</span><br><span class="line">          nums[j] = nums[j+<span class="number">1</span>];</span><br><span class="line">          nums[j+<span class="number">1</span>] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一次循环，通过依次比较相邻元素，若前一个大于后一个则交换位置，循环结束后，数组内最大值将位于数组最后一个位置；第二次循环，也依次比较相邻元素，除了最后一对（因为最后一个是最大值，前一个肯定比后一个小）,循环结束数组内倒数第二大的值将位于数组倒数第二个位置；……直到没有一对数字需要比较，此时数组以升序排列。</p><p>冒泡排序的时间复杂度为O(n<sup>2</sup>)，空间复杂度为O(1)，计算方法请参见“<a href="https://scycy2.github.io/2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/">算法复杂度</a>”一文。</p><h4 id="2-选择排序"><a href="#2-选择排序" class="headerlink" title="2. 选择排序"></a>2. 选择排序</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sort</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">SelectionSort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">      <span class="keyword">int</span> minIndex = i; <span class="comment">// 记录最小值的下标</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; nums.length; ++j) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[j] &lt; nums[minIndex]) &#123;</span><br><span class="line">          minIndex = j;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 交换最小值和当前位置</span></span><br><span class="line">      <span class="keyword">int</span> temp = nums[i];</span><br><span class="line">      nums[i] = nums[minIndex];</span><br><span class="line">      nums[minIndex] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一次循环找出当前数组最小值的下标，并放到数组开头；第二次循环找出除了数组第一个值的数组最小值，并交换到第二个位置；……直到数组最后两个值比较（并交换），此时数组按从小到大顺序排列。</p><p>选择排序的时间复杂度为O(n<sup>2</sup>)，空间复杂度为O(1)。</p><h4><span id="3">3. 直接插入排序</span></h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sort</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">InsertionSort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.length; ++i) &#123;</span><br><span class="line">      <span class="keyword">int</span> temp = nums[i]; <span class="comment">// 当前值与前面已排好序的值比较</span></span><br><span class="line">      <span class="keyword">int</span> j;</span><br><span class="line">      <span class="keyword">for</span> (j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span> &amp;&amp; nums[j] &gt; temp; --j) &#123;</span><br><span class="line">        nums[j + <span class="number">1</span>] = nums[j]; <span class="comment">// 如果当前值小于前面的值，则将前面的值向后移</span></span><br><span class="line">      &#125;</span><br><span class="line">      nums[j + <span class="number">1</span>] = temp; <span class="comment">// 直到当前值大于等于前面的某个值或者前面没有值的时候，将该值插入</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一次循环，获取当前值temp=nums[1]，将temp与nums[0]比较，若小于nums[0]，将nums[0]的值往后移变为nums[1]，然后temp插入到nums[1]前即nums[0]的位置；第二次循环（前两个值已排好序），获取temp=nums[2]，先与nums[1]比较，若大于则不变，小于则将nums[1]的值往后移变为nums[2]，再比较temp与nums[0]并重复之前的过程；……最终数组以升序排序。</p><p>插入排序的空间复杂度为O(1)，时间复杂度则与原数组排列顺序有关，如果原数组已按升序排序，则该算法的时间复杂度为O(n)（最好的情况）；若按降序排列，则时间复杂度为O(n<sup>2</sup>)（最差的情况）。</p><h4 id="4-快速排序"><a href="#4-快速排序" class="headerlink" title="4. 快速排序"></a>4. 快速排序</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sort</span></span>&#123;</span><br><span class="line">  <span class="comment">// start为待排序数组开始的下标，end为结束下标</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] quickSort(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</span><br><span class="line">    <span class="keyword">int</span> pivot = nums[start]; <span class="comment">// 取数组某个值(此处选第一个值)作为参照，比它小的放在它左边，大的放在右边</span></span><br><span class="line">    <span class="keyword">int</span> i = start;</span><br><span class="line">    <span class="keyword">int</span> j = end;</span><br><span class="line">    <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">      <span class="keyword">while</span> (i &lt; j &amp;&amp; nums[j] &gt; pivot) &#123;</span><br><span class="line">        j--;</span><br><span class="line">      &#125; <span class="comment">// 从右往左，本就比参照大的数则不动</span></span><br><span class="line">      <span class="keyword">while</span> (i &lt; j &amp;&amp; nums[i] &lt; pivot) &#123;</span><br><span class="line">        i++;</span><br><span class="line">      &#125; <span class="comment">// 从左往右，本就比参照小的数不动</span></span><br><span class="line">      <span class="keyword">if</span> (nums[i] == nums[j] &amp;&amp; i &lt; j) &#123;</span><br><span class="line">        i++;</span><br><span class="line">      &#125;<span class="keyword">else</span> &#123; <span class="comment">// 交换</span></span><br><span class="line">        <span class="keyword">int</span> temp = nums[i];</span><br><span class="line">        nums[i] = nums[j];</span><br><span class="line">        nums[j] = temp;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (i - <span class="number">1</span> &gt; start) nums = quickSort(nums, start, i - <span class="number">1</span>); </span><br><span class="line">    <span class="keyword">if</span> (j + <span class="number">1</span> &lt; start) nums = quickSort(nums, j + <span class="number">1</span>, end);</span><br><span class="line">    <span class="keyword">return</span> nums;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一趟排序将数组分为两部分，一部分小于参照值，另一部分则大于参照值；再分别对两部分进行排序（递归）。当两部分都有序时，则整个数组都为有序状态。</p><p>快速排序的空间复杂度为O(1)，其时间复杂度与选取的参照有关，若每次选取的参照可使数组等分，则经过log<sub>2</sub>n躺划分，可完成排序，此时时间复杂度为O(nlog<sub>2</sub>n)；若每次选取的参照为最大值或最小值，则需要经过n躺划分，可完成排序，此时时间复杂度为O(n<sup>2</sup>)。</p><h4 id="5-希尔排序"><a href="#5-希尔排序" class="headerlink" title="5. 希尔排序"></a>5. 希尔排序</h4><p>希尔排序是插入排序的一种，又称“缩小增量排序”。</p><p>希尔排序就是将数组根据下标的一定增量分组，然后对每一组进行直接插入排序；随后增量减少，每次增量不同，都进行一次直接插入排序，直到增量为1，此时就是上面所提到的<a href="#3">直接插入排序</a>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sort</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">shellSort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> n = nums.length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> step = n / <span class="number">2</span>; step &gt;= <span class="number">1</span>; step /= <span class="number">2</span>) &#123; <span class="comment">// 步长</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = step; i &lt; n; ++i) &#123; <span class="comment">// 内循环两层为插入排序</span></span><br><span class="line">        <span class="keyword">int</span> temp = nums[i];</span><br><span class="line">        <span class="keyword">int</span> j = i - step;</span><br><span class="line">      <span class="keyword">while</span> (j &gt;= <span class="number">0</span> &amp;&amp; nums[j] &gt; temp) &#123;</span><br><span class="line">        nums[j + step] = nums[j];</span><br><span class="line">          j -= step;</span><br><span class="line">        &#125;</span><br><span class="line">        nums[j + step] = temp;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一次循环增量（步长）step为数组长度n的一半，此时根据增量将数组分为n/2组，分别为(nums[0], nums[step])，(nums[1], nums[1+step])，……(nums[step-1], nums[2*step-1])，然后对每组进行直接插入排序；第二次循环增量再减半，根据增量数组可分为多组，再对各组进行直接插入排序；……最后增量为1，即普通的直接插入排序。</p><p>希尔排序的空间复杂度为O(1)，但时间复杂度很难计算，查阅资料后可得希尔排序的平均时间复杂度为O(n<sup>3/2</sup>)（注意这里是<strong>平均</strong>)。</p><p>希尔排序较直接插入排序快是因为当增量大时，进行直接插入排序的元素少，速度快；当增量逐渐减少，此时数组已基本有序，此时直接插入排序对基本有序的序列排序效率很高。</p><p>这里有一个例子来自维基百科（个人认为维基百科将此过程通过列来表达更加清晰易懂）</p><p>此处待补充。。。。。。。。。。。。。。。。。。。。。。。</p><h4>6. 归并排序</h4><p>归并排序是建立在归并操作上的一种有效的、稳定的排序算法，该算法采用了分治法(Divide and Conquer)。</p><p>将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Public <span class="class"><span class="keyword">class</span> <span class="title">Sort</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span>[] temp, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (start &lt; end) &#123;</span><br><span class="line">      <span class="keyword">int</span> mid = (start + end) / <span class="number">2</span>;</span><br><span class="line">      mergeSort(arr, temp, start, mid);</span><br><span class="line">      mergeSort(arr, temp, mid + <span class="number">1</span>, end);</span><br><span class="line">      </span><br><span class="line">      merge(arr, temp, start, mid, end);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span>[] temp, <span class="keyword">int</span> start, <span class="keyword">int</span> mid, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = start;</span><br><span class="line">    <span class="keyword">int</span> j = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= mid) &#123;</span><br><span class="line">      <span class="keyword">if</span> (arr[i] &lt;= arr[j]) &#123;</span><br><span class="line">        temp[k] = arr[i];</span><br><span class="line">        ++i;</span><br><span class="line">      &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">        temp[k] = arr[j];</span><br><span class="line">        ++j;</span><br><span class="line">      &#125;</span><br><span class="line">      ++k;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (i &lt;= mid) &#123;</span><br><span class="line">      temp[k] = arr[i];</span><br><span class="line">      ++i;</span><br><span class="line">      ++k;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (j &lt;= end) &#123;</span><br><span class="line">      temp[k] = arr[j];</span><br><span class="line">      ++j;</span><br><span class="line">      ++k;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    System.arraycopy(temp, <span class="number">0</span>, arr, start + <span class="number">0</span>, k);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 未完待续 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树四种遍历方式</title>
      <link href="2020/09/13/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/"/>
      <url>2020/09/13/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h4 id="1-二叉树定义-java"><a href="#1-二叉树定义-java" class="headerlink" title="1. 二叉树定义(java)"></a>1. 二叉树定义(java)</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Definition for a binary tree node.</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> val;</span><br><span class="line">  TreeNode left;</span><br><span class="line">  TreeNode right;</span><br><span class="line">  TreeNode(<span class="keyword">int</span> x) &#123; val = x; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-前、中、后序遍历-递归版"><a href="#2-前、中、后序遍历-递归版" class="headerlink" title="2. 前、中、后序遍历(递归版)"></a>2. 前、中、后序遍历(递归版)</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">    List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    helper(root, res);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">helper</span><span class="params">(TreeNode root, List &lt;Integer&gt; res)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// res.add(root.val); // 前序遍历，先将root的值加入List中</span></span><br><span class="line">    <span class="keyword">if</span> (root.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">        helper(root.left, res);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// res.add(root.val); // 中序遍历，先将左边节点的值加入List中，再加入root的值</span></span><br><span class="line">      <span class="keyword">if</span> (root.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">      helper(root.right, res);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// res.add(root.val); // 后序遍历，先将左右节点的值加入List中，最后加入root的值</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-前、中、后序遍历-迭代版"><a href="#3-前、中、后序遍历-迭代版" class="headerlink" title="3. 前、中、后序遍历(迭代版)"></a>3. 前、中、后序遍历(迭代版)</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//前序</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  List&lt;Integer&gt; ans = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    LinkedList&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;(); <span class="comment">// 需要不断的增删，因此用LinkedList</span></span><br><span class="line">    stack.add(root);</span><br><span class="line">    <span class="keyword">while</span> (!nodes.isEmpty()) &#123;</span><br><span class="line">      TreeNode n = stack.pollLast(); <span class="comment">//根据stack先进后出的原则，先从上到下获取所有左节点的值，再往右获取所有右节点的值</span></span><br><span class="line">      ans.add(n.val);</span><br><span class="line">      <span class="keyword">if</span> (n.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">        stack.add(n.right);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (n.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">        stack.add(n.left);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 中序</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4-层次遍历"><a href="#4-层次遍历" class="headerlink" title="4. 层次遍历"></a>4. 层次遍历</h4>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 未完待续 </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 总结 </tag>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法复杂度</title>
      <link href="2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
      <url>2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h3 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1. 时间复杂度"></a>1. 时间复杂度</h3><h4 id="1-1-大O表示法："><a href="#1-1-大O表示法：" class="headerlink" title="1.1 大O表示法："></a>1.1 大O表示法：</h4><p>用O(n)来体现算法的时间复杂度。</p><p>大O表示法O(f(n))中f(n)可以是1、n<sup>2</sup>、logn等，接下来看看如何推倒大O阶。</p><h4 id="1-2-推导大O阶规则："><a href="#1-2-推导大O阶规则：" class="headerlink" title="1.2 推导大O阶规则："></a>1.2 推导大O阶规则：</h4><p>a. <strong>用1来代替运行时间中的所有加法常数</strong></p><p>b. <strong>f(n)若是多项式，只保留最高阶项即可</strong></p><p>c. <strong>去掉最高阶项系数</strong></p><h4 id="1-3"><a href="#1-3" class="headerlink" title="1.3"></a>1.3</h4><ul><li><p>常数阶</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>; </span><br><span class="line">i++;</span><br><span class="line">System.out.println(<span class="string">&quot;O&quot;</span>);</span><br><span class="line">i--;</span><br><span class="line"><span class="comment">//每一句都只执行一次</span></span><br></pre></td></tr></table></figure><p>这段代码执行次数为4次，根据规则a，其时间复杂度为O(1)。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; <span class="number">100</span>) &#123;</span><br><span class="line">i++;</span><br><span class="line">&#125;</span><br><span class="line">System.out.Println(i);</span><br></pre></td></tr></table></figure><p>这段代码虽然循环了100次，但它的时间复杂度依然为O(1)，因为它的执行次数只是一个较大的常数.</p></li><li><p>线性阶</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">System.out.println(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此段代码为for循环，其中<code>int i = 0</code>只执行一次，<code>i &lt; n</code>执行n次，<code>i++</code>执行n次，打印语句也执行了n次，因此这段代码执行次数为3n+1，根据规则，T(n) = O(n)。</p></li><li><p>对数阶</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>; <span class="comment">//执行一次</span></span><br><span class="line"><span class="keyword">while</span> (i &lt; n) &#123; </span><br><span class="line">i *= <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(i); <span class="comment">//执行一次</span></span><br></pre></td></tr></table></figure><p>中间循环体<code>i &lt; n</code>以及<code>i *= 2</code>都分别执行了log<sub>2</sub>n次，因为i每次翻倍增加，通过计算可知中间循环体循环了log<sub>2</sub>n次，一次这段代码执行次数为1+2log<sub>2</sub>n，根据规则，T(n) = O(logn)。</p></li><li><p>平方阶</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++) &#123; <span class="comment">// for (int j = 0; j &lt; i + 1; j++)</span></span><br><span class="line">System.out.println(i + <span class="string">&quot; &quot;</span> + j);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最内层循环体执行次数为xn<sup>2</sup>(x为某常数)，可得该段代码执行次数最高阶项为xn<sup>2</sup>，根据规则，T(n) = O(n<sup>2</sup>)。</p><p>同样可得立方阶，四次方阶……</p></li><li><p>碰到的较复杂的算法复杂度分析</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * public class ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) &#123; val = x; &#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//合并K个有序链表</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">mergeKLists</span><span class="params">(ListNode[] lists)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (lists.length == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode ans = lists[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; lists.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (lists[i] == <span class="keyword">null</span>) <span class="keyword">continue</span>;</span><br><span class="line">            ans = mergeTwoLists(ans, lists[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">mergeTwoLists</span><span class="params">(ListNode l1, ListNode l2)</span> </span>&#123;</span><br><span class="line">        ListNode head = <span class="keyword">new</span> ListNode();</span><br><span class="line">        ListNode curr = head;</span><br><span class="line">        <span class="keyword">while</span>(l1 != <span class="keyword">null</span> &amp;&amp; l2 != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (l1.val &lt;= l2.val) &#123;</span><br><span class="line">                curr.next = l1;</span><br><span class="line">                curr = curr.next;</span><br><span class="line">                l1 = l1.next;</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                curr.next = l2;</span><br><span class="line">                curr = curr.next;</span><br><span class="line">                l2 = l2.next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (l1 == <span class="keyword">null</span>) &#123;</span><br><span class="line">            curr.next = l2;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (l2 == <span class="keyword">null</span>) &#123;</span><br><span class="line">            curr.next = l1;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> head.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先分析<code>mergeTwoLists</code>函数的时间复杂度，假设每个链表的长度为n，可简单地得出该函数的T(o) = O(n + n) = O(n) (为简便，只计算循环体执行次数)。再分析<code>mergeKLists</code>函数的时间复杂度，同样的每个链表长度都为n，第一次合并前两个链表，此时<code>ans</code> 的长度都为2n，第x次合并后，长度变为xn，第x次的时间代价为O(n + (x - 1) $\times$ n) = O(x $\times$ n)，此时总的时间代价可用求和公式计算O($\sum_{i=1}^k(i\times n)$) = O($\frac{(1+k)\times k}{2}\times n$) = O(k<sup>2</sup>$\times $n)，所以时间复杂度为O(k<sup>2</sup> $\times$ n)。</p></li></ul><h3 id="2-空间复杂度"><a href="#2-空间复杂度" class="headerlink" title="2. 空间复杂度"></a>2. 空间复杂度</h3><h4 id="2-1-空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S-n-来定义。"><a href="#2-1-空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S-n-来定义。" class="headerlink" title="2.1 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S(n)来定义。"></a>2.1 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S(n)来定义。</h4><h4 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h4><p>空间复杂度常用的有O(1), O(n), O(n<sup>2</sup>)</p><h4 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h4><ul><li><p>O(1)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nums.length - <span class="number">1</span> - i; j++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (nums[j] &gt; nums[j+<span class="number">1</span>]) &#123;</span><br><span class="line">        <span class="keyword">int</span> temp = nums[j];</span><br><span class="line">        nums[j] = nums[j+<span class="number">1</span>];</span><br><span class="line">        nums[j+<span class="number">1</span>] = temp;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>经典的冒泡排序以及插入排序、选择排序等都是空间复杂度为O(1)的算法，因为它没有临时占用额外的存储空间，只在自己的数组里进行交换。</p></li><li><p>O(n)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">climbStairs</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> dp[n];</span><br><span class="line">        &#125;</span><br><span class="line">        dp[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (n &gt;= <span class="number">3</span>)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= n; i++) &#123;</span><br><span class="line">                dp[i] = dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是leetcode上一道算法题，需要你计算爬到n阶且每次只能爬1或2阶有多少种不同的方法。很显然这是一道动态规划的题目，因此我们可以new一个数组来保存爬1～n阶各有多少种方法，而到n阶的方法就是到n-1阶和到n-2阶的方法和。这里我们new了一个长度为n+1的数组，用了额外的空间来存储我们需要的数据，因此空间复杂度为O(n+1) = O(n)。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 算法复杂度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo-theme-matery主题部分优化</title>
      <link href="2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/"/>
      <url>2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>hexo-theme-matery下载与配置: <a href="https://github.com/blinkfox/hexo-theme-matery">https://github.com/blinkfox/hexo-theme-matery</a></p><h3 id="1-关于个人信息的添加及修改："><a href="#1-关于个人信息的添加及修改：" class="headerlink" title="1. 关于个人信息的添加及修改："></a>1. 关于个人信息的添加及修改：</h3><p>以自己的个人信息作说明：</p><p>原：<img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/yuan.png" style="zoom:50%;"></p><p>现：</p><img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/PersonalInfo.png" style="zoom:50%;"><p>首先找到<code>/hexo-theme-matery/layout/about.ejs</code> ，并找到代码如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">&quot;author&quot;</span>&gt; <span class="comment">//作者信息</span></span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">&quot;post-statis hide-on-large-only&quot;</span> data-aos=<span class="string">&quot;zoom-in-right&quot;</span>&gt;</span><br><span class="line">        &lt;%- partial(<span class="string">&#x27;_partial/post-statis&#x27;</span>) %&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt; /</span><span class="regexp">/引用另一个文件，表示的是左边文章、分类、标签以及他们的数量</span></span><br><span class="line"><span class="regexp">    &lt;div class=&quot;title&quot;&gt;&lt;%- config.author %&gt;&lt;/</span>div&gt; <span class="comment">//作者名字，在blog/_config.yml中配置</span></span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">&quot;career&quot;</span>&gt;&lt;%- theme.profile.career %&gt;&lt;<span class="regexp">/div&gt; /</span><span class="regexp">/作者职业，在/</span>hexo-theme-matery/layout/about.ejs中配置</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">&quot;social-link hide-on-large-only&quot;</span> data-aos=<span class="string">&quot;zoom-in-left&quot;</span>&gt;</span><br><span class="line">    &lt;%- partial(<span class="string">&#x27;_partial/social-link&#x27;</span>) %&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt; /</span><span class="regexp">/右边github、email、QQ等的链接，也可自己减少或增加</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure><p>因此想要修改或增加自己的信息，需要在相应的配置文件里进行修改或增加。</p><p>比如我自己做的修改，首先打开<code>hexo-theme-matery</code>下的<code>_config.yml</code>文件，并找到<code>profile</code>属性，</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">profile:</span></span><br><span class="line">  <span class="attr">avatar:</span> <span class="string">/medias/头像.jpg</span></span><br><span class="line">  <span class="attr">career:</span> <span class="string">Undergraduate</span></span><br><span class="line">  <span class="attr">school:</span> <span class="string">University</span> <span class="string">of</span> <span class="string">Nottingham</span> <span class="string">Ningbo</span> <span class="string">China</span> <span class="string">(UNNC)</span></span><br><span class="line">  <span class="attr">major:</span> <span class="string">Computer</span> <span class="string">Science</span> <span class="string">with</span> <span class="string">Artificial</span> <span class="string">Intelligence</span> <span class="string">(4+0)</span></span><br><span class="line">  <span class="attr">introduction:</span> <span class="string">If</span> <span class="string">you</span> <span class="string">wish</span> <span class="string">to</span> <span class="string">succeed,</span> <span class="string">you</span> <span class="string">should</span> <span class="string">use</span> <span class="string">persistence</span> <span class="string">as</span> <span class="string">your</span> <span class="string">good</span> <span class="string">friend,</span> <span class="string">experience</span> <span class="string">as</span> <span class="string">your</span> <span class="string">reference,</span> <span class="string">prudence</span> <span class="string">as</span> <span class="string">your</span> <span class="string">brother</span> <span class="string">and</span> <span class="string">hope</span> <span class="string">as</span> <span class="string">your</span> <span class="string">sentry.</span></span><br></pre></td></tr></table></figure><p>在<code>profile</code>里修改了头像，职业，以及新增了学校和专业。</p><p>那么该如何将这些特性在网页里显示出来呢？我们需要在上面说的<code>about.ejs</code>这个文件里做相应的修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class&#x3D;&quot;title&quot;&gt;&lt;%- config.author %&gt;&lt;&#x2F;div&gt;</span><br><span class="line">&lt;div class&#x3D;&quot;career&quot;&gt;&lt;%- theme.profile.career %&gt;&lt;&#x2F;div&gt;</span><br><span class="line">&lt;div class&#x3D;&quot;major&quot;&gt;&lt;%- theme.profile.major %&gt;&lt;&#x2F;div&gt;</span><br><span class="line">&lt;div class&#x3D;&quot;school&quot;&gt;&lt;%- theme.profile.school %&gt;&lt;&#x2F;div&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>只要加几个<code>div</code>块元素，就能显示你的其他信息。</p><h3 id="2-Post封面图片的修改"><a href="#2-Post封面图片的修改" class="headerlink" title="2. Post封面图片的修改"></a>2. Post封面图片的修改</h3><p>先找到<code>/hexo-theme/matery/layout/_partial/post_cover.ejs</code>中的代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (page.img) &#123;</span><br><span class="line">    featureimg = url_for(page.img); <span class="comment">// 如果设置了img属性，那么封面图片就是你设置的图片</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">var</span> hashCode = <span class="function"><span class="keyword">function</span> (<span class="params">str</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!str &amp;&amp; str.length === <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> hash = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>, len = str.length; i &lt; len; i++) &#123;</span><br><span class="line">            hash = ((hash &lt;&lt; <span class="number">5</span>) - hash) + str.charCodeAt(i);</span><br><span class="line">            hash |= <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hash;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> len = theme.featureImages.length;</span><br><span class="line">    <span class="keyword">var</span> num = <span class="built_in">Math</span>.abs(hashCode(page.title) % len);</span><br><span class="line"></span><br><span class="line">    featureimg = theme.jsDelivr.url</span><br><span class="line">            ? theme.jsDelivr.url + url_for(theme.featureImages[num])</span><br><span class="line">            : url_for(theme.featureImages[num]);</span><br><span class="line">&#125; <span class="comment">// 否则将会采用作者的算法，根据title的长度来计算hashcode，再根据hashcode从默认的图片中选出一张。如果两篇文章的title都是四个字，那这两篇的封面就是同一张图片。</span></span><br></pre></td></tr></table></figure><p>根据代码可知，想要一篇post用特定的图片，就必须设置这篇post的img属性。如下</p><img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/屏幕快照 2020-07-20 下午2.41.07.png" style="zoom:50%;"><p>可将图片移至<code>hexo-theme-matery/source/medias/featureimages</code>下，并将img属性设置为<code>/medias/featureimages/‘图片名’</code>即可。</p>]]></content>
      
      
      <categories>
          
          <category> 博客优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/07/16/hello-world/"/>
      <url>2020/07/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 初始内容 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
