<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Neural-Network Implementation</title>
      <link href="2021/08/24/Neural-Network-Implementation/"/>
      <url>2021/08/24/Neural-Network-Implementation/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true">#Initialize parameters for deep neural network</span><span class="token keyword">def</span> <span class="token function">initialize_parameters_deep</span><span class="token punctuation">(</span>layer_dims<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">"""  Argument:  layer_dims -- python array (list) containing the dimensions of each layer in network    Returns:  parameters -- python dictionary containing parameters "W1", "b1", ..., "WL", "bL":                                  Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])                                  bl -- bias vector of shape (layer_dims[l], 1)  """</span>  parameters <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  L <span class="token operator">=</span> len<span class="token punctuation">(</span>layer_dims<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># number of layers in the network</span>  <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> L<span class="token punctuation">)</span><span class="token punctuation">:</span>      parameters<span class="token punctuation">[</span><span class="token string">'W'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> layer_dims<span class="token punctuation">[</span>l<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>      parameters<span class="token punctuation">[</span><span class="token string">'b'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>              <span class="token keyword">assert</span><span class="token punctuation">(</span>parameters<span class="token punctuation">[</span><span class="token string">'W'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> layer_dims<span class="token punctuation">[</span>l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token keyword">assert</span><span class="token punctuation">(</span>parameters<span class="token punctuation">[</span><span class="token string">'b'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">return</span> parameters<span class="token comment" spellcheck="true">#Implement the linear part of a layer's forward propagation.</span><span class="token keyword">def</span> <span class="token function">linear_forward</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    A -- activations from previous layer (or input data): (size of previous layer, number of examples)    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)    b -- bias vector, numpy array of shape (size of the current layer, 1)    Returns:    Z -- the input of the activation function, also called pre-activation parameter     cache -- a python tuple containing "A", "W" and "b" ; stored for computing the backward pass efficiently    """</span>    Z <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W<span class="token punctuation">,</span> A<span class="token punctuation">)</span> <span class="token operator">+</span> b    cache <span class="token operator">=</span> <span class="token punctuation">(</span>A<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        <span class="token keyword">return</span> Z<span class="token punctuation">,</span> cache<span class="token comment" spellcheck="true">#Implement the forward propagation for the LINEAR->ACTIVATION layer</span><span class="token keyword">def</span> <span class="token function">linear_activation_forward</span><span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">,</span> activation<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)    b -- bias vector, numpy array of shape (size of the current layer, 1)    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"    Returns:    A -- the output of the activation function, also called the post-activation value     cache -- a python tuple containing "linear_cache" and "activation_cache";             stored for computing the backward pass efficiently    """</span>        <span class="token keyword">if</span> activation <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>        Z<span class="token punctuation">,</span> linear_cache <span class="token operator">=</span> linear_forward<span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        A<span class="token punctuation">,</span> activation_cache <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>Z<span class="token punctuation">)</span>    <span class="token keyword">elif</span> activation <span class="token operator">==</span> <span class="token string">"relu"</span><span class="token punctuation">:</span>        Z<span class="token punctuation">,</span> linear_cache <span class="token operator">=</span> linear_forward<span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        A<span class="token punctuation">,</span> activation_cache <span class="token operator">=</span> relu<span class="token punctuation">(</span>Z<span class="token punctuation">)</span>            cache <span class="token operator">=</span> <span class="token punctuation">(</span>linear_cache<span class="token punctuation">,</span> activation_cache<span class="token punctuation">)</span>    <span class="token keyword">return</span> A<span class="token punctuation">,</span> cache  <span class="token comment" spellcheck="true">#Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation</span><span class="token comment" spellcheck="true">#For other computation, we can change the for loop</span><span class="token keyword">def</span> <span class="token function">L_model_forward</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    X -- data, numpy array of shape (input size, number of examples)    parameters -- output of initialize_parameters_deep()        Returns:    AL -- activation value from the output (last) layer    caches -- list of caches containing:                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)    """</span>    caches <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    A <span class="token operator">=</span> X    L <span class="token operator">=</span> len<span class="token punctuation">(</span>parameters<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>                  <span class="token comment" spellcheck="true"># number of layers in the neural network</span>        <span class="token comment" spellcheck="true"># Implement [LINEAR -> RELU]*(L-1). Add "cache" to the "caches" list.</span>    <span class="token comment" spellcheck="true"># The for loop starts at 1 because layer 0 is the input</span>    <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> L<span class="token punctuation">)</span><span class="token punctuation">:</span>        A_prev <span class="token operator">=</span> A         A<span class="token punctuation">,</span> cache <span class="token operator">=</span> linear_activation_forward<span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">)</span>        caches<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cache<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Implement LINEAR -> SIGMOID. Add "cache" to the "caches" list.</span>    AL<span class="token punctuation">,</span> cache <span class="token operator">=</span> linear_activation_forward<span class="token punctuation">(</span>A<span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"sigmoid"</span><span class="token punctuation">)</span>    caches<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cache<span class="token punctuation">)</span>              <span class="token keyword">return</span> AL<span class="token punctuation">,</span> caches  <span class="token comment" spellcheck="true">#Implement the cost function</span><span class="token keyword">def</span> <span class="token function">compute_cost</span><span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)    Returns:    cost -- cross-entropy cost    """</span>        m <span class="token operator">=</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Compute loss from aL and y.</span>    cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token operator">/</span>m<span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>AL<span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>Y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>AL<span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span>        cost <span class="token operator">=</span> np<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span>        <span class="token keyword">return</span> cost  <span class="token comment" spellcheck="true">#Implement the linear portion of backward propagation for a single layer (layer l)</span><span class="token keyword">def</span> <span class="token function">linear_backward</span><span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> cache<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    dZ -- Gradient of the cost with respect to the linear output (of current layer l)    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer    Returns:    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev    dW -- Gradient of the cost with respect to W (current layer l), same shape as W    db -- Gradient of the cost with respect to b (current layer l), same shape as b    """</span>    A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b <span class="token operator">=</span> cache    m <span class="token operator">=</span> A_prev<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    dW <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>m<span class="token operator">*</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> A_prev<span class="token punctuation">.</span>T<span class="token punctuation">)</span>    db <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>m<span class="token operator">*</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    dA_prev <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dZ<span class="token punctuation">)</span>        <span class="token keyword">return</span> dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db  <span class="token comment" spellcheck="true">#Implement the backward propagation for the LINEAR->ACTIVATION layer.</span><span class="token keyword">def</span> <span class="token function">linear_activation_backward</span><span class="token punctuation">(</span>dA<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> activation<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    dA -- post-activation gradient for current layer l     cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"        Returns:    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev    dW -- Gradient of the cost with respect to W (current layer l), same shape as W    db -- Gradient of the cost with respect to b (current layer l), same shape as b    """</span>    linear_cache<span class="token punctuation">,</span> activation_cache <span class="token operator">=</span> cache        <span class="token keyword">if</span> activation <span class="token operator">==</span> <span class="token string">"relu"</span><span class="token punctuation">:</span>        dZ <span class="token operator">=</span> relu_backward<span class="token punctuation">(</span>dA<span class="token punctuation">,</span> activation_cache<span class="token punctuation">)</span>        dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db <span class="token operator">=</span> linear_backward<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> linear_cache<span class="token punctuation">)</span>    <span class="token keyword">elif</span> activation <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>        dZ <span class="token operator">=</span> sigmoid_backward<span class="token punctuation">(</span>dA<span class="token punctuation">,</span> activation_cache<span class="token punctuation">)</span>        dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db <span class="token operator">=</span> linear_backward<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> linear_cache<span class="token punctuation">)</span>        <span class="token keyword">return</span> dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db<span class="token comment" spellcheck="true">#Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group</span><span class="token keyword">def</span> <span class="token function">L_model_backward</span><span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> caches<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    AL -- probability vector, output of the forward propagation (L_model_forward())    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)    caches -- list of caches containing:                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])        Returns:    grads -- A dictionary with the gradients             grads["dA" + str(l)] = ...              grads["dW" + str(l)] = ...             grads["db" + str(l)] = ...     """</span>    grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    L <span class="token operator">=</span> len<span class="token punctuation">(</span>caches<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># the number of layers</span>    m <span class="token operator">=</span> AL<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    Y <span class="token operator">=</span> Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>AL<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># after this line, Y is the same shape as AL</span>        <span class="token comment" spellcheck="true"># Initializing the backpropagation</span>    dAL <span class="token operator">=</span> <span class="token operator">-</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> AL<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> Y<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> AL<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Lth layer (SIGMOID -> LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]</span>    current_cache <span class="token operator">=</span> caches<span class="token punctuation">[</span>L<span class="token number">-1</span><span class="token punctuation">]</span>    dA_prev_temp<span class="token punctuation">,</span> dW_temp<span class="token punctuation">,</span> db_temp <span class="token operator">=</span> linear_activation_backward<span class="token punctuation">(</span>dAL<span class="token punctuation">,</span> current_cache<span class="token punctuation">,</span> <span class="token string">"sigmoid"</span><span class="token punctuation">)</span>    grads<span class="token punctuation">[</span><span class="token string">"dA"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dA_prev_temp    grads<span class="token punctuation">[</span><span class="token string">"dW"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dW_temp    grads<span class="token punctuation">[</span><span class="token string">"db"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> db_temp        <span class="token comment" spellcheck="true"># Loop from l=L-2 to l=0</span>    <span class="token keyword">for</span> l <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>range<span class="token punctuation">(</span>L<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># lth layer: (RELU -> LINEAR) gradients.</span>        <span class="token comment" spellcheck="true"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span>        current_cache <span class="token operator">=</span> caches<span class="token punctuation">[</span>l<span class="token punctuation">]</span>        dA_prev_temp<span class="token punctuation">,</span> dW_temp<span class="token punctuation">,</span> db_temp <span class="token operator">=</span> linear_activation_backward<span class="token punctuation">(</span>dA_prev_temp<span class="token punctuation">,</span> current_cache<span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">)</span>        grads<span class="token punctuation">[</span><span class="token string">"dA"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dA_prev_temp        grads<span class="token punctuation">[</span><span class="token string">"dW"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dW_temp        grads<span class="token punctuation">[</span><span class="token string">"db"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> db_temp    <span class="token keyword">return</span> grads  <span class="token comment" spellcheck="true">#Update parameters using gradient descent</span><span class="token keyword">def</span> <span class="token function">update_parameters</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    params -- python dictionary containing your parameters     grads -- python dictionary containing your gradients, output of L_model_backward        Returns:    parameters -- python dictionary containing your updated parameters                   parameters["W" + str(l)] = ...                   parameters["b" + str(l)] = ...    """</span>    parameters <span class="token operator">=</span> params<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    L <span class="token operator">=</span> len<span class="token punctuation">(</span>parameters<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span> <span class="token comment" spellcheck="true"># number of layers in the neural network</span>    <span class="token comment" spellcheck="true"># Update rule for each parameter. Use a for loop.</span>    <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">:</span>        parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-</span> learning_rate <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token string">"dW"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-</span> learning_rate <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token string">"db"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> parameters<span class="token comment" spellcheck="true">#Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.</span><span class="token keyword">def</span> <span class="token function">L_layer_model</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> layers_dims<span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.0075</span><span class="token punctuation">,</span> num_iterations <span class="token operator">=</span> <span class="token number">3000</span><span class="token punctuation">,</span> print_cost<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).    learning_rate -- learning rate of the gradient descent update rule    num_iterations -- number of iterations of the optimization loop    print_cost -- if True, it prints the cost every 100 steps        Returns:    parameters -- parameters learnt by the model. They can then be used to predict.    """</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    costs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                         <span class="token comment" spellcheck="true"># keep track of cost</span>        <span class="token comment" spellcheck="true"># Parameters initialization.</span>    parameters <span class="token operator">=</span> initialize_parameters_deep<span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Loop (gradient descent)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.</span>        AL<span class="token punctuation">,</span> caches <span class="token operator">=</span> L_model_forward<span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>        cost <span class="token operator">=</span> compute_cost<span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>        grads <span class="token operator">=</span> L_model_backward<span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> caches<span class="token punctuation">)</span>        parameters <span class="token operator">=</span> update_parameters<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span>                        <span class="token comment" spellcheck="true"># Print the cost every 100 iterations</span>        <span class="token keyword">if</span> print_cost <span class="token operator">and</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> i <span class="token operator">==</span> num_iterations <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cost after iteration {}: {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> np<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>cost<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> i <span class="token operator">==</span> num_iterations<span class="token punctuation">:</span>            costs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>        <span class="token keyword">return</span> parameters<span class="token punctuation">,</span> costs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Neural Network </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Advice for applying machine learning</title>
      <link href="2021/06/29/Advice-for-applying-machine-learning/"/>
      <url>2021/06/29/Advice-for-applying-machine-learning/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Evaluating-a-Learning-Algorithm"><a href="#1-Evaluating-a-Learning-Algorithm" class="headerlink" title="1. Evaluating a Learning Algorithm"></a>1. Evaluating a Learning Algorithm</h4><h5 id="1-1-Evaluating-a-Hypothesis"><a href="#1-1-Evaluating-a-Hypothesis" class="headerlink" title="1.1 Evaluating a Hypothesis"></a>1.1 Evaluating a Hypothesis</h5><ul><li><p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a <strong>training set</strong> and a <strong>test set</strong>. Typically, the training set consists of $70%$ of data and the test set is the remaining $30%$.</p></li><li><p>The new procedure using these two sets is then:</p><ul><li>Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set</li><li>Compute the test set error $J_{test}(\Theta)$</li></ul></li><li><p><strong>The test set error</strong></p><ul><li><p>For linear regression: $J_{test}(\Theta) = \frac{1}{2m_{test}}\Sigma_{i=1}^{m_{test}}(h_\Theta(x_{test}^{(i)})-y_{test}^{(i)})^2$</p></li><li><p>For classification ~ Misclassification error (aka $0/1$ misclassification error):<br>$$<br>err(h_\Theta(x), y) = \begin{cases}1\quad \text{if } h_\Theta \ge 0.5 \text{ and } y=0 \text{ or } h_\Theta \lt 0.5 \text{ and } y=1\ 0\quad \text{otherwise}\end{cases}<br>$$</p><p>$$<br>\text{Test Error} = \frac{1}{m_{test}}\Sigma_{i=1}^{m_{test}}err(h_\Theta(x_{test}^{(i)}), y_{test}^{(i)})<br>$$</p></li></ul></li></ul><h5 id="1-2-Model-Selection-and-Train-Validation-Test-Sets"><a href="#1-2-Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="1.2 Model Selection and Train/Validation/Test Sets"></a>1.2 Model Selection and Train/Validation/Test Sets</h5><ul><li><p>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could overfit and as a result the predication on the test set would be poor. The error of the hypothesis as measured on the dataset with which you trained the parameters will be lower than the error on any other data set.</p></li><li><p>Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of the hypothesis, test each degree of polynomial and look at the error result.</p></li><li><p>One way to break down the dataset into the three sets is:</p><ul><li>Training set: $60%$</li><li>Cross validation set: $20%$</li><li>Test set: $20%$</li></ul></li><li><p>Calculate three separate error values for the three different sets using the following method:</p><ol><li>Optimize the parameters in $\Theta$ using the training set for each polynomial degree.</li><li>Find the polynomial degree $d$ with the least error using the cross validation set.</li><li>Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);</li></ol></li><li><p>This way, the degree of the polynomial $d$ has not been trained using the test set.</p></li></ul><h4 id="2-Bias-vs-Variance"><a href="#2-Bias-vs-Variance" class="headerlink" title="2. Bias vs. Variance"></a>2. Bias vs. Variance</h4><h5 id="2-1-Diagnosing-Bias-vs-Variance"><a href="#2-1-Diagnosing-Bias-vs-Variance" class="headerlink" title="2.1 Diagnosing Bias vs. Variance"></a>2.1 Diagnosing Bias vs. Variance</h5><ul><li><p>Need to distinguish whether <strong>bias</strong> or <strong>variance</strong> is the problem contributing to bad predictions.</p></li><li><p>High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.</p></li><li><p>The training error will tend to <strong>decrease</strong> as we increase the degree $d$ of the polynomial.</p><p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase $d$ up to a point, and then it will <strong>increase</strong> as $d$ is increased, forming a convex curve.</p></li><li><p><strong>High bias (underfitting)</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$</p></li><li><p><strong>High variance (overfitting)</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$</p></li><li><p>Summurized in the figure below:</p><img src="/2021/06/29/Advice-for-applying-machine-learning/BiasAndVariance.png" style="zoom:50%;"></li></ul><h5 id="2-2-Regularization-and-Bias-Variance"><a href="#2-2-Regularization-and-Bias-Variance" class="headerlink" title="2.2 Regularization and Bias/Variance"></a>2.2 Regularization and Bias/Variance</h5><ul><li><img src="/2021/06/29/Advice-for-applying-machine-learning/RegularizationAndBias:Variance.png" style="zoom:50%;"></li><li><p>In the figure above, as $\lambda$ increases, the fit becomes more rigid. On the other hand, as $\lambda$ approaches $0$, the model tends to overfit the data.</p></li><li><p>In order to choose the model and the regularization term $\lambda$, need to:</p><ol><li>Create a list of lambdas (i.e. $\lambda \in {0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24}$);</li><li>Create a set of models with different degrees or any other variants.</li><li>Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$.</li><li>Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ <strong>without</strong> regularization or $\lambda = 0$.</li><li>Select the best combo that produces the lowest error on the cross validation set.</li><li>Using the best combo $\Theta$ and $\lambda$, apply in on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.</li></ol></li></ul><h5 id="2-3-Learning-Curves"><a href="#2-3-Learning-Curves" class="headerlink" title="2.3 Learning Curves"></a>2.3 Learning Curves</h5><ul><li>Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence<ul><li>As the training set gets larger, the error for a quadratic function increases.</li><li>The error value will plateau out after a certain $m$, or training set size.</li></ul></li><li><strong>Experiencing high bias</strong>:<ul><li><strong>Low training set size</strong>: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.</li><li><strong>Large traning set size</strong>: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta) \approx J_{CV}(\Theta)$.</li><li>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data will not <strong>(by itself)</strong> help much.</li></ul></li><li><strong>Experiencing high variance</strong>:<ul><li><strong>Low training set size</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.</li><li><strong>Large training set size</strong>: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) \lt J_{CV}(\Theta)$ but the difference between them remains significant.</li><li>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is likely to help.</li></ul></li></ul><h5 id="2-4-Deciding-what-to-do-next-revisited"><a href="#2-4-Deciding-what-to-do-next-revisited" class="headerlink" title="2.4 Deciding what to do next revisited"></a>2.4 Deciding what to do next revisited</h5><ul><li>The decision process can be broken down as follows:<ul><li><strong>Getting more training examples</strong>: Fixes high variance</li><li><strong>Trying smaller sets of features</strong>: Fixes high variance</li><li><strong>Adding features</strong>: Fixes high bias</li><li><strong>Adding polynomial features</strong>: Fixes high bias</li><li><strong>Decreasing $\lambda$</strong>: Fixes high bias</li><li><strong>Increasing $\lambda$</strong>: Fixes high variance</li></ul></li><li><strong>Diagnosing Neural Networks</strong><ul><li>A neural network with fewer parameters is <strong>prone to underfitting</strong>. It is also <strong>Computationally cheaper</strong>.</li><li>A large neural network with more parameters is <strong>prone to overfitting</strong>. It is also <strong>computationally expensive</strong>. In this case, use regularization (increase $\lambda$) to address overfitting.</li><li>Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using cross validation set. Then select the one that performs best.</li></ul></li><li><strong>Model Complexity Effects</strong>:<ul><li>Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</li><li>Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.</li><li>In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Bias/Variance </tag>
            
            <tag> Cross Validation </tag>
            
            <tag> Learning Curves </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="2021/06/22/Support-Vector-Machine/"/>
      <url>2021/06/22/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Optimization-Objective"><a href="#1-Optimization-Objective" class="headerlink" title="1. Optimization Objective"></a>1. Optimization Objective</h4><ul><li><p>$$<br>\min_\theta C \sum_{i=1}^m\left[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})\right] + \frac{1}{2}\sum_{i=1}^n\theta_J^2<br>$$</p></li><li><p>$z = \theta^Tx$</p></li><li><p><img src="/2021/06/22/Support-Vector-Machine/cost1.png" style="zoom:50%;"> <img src="/2021/06/22/Support-Vector-Machine/cost0.png" style="zoom:50%;"></p></li><li><p>if $y = 1$, $\theta^Tx \ge 1$</p><p>if $y=0$, $\theta^Tx\le -1$</p></li></ul><h4 id="2-Decision-Boundary"><a href="#2-Decision-Boundary" class="headerlink" title="2. Decision Boundary"></a>2. Decision Boundary</h4><ul><li><p>$$<br>\begin{align*}<br>&amp; \min_\theta \frac{1}{2}\sum_{i=1}^n\theta_j^2\<br>&amp; s.t.\quad \theta^Tx^{(i)} \ge 1\quad \text{if}\ y^{(i)} = 1\<br>&amp; \quad \quad\ \ \ \theta^Tx^{(i)} \le -1\quad \text{if}\ y^{(i)} = 0<br>\end{align*}<br>$$</p></li><li><p>Linearly Separable case</p><img src="/2021/06/22/Support-Vector-Machine/LinearSeparable.png" style="zoom:50%;"></li><li><img src="/2021/06/22/Support-Vector-Machine/LinearSeparable1.png" style="zoom:50%;"></li></ul><h4 id="3-Kernel"><a href="#3-Kernel" class="headerlink" title="3. Kernel"></a>3. Kernel</h4><ul><li><p>Given $x$, compute new feature depending on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)}, \cdots$</p></li><li><p>$f_1 = similarity(x, l^{(1)}) = \exp\left(-\frac{\Vert x-l^{(1)}\Vert^2}{2\sigma^2}\right)$</p><p>if $x \approx l^{(1)}: f_1 \approx \exp\left(-\frac{0^2}{2\sigma^2}\right) \approx 1$</p><p>If $x$ is far from $l^{(1)}: f_1 = \exp\left(-\frac{(\text{large number})^2}{2\sigma^2}\right) \approx 0$</p></li><li><p><strong>SVM with Kernels</strong></p><ul><li><p>Given $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})$</p></li><li><p>Choose $l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, \cdots, l^{(m)} = x^{(m)}$</p></li><li><p>Given example $x$:</p><ul><li>$f_1 = similarity(x, l^{(1)})$</li><li>$f_2 = similarity(x, l^{(2)})$</li><li>$\cdots$</li></ul></li><li><p>For training example $(x^{(i)}, y^{(i)})$</p><ul><li>$x^{(i)}\rightarrow {f_1^{(i)} = sim(x^{(i)}, l^{(1)})\ \vdots \ f_m^{(i)} = sim(x^{(i)}), l^{(m)}}$</li><li>$f^{(i)} = x^{(i)}\f_0^{(i)} = 1$</li></ul></li><li><p>Hypothesis: Given $x$, compute features $f\in R^{m+1}$</p><ul><li><p>Predict “$y=1$” if $\theta^Tf \ge 0$</p></li><li><p>Training<br>$$<br>\min_\theta C \sum_{i=1}^m\left[y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)})\right] + \frac{1}{2}\sum_{i=1}^n\theta_J^2<br>$$</p></li></ul></li></ul></li></ul><h4 id="4-SVM-parameters"><a href="#4-SVM-parameters" class="headerlink" title="4. SVM parameters"></a>4. SVM parameters</h4><ul><li>$C(=\frac{1}{\lambda})$<ul><li>Large $C$: Lower bias, high variance.</li><li>Small $C$: Higher bias, low variance.</li></ul></li><li>$\sigma^2$<ul><li>Large $\sigma^2$: Features $f_i$ vary more smoothly. Higher bias, lower variance.</li><li>Small $\sigma^2$: Features $f_i$ vary less smoothly. Lower bias, higher variance.</li></ul></li></ul><h4 id="5-Logistic-regression-vs-SVMs"><a href="#5-Logistic-regression-vs-SVMs" class="headerlink" title="5. Logistic regression vs. SVMs"></a>5. Logistic regression vs. SVMs</h4><p>$n = \text{number of features }(x\in R^{n+1}), m = \text{number of training examples}$</p><ul><li>If $n$ is large (relative to $m$):<ul><li>Use logistic regression, or SVM without a kernel (“linear kernel”)</li></ul></li><li>If $n$ is small, $m$ is intermediate:<ul><li>Use SVM with Gaussian kernel</li></ul></li><li>If $n$ is small, $m$ is large:<ul><li>Create/add more features, then use logistic regression or SVM without a kernel</li></ul></li><li>Neural network likely to work well for most of these settings, but may be slower to train.</li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network</title>
      <link href="2021/06/16/Neural-Network/"/>
      <url>2021/06/16/Neural-Network/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Model-Representation"><a href="#1-Model-Representation" class="headerlink" title="1. Model Representation"></a>1. Model Representation</h4><ul><li><p>Input: features $x_1\cdots x_n$</p></li><li><p>Output: the result of the hypothesis function.</p></li><li><img src="/2021/06/16/Neural-Network/Neural Network Rep1.png" style="zoom:50%;"></li><li><p>$x_0$ input node is called the “bias unit”. It is always equal to $1$.</p></li><li><p>The same logistic function as in classification, $\frac{1}{1+e^{-\theta^Tx}}$, which is also called sigmoid (logistic) <strong>activation</strong> function.</p></li><li><p>“theta” parameters are called “weights”.</p></li><li><p>A simplistic representation:<br>$$<br>[x_0x_1x_2]\rightarrow [\ ]\rightarrow h_\theta(x)<br>$$</p></li><li><p>Inpur nodes (layer 1), known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p></li><li><p>The intermediate layers of nodes between the input and output layers called the “hidden layers”.</p></li><li><img src="/2021/06/16/Neural-Network/Neural Network Rep2.png" style="zoom:50%;"></li><li><p>We label these intermediate or “hidden” layer nodes $a_0^2\cdots a_n^2$ and call them “activation units”.</p></li><li><p>$$<br>\begin{align*}<br>&amp; a_i^{(j)} = \text{activation}\ \text{of unit}\ i\ \text{in layer}\ j\<br>&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer}\ j \text{ to layer}\ j+1<br>\end{align*}<br>$$</p></li><li><p>$$<br>[x_0x_1x_2x_3]\rightarrow \left[a_1^{(2)}a_2^{(2)}a_3^{(2)}\right]\rightarrow h_\theta(x)<br>$$</p></li><li><p>The values for each of the “activation” nodes is obtained as follows:<br>$$<br>\begin{align*}<br>&amp;&amp; a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_2 + \Theta_{12}^{(1)}x_1 + \Theta_{13}^{(1)}x_3)\<br>&amp;&amp; a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_2 + \Theta_{22}^{(1)}x_1 + \Theta_{23}^{(1)}x_3)\<br>&amp;&amp; a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_2 + \Theta_{32}^{(1)}x_1 + \Theta_{33}^{(1)}x_3)\<br>&amp;&amp; h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})<br>\end{align*}<br>$$</p></li><li><p>The dimensions of matrices of weights is determined as follows:</p><p><strong>If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1}\times (s_j+1)$</strong>.</p><p>The $+1$ comes from the addition in $\Theta^{(j)}$ of the “bias nodes”, $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will.</p></li></ul><h4 id="2-Vectorized-Representation"><a href="#2-Vectorized-Representation" class="headerlink" title="2. Vectorized Representation"></a>2. Vectorized Representation</h4><ul><li><p>Setting $x = a^{(1)}$</p><p>$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$</p></li><li><p>Multiply matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$ (where $s_j$ is the number of activation nodes) by vector $a^{(j-1)}$ with height $n+1$. This gives vector $z^{(j)}$ with height $s_j$. Now the vector of activation nodes for layer $j$ as follows:<br>$$<br>a^{(j)} = g(z^{(j)})<br>$$<br>where function $g$ can be applied element-wise to vector $z^{(j)}$</p></li><li><p>Add a bias unit (equal to $1$) to layer $j$ after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to $1$.</p></li><li><p>Final result $h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})$</p></li></ul><h4 id="3-Cost-Function"><a href="#3-Cost-Function" class="headerlink" title="3. Cost Function"></a>3. Cost Function</h4><ul><li><p>First define a few variables need to use:</p><ul><li>$L = \text{total number of layers in the network}$</li><li>$s_l = \text{number of units (not counting bias unit) in layer } l$</li><li>$K = \text{number of output units/classes}$</li></ul></li><li><p>In neural networks, we may have many output nodes. Denote $h_\theta(x)_k$ as being a hypothesis that results in the $k^{th}$ output.</p></li><li><p>The cost function for neural networks is going to be generalization of the one used for logistic regression.<br>$$<br>J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k-1}^{K} \left[y_{k}^{(i)} \log((h_\Theta(x^{(i)}))<em>k) + (1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))<em>k)\right] + \frac{\lambda}{2m} \sum</em>{l=1}^{L-1} \sum</em>{i=1}^{s_l} \sum_{j=1}^{s_l+1}(\Theta_{j, i}^{(i)})^2<br>$$<br>We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.</p></li><li><p>In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.</p></li><li><p>Note:</p><ul><li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer.</li><li>the triple sum simply adds up the squares of all the individual $\Theta$s in the entire network.</li><li>the $i$ in the triple sum does <strong>not</strong> refer to training example $i$.</li></ul></li></ul><h4 id="4-Backpropagation-Algorithm"><a href="#4-Backpropagation-Algorithm" class="headerlink" title="4. Backpropagation Algorithm"></a>4. Backpropagation Algorithm</h4><ul><li><p>“Backpropagation” is neural-network terminology for minimizing the cost function.</p></li><li><p>Compute the partial derivative of $J(\Theta)$:<br>$$<br>\frac{\partial}{\partial \Theta_{i, j}^{(l)}}J(\Theta)<br>$$</p></li><li><p>Procedure:</p><ul><li>Given training examples set ${(x^{(1)}, y^{(1)})\cdots(x^{(m)}, y^{(m)})}$</li><li>Set $\Delta_{ij}^{(l)} = 0$ for all $(l, i, j)$</li><li>For $i = 1$ to $m$:<ul><li>Set $a^{(1)} = x^{(i)}$</li><li>Perform forward propagation to compute $a^{(l)}$ for $l = 2,3,\cdots, L$</li><li>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$</li><li>Compute $\delta^{(L-1)}, \delta^{(L-2)}, \cdots, \delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T\delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1-a^{(l)})$<ul><li>The delta values of layer $l$ are calculated by multiplying the delta values in the next layer with the theta matrix of layer $l$. We then element-wise multiply that with a function called $g^{\prime}$, or g-prime, which is the derivative of the activation function $g$ evaluated with the input values given by $z^{(l)}$.</li></ul></li><li>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$</li></ul></li><li>$D_{ij}^{(l)} := \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)} \text{ if } j \ne 0$</li><li>$D_{ij}^{(l)} := \Delta_{ij}^{(l)} \text{ if } j = 0$</li></ul></li><li><p>Intuitively, $\delta_j^{(l)}$ is the “error” for $a_j^{(l)}$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function: $\delta_j^{(l)} = \frac{\partial}{\partial z_j^{(l)}}cost(t)$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Overfitting</title>
      <link href="2021/06/09/Overfitting/"/>
      <url>2021/06/09/Overfitting/</url>
      
        <content type="html"><![CDATA[<h4 id="1-The-Problem-of-Overfitting"><a href="#1-The-Problem-of-Overfitting" class="headerlink" title="1. The Problem of Overfitting"></a>1. The Problem of Overfitting</h4><ul><li><strong>Underfitting</strong>, or high bias, is when the form of our hypothesis function $h$ maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.</li><li>At the other extreme, <strong>overfitting</strong>, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</li><li>Two main options to address the issue of overfitting:<ul><li>Reduce the number of features:<ul><li>Manually select which features to keep.</li><li>Use a model selection algorithm.</li></ul></li><li>Regularization<ul><li>Keep all the features, but reduce the magnitude of parameters $\theta_j$.</li><li>Regularization works well when we have a lot of slightly useful features.</li></ul></li></ul></li></ul><h4 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h4><ul><li><p>If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.</p></li><li><p>$$<br>\min_\theta\frac{1}{2m}\Sigma_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \Sigma_{j=1}^n \theta_j^2<br>$$</p></li><li><p>The $\lambda$, or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.</p></li><li><p>Using the above cost function with extra summation, we can smooth the output of our hypothesis function to reduce overfitting.</p></li><li><p>If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p></li></ul><h4 id="3-Regularized-Linear-Regression"><a href="#3-Regularized-Linear-Regression" class="headerlink" title="3. Regularized Linear Regression"></a>3. Regularized Linear Regression</h4><ul><li><p><strong>Gradient Descent</strong><br>$$<br>\begin{align*}<br>&amp; Repeat\ {\<br>&amp; \quad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\<br>&amp; \quad\theta_j := \theta_j - \alpha\left[ \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\right) + \frac{\lambda}{m}\theta_j\right] \quad\quad j\in{1, 2\cdots n}\<br>&amp; }<br>\end{align*}<br>$$</p></li><li><p>The term $\frac{\lambda}{m}\theta_j$ performs regularization. With some manipulation, the update rule can also be represented as:<br>$$<br>\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\Sigma_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$<br>The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than $1$. Intuitively reduce the value of $\theta_j$ by some amount on every update.</p></li><li><p><strong>Normal Equation</strong><br>$$<br>\theta = (X^TX+\lambda L)^{-1}X^Ty\quad<br>where\ L = \begin{bmatrix} 0 \ \ &amp; 1 \ \ &amp;\ &amp; 1 \ \ &amp; \ &amp; \ &amp; \ddots \ \ &amp; \ &amp; \ &amp;\ &amp; 1\end{bmatrix}<br>$$<br>$L$ is a matrix with $0$ at the top left and $1$’s down the diagonal, with $0$’s everywhere else. It should have dimension $(n+1)\times (n+1)$. Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number $\lambda$.</p></li><li><p>When we add the term $\lambda L$, then $X^TX + \lambda L$ becomes invertible.</p></li></ul><h4 id="4-Regularized-Logistic-Regression"><a href="#4-Regularized-Logistic-Regression" class="headerlink" title="4. Regularized Logistic Regression"></a>4. Regularized Logistic Regression</h4><ul><li><p>Cost function<br>$$<br>J(\theta) = -\frac{1}{m}\Sigma_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i )}))] + \frac{\lambda}{m}\Sigma_{j=1}^n\theta_j^2<br>$$<br>The second sum, $\Sigma_{j=1}^n\theta_j^2$ <strong>means to explicitly exclude</strong> the bias term $\theta_0$.</p></li><li><p><strong>Gradient Descent</strong><br>$$<br>\begin{align*}<br>&amp; Repeat\ {\<br>&amp; \quad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\<br>&amp; \quad\theta_j := \theta_j - \alpha\left[ \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\right) + \frac{\lambda}{m}\theta_j\right] \quad\quad j\in{1, 2\cdots n}\<br>&amp; }<br>\end{align*}<br>$$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Classification </tag>
            
            <tag> Overfitting </tag>
            
            <tag> Regression </tag>
            
            <tag> Regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression -- Classification</title>
      <link href="2021/06/08/Logistic-Regression-Classification/"/>
      <url>2021/06/08/Logistic-Regression-Classification/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Classification"><a href="#1-Classification" class="headerlink" title="1. Classification"></a>1. Classification</h4><ul><li>The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.</li><li><strong>Binary classification problem</strong><ul><li>$y$ can take on only two values, $0$ and $1$. (Multi-class later)</li><li>$0$ is called the negative class, and $1$ the positive class, and they are sometimes also denoted by the symbols “-“ and “+”.</li><li>Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the <strong>label</strong> for the training example.</li></ul></li></ul><h4 id="2-Hypothesis-Representation"><a href="#2-Hypothesis-Representation" class="headerlink" title="2. Hypothesis Representation"></a>2. Hypothesis Representation</h4><ul><li><p>By using the “Sigmoid Function”, also called the “Logistic Function”:<br>$$<br>\begin{align*}<br>&amp; h_{\theta}(x) = g(\theta^Tx)\<br>&amp; z = \theta^Tx\<br>&amp; g(z) = \frac{1}{1+e^{-z}}<br>\end{align*}<br>$$</p></li><li><p>The function $g(z)$, maps any real number to the $(0,1)$ interval, making it useful for transforming an arbitrary-valued function into a function suited for classification.</p></li><li><p>$h_{\theta}(x)$ will give us the <strong>probability</strong> that our output is $1$. The probability that the prediction is $0$ is just the complement of the probability that it is $1$.<br>$$<br>\begin{align*}<br>&amp; h_{\theta}(x) = P(y=1\mid x;\theta) = 1 - P(y=0\mid x;\theta)\<br>&amp; P(y=0\mid x;\theta) + P(y=1\mid x;\theta) = 1<br>\end{align*}<br>$$</p></li></ul><h4 id="3-Decision-Boundary"><a href="#3-Decision-Boundary" class="headerlink" title="3. Decision Boundary"></a>3. Decision Boundary</h4><ul><li><p>$$<br>\theta^Tx\ge 0 \Rightarrow y=1\<br>\theta^Tx\lt 0 \Rightarrow y=0<br>$$</p></li><li><p>The <strong>decision boundary</strong> is the line that separates the area where $y = 1$ and where $y = 0$. It is created by the hypothesis function.</p></li><li><p>The input to the sigmoid function $g(z)$ (e.g. $\theta^TX$) doesn’t need to be linear, and could be a function that describes a circle or any shape to fit our data.</p></li></ul><h4 id="4-Cost-Function"><a href="#4-Cost-Function" class="headerlink" title="4. Cost Function"></a>4. Cost Function</h4><ul><li><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p></li><li><p>Instead, our cost function for logistic regression looks like:<br>$$<br>\begin{align*}<br>&amp; J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})\<br>&amp; Cost(h_\theta(x), y) = -\log(h_\theta(x))\quad\quad if\ y = 1\<br>&amp; Cost(h_\theta(x), y) = -\log(1-h_\theta(x))\ if\ y = 0<br>\end{align*}<br>$$</p></li><li><p>$$<br>\begin{align*}<br>&amp; Cost(h_{\theta}(x), y) = 0\ if\ h_\theta(x) = y\<br>&amp; Cost(h_\theta(x), y) \rightarrow \infin\ if\ y = 0\ and\ h_\theta(x)\rightarrow 1\<br>&amp; Cost(h_\theta(x), y) \rightarrow \infin\ if\ y = 1\ and\ h_\theta(x) \rightarrow 0<br>\end{align*}<br>$$</p></li><li><p>If the correct answer ‘$y$’ is $0$, then the cost function will be $0$ if our hypothesis function also ouputs $0$. If the hypothesis approaches $1$, then the cost function will approach infinity. If the correct answer ‘$y$’ is $1$, the case is reverse.</p></li><li><p>Not that writing the cost function in this way guarantees that $J(\theta)$ is convex for logistic regression.</p></li></ul><h4 id="5-Simplified-Cost-Function"><a href="#5-Simplified-Cost-Function" class="headerlink" title="5. Simplified Cost Function"></a>5. Simplified Cost Function</h4><ul><li><p>Compress the cost function’s two conditional cses into one case:<br>$$<br>Cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))<br>$$</p></li><li><p>Cost function<br>$$<br>J(\theta) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))]<br>$$</p></li><li><p>A vectorized implementation:<br>$$<br>h = g(X\theta)\<br>J(\theta) = \frac{1}{m}(-y^T\log(h) - (1-y)^T\log(1-h))<br>$$</p></li></ul><h4 id="6-Gradient-Descent"><a href="#6-Gradient-Descent" class="headerlink" title="6. Gradient Descent"></a>6. Gradient Descent</h4><ul><li><p>$$<br>\begin{align*}<br>&amp; Repeat\ { \<br>&amp; \theta_j := \theta_j - \frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\<br>&amp; }<br>\end{align*}<br>$$</p></li><li><p>A vectorized implementation:<br>$$<br>\theta := \theta - \frac{\alpha}{m}X^T(g(X\theta) - \vec{y})<br>$$</p></li></ul><h4 id="7-Advanced-Optimization"><a href="#7-Advanced-Optimization" class="headerlink" title="7. Advanced Optimization"></a>7. Advanced Optimization</h4><ul><li>“Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize $\theta$ that can be used instead of gradient descent.</li></ul><h4 id="8-Multiclass-Classification-One-vs-all"><a href="#8-Multiclass-Classification-One-vs-all" class="headerlink" title="8. Multiclass Classification: One-vs-all"></a>8. Multiclass Classification: One-vs-all</h4><ul><li><p>Instead of $y = {0, 1}$, we will expand the definition so that $y = {0, 1\cdots n}$.</p></li><li><p>Since $y = {0, 1\cdots n}$, we divide the problem into $n+1$ binary classification problems; in each one, predict the probability that ‘$y$’ is a member of one of our classes.</p></li><li><p>$$<br>\begin{align*}<br>&amp; y\in {0, 1\cdots n}\<br>&amp; h_\theta^{(0)}(x) = P(y = 0\mid x;\theta)\<br>&amp; h_\theta^{(1)}(x) = P(y = 1\mid x;\theta)\<br>&amp; \cdots\<br>&amp; h_\theta^{(n)}(x) = P(y = n\mid x;\theta)\<br>&amp; prediction = \max_{i}(h_\theta^{(i)}(x))<br>\end{align*}<br>$$</p></li><li><p>We are basically choosing one class and then lumping all the others into a single second class. Do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Priority Queue</title>
      <link href="2021/06/02/Priority-Queue/"/>
      <url>2021/06/02/Priority-Queue/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A priority queue is an abstract data type for storing a collection of prioritized elements that supports<ul><li>arbitrary element insertion,</li><li>removal of elements in order of priority (the element with the first priority can be removed at any time)</li></ul></li></ul><h4 id="2-Priority-Queue-ADT"><a href="#2-Priority-Queue-ADT" class="headerlink" title="2. Priority Queue ADT"></a>2. Priority Queue ADT</h4><ul><li>A priority queue stores a collection of entries</li><li>Each entry is a pair (key, value)</li><li><strong>Entry ADT</strong><ul><li>An <em>entry</em> in a priority queue is simply a key-value pair</li><li>Priority queues store entries to allow for efficient insertion and removal based on keys</li></ul></li></ul><h4 id="3-Sequence-based-Priority-Queue"><a href="#3-Sequence-based-Priority-Queue" class="headerlink" title="3. Sequence-based Priority Queue"></a>3. Sequence-based Priority Queue</h4><ul><li>Implementation with an unsorted list<ul><li><em>insert</em> takes $O(1)$ time since we can insert the item at the beginning or end of the sequence</li><li><em>removeMin</em> and <em>min</em> take $O(n)$ time since we have to traverse the entire sequence to find the smallest key</li></ul></li><li>Implementation with a sorted list<ul><li><em>insert</em> takes $O(n)$ time since we have to find the place where to insert the item</li><li><em>removeMin</em> and <em>min</em> take $O(1)$ time, since the smallest key is at the beginning</li></ul></li></ul><h4 id="4-Priority-Queue-Sorting"><a href="#4-Priority-Queue-Sorting" class="headerlink" title="4. Priority Queue Sorting"></a>4. Priority Queue Sorting</h4><ul><li>Use a priority queue to sort a list of comparable elements<ul><li>Insert the elements one by one with a series of <em>insert</em> operations</li><li>Remove the elements in sorted order with a series of <em>removeMin</em> operations</li></ul></li><li>Selection-Sort<ul><li>Selection-sort is the variation of PQ-sort where the prioroty queue is implemented with an unsorted sequence</li><li>Running time of Selection-sort:<ul><li>Inserting the elements into the priority queue with $n$ insert operations takes $O(n)$ time</li><li>Removing the elements in sorted order from the priority queue with $n$ <em>removeMin</em> operations takes time proportional to $1+2+\cdots + n$</li></ul></li><li>Selection-sort runs in $O(n^2)$ time</li></ul></li><li>Insertion-Sort<ul><li>Insertion-sort is the variation of PQ-sort where the priority queue is implemented with a sorted sequence</li><li>Running time of Insertion-sort:<ul><li>Inserting the elements into the priority queue with $n$ <em>insert</em> operations takes time proportional to $1+2+\cdots +n$</li><li>Removing the elements in sorted order from the priority queue with a series of $n$ <em>removeMin</em> operations takes $O(n)$ time</li></ul></li><li>Insertion-sort runs in $O(n^2)$ time</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="2021/06/01/Linear-Regression/"/>
      <url>2021/06/01/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h5 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h5><ul><li>$x^{(i)}$ to denote the “input” variables, also called input features.</li><li>$y^{(i)}$ to denote the “output” or target variable that we are trying to predict.</li><li>A pair $(x^{(i)}, y^{(i)})$ is called a training example.</li><li>The dataset that we’ll be using to learn - a list of $m$ training examples $(x^{(i)}, y^{(i)}); i = 1, \cdots, m$ - is called a training set.</li><li>Note that the superscript “$(i)$” in the notation is simply an index into the training set, and has nothing to do with exponentiation.</li><li>$X$ to denote the space of input values.</li><li>$Y$ to denote the space of output values.</li></ul><h4 id="1-Model-Representation"><a href="#1-Model-Representation" class="headerlink" title="1. Model Representation"></a>1. Model Representation</h4><ul><li>Given a training set, to learn a function $h: X\rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. $h$ is called a hypothesis.</li><li>When the target variable that we’re trying to predict is continuous, we call the learning problem a regression problem.</li></ul><h4 id="2-Cost-Function-Loss-Function"><a href="#2-Cost-Function-Loss-Function" class="headerlink" title="2. Cost Function (Loss Function)"></a>2. Cost Function (Loss Function)</h4><ul><li><p>This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from $x$’s and the actual output $y$’s.</p></li><li><p>$$<br>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y_i} - y_i)^2 = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2<br>$$</p></li><li><p>This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $(\frac{1}{2})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\frac{1}{2}$ term.</p></li><li><p>As a goal, we should try to minimize the cost function.</p></li></ul><h4 id="3-Gradient-Descent"><a href="#3-Gradient-Descent" class="headerlink" title="3. Gradient Descent"></a>3. Gradient Descent</h4><ul><li><p>The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter $\alpha$, which is called the learning rate.</p></li><li><p>A smaller $\alpha$ would result in a smaller step and a larger $\alpha$ results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J(\theta_0, \theta_1)$.</p></li><li><p>The gradient descent algorithm is:</p><ul><li><p>repeat until convergence:<br>$$<br>\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)<br>$$<br>where</p><p>$j = 0, 1$ represents the feature index number.</p></li></ul></li><li><p>At each iterarion $j$, one should simultaneously update the parameters $\theta_1, \theta_2, \cdots, \theta_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation.</p></li><li><p>We should adjust our parameter $\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p></li><li><p>Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed. As we approach a local optimum, gradient descent will automatically take smaller steps. So, no need to decrese $\alpha$ over time.</p></li></ul><h4 id="4-Multiple-Features"><a href="#4-Multiple-Features" class="headerlink" title="4. Multiple Features"></a>4. Multiple Features</h4><ul><li><p>Linear regression with multiple variables is also known as <strong>“multivariate linear regression”</strong>.</p></li><li><p>$x^{(i)}_j = $ value of feature $j$ in the $i^{th}$ training example</p></li><li><p>$x^{(i)} = $ the input (features) of the $i^{th}$ training example</p></li><li><p>$m = $ the number of training examples</p></li><li><p>$n = $ the number of features</p></li><li><p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:<br>$$<br>h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \cdots + \theta_nx_n<br>$$</p></li><li><p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:<br>$$<br>h_{\theta}(x) = \begin{bmatrix}\theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n\end{bmatrix} \begin{bmatrix}x_0\x_1\ \vdots\ x_n\end{bmatrix} = \theta^Tx<br>$$</p></li></ul><h4 id="5-Gradient-Descent-for-Multiple-Variables"><a href="#5-Gradient-Descent-for-Multiple-Variables" class="headerlink" title="5. Gradient Descent for Multiple Variables"></a>5. Gradient Descent for Multiple Variables</h4><ul><li>repeat until convergence:<br>$$<br>\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\times x_j^{(i)}\quad for\ j := 0\cdots n<br>$$</li></ul><h4 id="6-Gradient-Descent-in-Practice-Feature-Scaling"><a href="#6-Gradient-Descent-in-Practice-Feature-Scaling" class="headerlink" title="6. Gradient Descent in Practice - Feature Scaling"></a>6. Gradient Descent in Practice - Feature Scaling</h4><ul><li><p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p></li><li><p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally: $-1 \le x_{(i)} \le 1$ or $-0.5 \le x_{(i)} \le 0.5$. These aren’t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p></li><li><p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown this formula:<br>$$<br>x_i := \frac{x_i - \mu_i}{s_i}<br>$$<br>Where $\mu_i$ is the <strong>average</strong> of all the values for feature $(i)$ and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.</p></li></ul><h4 id="7-Gradient-Descent-in-Practice-Learning-Rate"><a href="#7-Gradient-Descent-in-Practice-Learning-Rate" class="headerlink" title="7. Gradient Descent in Practice - Learning Rate"></a>7. Gradient Descent in Practice - Learning Rate</h4><ul><li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent. If $J(\theta)$ ever increases, then you probably need to decrease $\alpha$.</li><li><strong>Automatic convergence test.</strong> Declare convergence if $J(\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it’s difficult to choose this threshold value.</li><li>It has been proven that if learning rate $\alpha$ is sufficiently small, then $J(\theta)$ will decrease on every iteration.</li><li>To summarize:<ul><li>If $\alpha$ is too small: slow convergence.</li><li>If $\alpha$ is too large: may not decrease on every iteration and thus may not converge.</li></ul></li></ul><h4 id="8-Features-and-Polynomial-Regression"><a href="#8-Features-and-Polynomial-Regression" class="headerlink" title="8. Features and Polynomial Regression"></a>8. Features and Polynomial Regression</h4><ul><li>We can improve our features and the form of our hypothesis function in a couple different ways.</li><li>We can <strong>combine</strong> multiple features into one.</li><li><strong>Polynomial Regression</strong><ul><li>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</li><li>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</li><li>For example, if our hypothesis function is $h_{\theta}(x) = \theta_0 + \theta_1 x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$.</li><li>One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.</li></ul></li></ul><h4 id="9-Normal-Equation"><a href="#9-Normal-Equation" class="headerlink" title="9. Normal Equation"></a>9. Normal Equation</h4><ul><li><p>A second way of minimizing $J$.</p></li><li><p>In the “Normal Equation” method, we will minimize $J$ by explicitly taking its derivatives with respect to the $\theta_j$’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$</p></li><li><p>There is <strong>no need</strong> to do feature scaling with the normal equation.</p></li><li><p>The comparison of gradient descent and the normal equation:</p><img src="/2021/06/01/Linear-Regression/Screen Shot 2021-06-04 at 10.02.03 PM.png" style="zoom:50%;"></li><li><p>With the normal equation, computing the inversion has complexity $O(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, when $n$ exceeds $10,000$ it might be a good time to go from a normal solution to an iterative process.</p></li><li><p><strong>Noninvertibility</strong></p><ul><li>If $X^TX$ is <strong>noninvertible</strong>, the common causes might be having:<ul><li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li><li>Too many features (e.g. $m\le n$). In this case, delete some features or use “regularization”.</li></ul></li><li>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Red-Black Tree</title>
      <link href="2021/05/26/Red-Black-Tree/"/>
      <url>2021/05/26/Red-Black-Tree/</url>
      
        <content type="html"><![CDATA[<h2 align="center">Red-Black Trees</h2><h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A red-black tree is a binary search tree that satisfies the following properties:<ul><li><strong>Root Property</strong>: the root is black</li><li><strong>External Property</strong>: every leaf is black</li><li><strong>Internal/Red Property</strong>: the children of a red node are black</li><li><strong>Depth Property</strong>: all the leaves have the same <em>black depth</em>, defined as the number of proper ancestors that are black</li></ul></li></ul><h4 id="2-From-Red-Black-to-2-4-Trees"><a href="#2-From-Red-Black-to-2-4-Trees" class="headerlink" title="2. From Red-Black to $(2,4)$ Trees"></a>2. From Red-Black to $(2,4)$ Trees</h4><ul><li>Given a red-black tree, we can construct a corresponding $(2,4)$ tree:<ul><li>merge every red node $w$ into its parent, storing the entry from $w$ at its parent</li><li>the children of $w$ become ordered children of the parent</li></ul></li><li>Depth Property:<ul><li>$(2,4)$ Tree: all the external nodes have the same depth</li><li>Red-Black Tree: all the leaves have the same <em>black depth</em></li></ul></li><li>A red-black tree is a representation of a $(2,4)$ tree by means of a binary tree whose nodes are colored red or black</li><li>In comparison with its associated $(2,4)$ tree, a red-black tree has<ul><li>same logarithmic time performance</li><li>simpler implementation with a single node type</li></ul></li></ul><h4 id="3-Height-of-a-Red-Black-Tree"><a href="#3-Height-of-a-Red-Black-Tree" class="headerlink" title="3. Height of a Red-Black Tree"></a>3. Height of a Red-Black Tree</h4><ul><li>Theorem: A red-black tree storing $n$ items has height $O(\log n)$<ul><li>Proof:<ul><li>The height of a red-black tree is at most twice the height of its associated $(2,4)$ tree, which is $O(\log n)$</li></ul></li></ul></li></ul><h4 id="4-Search"><a href="#4-Search" class="headerlink" title="4. Search"></a>4. Search</h4><ul><li>The search algorithm for a red-black tree is the same as that for a binary search tree</li><li>Searching in a red-black tree takes $O(\log n)$ time</li></ul><h4 id="5-Insertion"><a href="#5-Insertion" class="headerlink" title="5. Insertion"></a>5. Insertion</h4><ul><li><p>To insert $(k, o)$, we execute the insertion algorithm for binary search trees and color <strong>red</strong> the newly inserted node $z$ <em>unless it is the root</em></p><ul><li>We preserve the root, external, and depth properties</li><li>If the parent $v$ of $z$ is black, we also preserve the internal property and we are done</li><li>Else ($v$ is red) we have a <strong>double red</strong> (i.e., a violation of the internal property), which requires a reorganization of the tree</li></ul></li><li><p><strong>Remedying a Double Red</strong></p><ul><li>Consider a double red with child $z$ and parent $v$, and let $w$ be the sibling of $v$</li><li>Case1: $w$ is black<ul><li>The double red is an incorrect replacement of a 4-node</li><li><strong>Restructuring</strong>: we change the 4-node replacement</li></ul></li><li>Case2: $w$ is red<ul><li>The double red corresponds to an overflow</li><li><strong>Recoloring</strong>: we perform the equivalent of a <strong>split</strong></li></ul></li></ul></li><li><p><strong>Restructuring</strong></p><ul><li><p>A restructuring remedies a child-parent double red when the parent red node has a black sibling</p></li><li><p>It is equivalent to restoring the <em>correct replacement</em> of a 4-node</p></li><li><p>The internal property is restored and the other properties are preserved</p></li><li><p>There are four restructuring configrations depending on whether the double red nodes are left or right children</p><img src="/2021/05/26/Red-Black-Tree/Screen Shot 2021-05-10 at 7.22.25 PM.png" style="zoom:50%;"></li></ul></li><li><p><strong>Recoloring</strong></p><ul><li>A recoloring remedies a child-parent double red when the parent red node has a red sibling</li><li>The parent $v$ and its sibling $w$ become black and the grandparent $u$ becomes red, unless it is the root</li><li>It is equivalent to performing a split on a 5-node</li><li><em>The double red violation may propagate to the grangparent $u$</em></li></ul></li><li><p>Analysis of Insertion</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Algorithm insert(k, o)1. We search for key k to locate the insertion node z2. We add the new entry (k, o) at node z and color z red3. while doubleRed(z)        if isBlack(sibling(parent(z)))            z <- restructure(z)            return        else sibling(parent(z)) is red            z <- recolor(z)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Recall that a red-black tree has $O(\log n)$ height</p></li><li><p>Step 1 takes $O(\log n)$ time because we visit $O(\log n)$ nodes</p></li><li><p>Step 2 takes $O(1)$ time</p></li><li><p>Step 3 takes $O(\log n)$ time because we perform</p><ul><li>$O(\log n)$ recoloring, each taking $O(1)$ time, and</li><li>at most one restructuring taking $O(1)$ time</li></ul></li><li><p>Thus, an insertion in a red-black tree takes $O(\log n)$ time</p></li></ul></li></ul><h4 id="6-Deletion"><a href="#6-Deletion" class="headerlink" title="6. Deletion"></a>6. Deletion</h4><ul><li>To perform operation $remove(k)$, we first execute the deletion algorithm for binary search trees</li><li>Let $v$ be the internal node removed, $w$ the external node removed, and $r$ the sibling of $w$<ul><li>If $v$ was red, the resulting tree remains a valid red-black tree</li><li>If $v$ was black and $r$ was red, we color $r$ black and we are done</li><li>Else ($v$ and $r$ were both black) we color $r$ <em><strong>double black</strong></em>, to preserve the depth property</li></ul></li><li><strong>Remedying a Double Black</strong><ul><li>The algorithm for remedying a double black node $r$ with sibling $y$ considers three cases</li><li>Case1: $y$ is black and has a red child<ul><li>We perform a <strong>restructuring</strong>, equivalent to a <strong>transfer</strong>, and we are done.</li></ul></li><li>Case2: sibling $y$ of $r$ is black and its children are both black<ul><li>We perform a <strong>recoloring</strong>, equivalent to a <strong>fusion</strong>, which may propagate up the double black violation</li></ul></li><li>Case3: $y$ is red<ul><li>We perform an <strong>adjustment</strong>, equivalent to choosing a different representation of a 3-node, after which either Case 1 or Case 2 applies</li></ul></li></ul></li><li>Deletion in a red-black tree takes $O(\log n)$ time</li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pattern Matching</title>
      <link href="2021/04/15/Pattern-Matching/"/>
      <url>2021/04/15/Pattern-Matching/</url>
      
        <content type="html"><![CDATA[<h4 id="1-What-is-pattern-matching"><a href="#1-What-is-pattern-matching" class="headerlink" title="1. What is pattern matching?"></a>1. What is pattern matching?</h4><ul><li>Given:<ul><li>A text string of length $n$</li><li>A pattern string of length $m\lt n$</li></ul></li><li>Determine:<ul><li>Whether the pattern is a <em><strong>substring</strong></em> of the text;</li><li>Find all indices at which the pattern begins.</li></ul></li></ul><h4 id="2-Brute-force-algorithm"><a href="#2-Brute-force-algorithm" class="headerlink" title="2. Brute-force algorithm"></a>2. Brute-force algorithm</h4><ul><li>Compare the pattern $P$ with the text $T$ for each possible shift of $P$ relative to $T$, until either<ul><li>A match is found, or</li><li>All placements of the pattern have been tried.</li></ul></li><li>Brute-force pattern matching runs in time $O(nm)$ in worst case.</li></ul><h4 id="3-Boyer-Moore-algorithm"><a href="#3-Boyer-Moore-algorithm" class="headerlink" title="3. Boyer-Moore algorithm"></a>3. Boyer-Moore algorithm</h4><ul><li><p>Boyer-Moore heuristics</p><ul><li><em><strong>Looking-glass heuristic</strong></em>: Compare $P$ with a subsequence of $T$ from  <em><strong>right to left</strong></em>.</li><li><em><strong>Character-jump heuristic</strong></em>: When a mismatch occurs at $T[i] = c$<ul><li>If $P$ contains $c$, shift $P$ to align the <em><strong>last (right-most)</strong></em> occurrence of $c$ in $P$ with $T[i]$</li><li>Else, shift $P$ to align $P[0]$ with $T[i+1]$</li></ul></li></ul></li><li><p>Last-occurrence function</p><ul><li>Objective: speed up the search by <em><strong>look-up table</strong></em>.</li><li>BM algorithm <em><strong>preprocesses the pattern $P$</strong></em> and the alphabet $S$ to build the last-occurrence function $L$ mapping $S$ to integers, where $L(c)$ is defined as<ul><li>The largest index $i$ such that $P[i] = c$ or</li><li>$-1$ if no such index exists.</li></ul></li></ul></li><li><p>The Boyer-Moore algorithm</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Algorithm BoyerMooreMatch(T, P, Alphabet)    L = lastOccurenceFunction(P, Alphabet)    i = m - 1    j = m - 1    repeat        if T[i] == P[j] // matching            if j == 0                return i // match at i            else                i = i - 1                j = j - 1        else // character-jump            l = L[T[i]]            i = i + m - min(j, 1 + l)            j = m - 1 //rightmost    until i > n - 1    return -1 // no match<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>Analysis</p><ul><li>Boyer-Moore’s algorithm runs in time $O(nm)$ in worst case</li><li>The worst case may occur in images and DNA sequences but unlikely in English text.</li><li>Boyer-Moore’s algorithm is <em><strong>significantly faster</strong></em> than the brute-force algorithm on English text.</li></ul></li><li><p>Key innovations of BM algorithm</p><ul><li><em><strong>Pre-indexing mismatched Text characters in Pattern.</strong></em></li></ul></li></ul><h4 id="4-KMP-algorithm"><a href="#4-KMP-algorithm" class="headerlink" title="4. KMP algorithm"></a>4. KMP algorithm</h4><ul><li><p>Key idea of KMP algorithm: precompute <em><strong>self-overlaps</strong></em> between portion of the pattern, so when a mismatch occurs, we know the maximum amount to shift the pattern.</p></li><li><p>Failure function</p><ul><li><p>Indicate the shift of the pattern upon a failed comparison.</p><ul><li>$f(k)$: the length of <em><strong>the longest prefix</strong></em> of the pattern that is a suffix of the substring pattern$[1,2,\cdots,k]$. pattern$[0]$ is excluded as we shift at least one character.</li><li>Mismatch at pattern$[k+1]$, $f(k)$ tells how many preceding characters can be reused to restart the pattern.</li></ul></li><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Algorithm failureFunction(P)    F[0] = 0    i = 1    j = 0    while i < m        if P[i] == P[j]            F[i] = j + 1            i = i + 1            j = j + 1        else if j > 0 then            j = F[j-1]        else            F[i] = 0 //no match            i = i + 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>Time complexity</p><ul><li>$O(|P|+|T|)$.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Heap</title>
      <link href="2020/11/22/Heap/"/>
      <url>2020/11/22/Heap/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A heap is a binary tree storing keys at its nodes and satisfying the following properties:<ul><li><strong>Heap-Order</strong>: for every internal node $v$ other than the root, $key(v) \ge key(parent(v))$</li><li><strong>Complete Binary Tree</strong>: let $h$ be the height of the heap<ul><li>levels $i = 0, \cdots, h-1$ have the maximal number of nodes, i.e. there are $2^i$ nodes at depth $i$.</li><li>The remaining nodes at level/depth $h$ reside in the leftmost possible positions at that level/depth.</li></ul></li><li>The <strong>last node</strong> of a heap is the rightmost node of maximum depth</li></ul></li></ul><h4 id="2-Insertion-into-a-Heap"><a href="#2-Insertion-into-a-Heap" class="headerlink" title="2. Insertion into a Heap"></a>2. Insertion into a Heap</h4><ul><li><p>Method insertItem of the priority queue ADT corresponds to the insertion of a key $k$ to the heap</p></li><li><p>The insertion algorithm consists os three steps</p><ul><li>Find the insertion node $z$ (the new last node)</li><li>Store $k$ at $z$</li><li>Restore the heap-order property</li></ul></li><li><p><strong>Upheap</strong></p><ul><li><p>After the insertion of a new key $k$, the heap-order property may be violated</p></li><li><p>Algorithm up heap restores the heap-order property by swapping $k$ along an upward path from the insertion node</p></li><li><p>Upheap terminates when the key $k$ reaches the root or a node whose parent has a key smaller than or equal to $k$</p></li><li><p>Since a heap has height $O(\log n)$, up heap runs in $O(\log n)$ time</p></li></ul></li></ul><h4 id="3-Removal-from-a-Heap"><a href="#3-Removal-from-a-Heap" class="headerlink" title="3. Removal from a Heap"></a>3. Removal from a Heap</h4><ul><li>Method removeMin of the priority queue ADT corresponds to the removal of the root key from the heap</li><li>The removal algorithm consists of three steps<ul><li>Replace the root key with the key of the last node $w$</li><li>Remove $w$</li><li>Restore the heap-order property</li></ul></li><li><strong>Downheap</strong><ul><li>After replacing the root key with the key $k$ of the last node, the heap-order property may be violated</li><li>Algorithm downheap restores the heap-order property by swapping key $k$ along a downward path from the root (always swap with the smallest child)</li><li>Downheap terminates when key $k$ reaches a leaf or a node whose children have keys greater than or equal to $k$</li><li>Since a heap has height $O(\log n)$, downheap runs in $O(\log n)$ time</li></ul></li></ul><h4 id="4-Array-besed-Heap-Implementation"><a href="#4-Array-besed-Heap-Implementation" class="headerlink" title="4. Array-besed Heap Implementation"></a>4. Array-besed Heap Implementation</h4><ul><li>We can represent a heap with $n$ keys by means of a vector or ArrayList of length $n+1$</li><li>The cell of at index $0$ is not used</li><li>Links between nodes are not explicitly stored</li><li>For the node at index $i$<ul><li>the left child is at index $2i$</li><li>the right child is at index $2i+1$</li></ul></li><li>Operation insert corresponds to inserting at index $n+1$</li><li>Operation removeMin corresponds to moving index $n$ to index $1$</li></ul><h4 id="5-Implementing-Priority-Queue-with-a-Heap"><a href="#5-Implementing-Priority-Queue-with-a-Heap" class="headerlink" title="5. Implementing Priority Queue with a Heap"></a>5. Implementing Priority Queue with a Heap</h4><ul><li>To create a priority queue, initialise a heap</li><li>To insert in the priority queue, insert in the heap</li><li>To get the value with the minimal key, ask for the value of the root of the heap</li><li>To dequeue the highest priority item, remove the root and return the value stored there.</li></ul><h4 id="6-Heap-Sort"><a href="#6-Heap-Sort" class="headerlink" title="6. Heap-Sort"></a>6. Heap-Sort</h4><ul><li>Using a heap-based priority queue, we can sort a sequence of $n$ elements in $O(n\log n)$ time</li><li>The resulting algorithm is called heap-sort</li><li>Heap-sort is much faster than quadratic sorting algorithms, such as insertion-sort and selection-sort</li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Process Scheduling</title>
      <link href="2020/10/13/Process-scheduling/"/>
      <url>2020/10/13/Process-scheduling/</url>
      
        <content type="html"><![CDATA[<h4 id="1-CPU密集型进程"><a href="#1-CPU密集型进程" class="headerlink" title="1. CPU密集型进程"></a>1. CPU密集型进程</h4><ol><li>CPU密集型也叫计算密集型，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading很高。</li><li>在多重程序系统中，大部份时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部份时间用在三角函数和开根号的计算，便是属于CPU bound的程序。</li><li>CPU bound的程序一般而言CPU占用率相当高。这可能是因为任务本身不太需要访问I/O设备，也可能是因为程序是多线程实现因此屏蔽掉了等待I/O的时间。</li></ol><h4 id="2-IO密集型进程"><a href="#2-IO密集型进程" class="headerlink" title="2. IO密集型进程"></a>2. IO密集型进程</h4><ol><li>IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。</li><li>I/O bound的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而pipeline做得不是很好，没有充分利用处理器能力。</li></ol><h4 id="3-CPU密集型-vs-IO密集型"><a href="#3-CPU密集型-vs-IO密集型" class="headerlink" title="3. CPU密集型 vs IO密集型"></a>3. CPU密集型 vs IO密集型</h4><ol><li><p>计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，<strong>所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。</strong></p><p>计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。</p></li><li><p>第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。</p><p>IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。</p></li></ol><h4 id="4-进程调度"><a href="#4-进程调度" class="headerlink" title="4. 进程调度"></a>4. 进程调度</h4><ol><li><strong>Classification by Time Horizon</strong></li></ol><ul><li><strong>Long term</strong>: applies to new processes and controls the <strong>degree of multiprogramming</strong> by deciding which processes to admit to the system when a good <strong>mix</strong> of <strong>CPU</strong> and <strong>I/O bound processes</strong> is favorable to keep all resources as busy as possible.</li><li><strong>Medium term</strong>: controls <strong>swapping</strong> and the <strong>degree of multi-programming</strong> (Memory management).</li><li><strong>Short term (dispatcher)</strong>: decide which process to run next.</li></ul><ol start="2"><li><strong>Classification by Approach</strong></li></ol><ul><li><strong>Non-preemptive(非抢占式或非剥夺式)</strong>: processes are only interrupted voluntarily</li><li><strong>Preemptive(抢占式或剥夺式)</strong>: processes can be <strong>interrupted forcefully</strong> or <strong>voluntarily</strong><ul><li>E.g. preemptive scheduling algorithm picks a process and lets it run for a maximum of some fixed time. (clock interrupt)</li><li>This requires context switches, which generates <strong>overhead</strong></li><li>Prevents processes from <strong>monopolizing(垄断，独占) the CPU</strong></li><li><strong>Most popular</strong> modern operation systems are preemptive</li></ul></li></ul><h4 id="5-性能评估"><a href="#5-性能评估" class="headerlink" title="5. 性能评估"></a>5. 性能评估</h4><ol><li><strong>User oriented criteria</strong>: </li></ol><ul><li><p><strong>Response time(响应时间)</strong>: minimize the time between creating the job and its first</p><p>execution</p></li><li><p><strong>Turnaround time(周转时间)</strong>: minimize the time between creating the job and finishing it</p></li></ul><ol start="2"><li><strong>System oriented criteria</strong>:</li></ol><ul><li><strong>Throughput(吞吐量)</strong>: maximize the number of jobs processed per hour</li><li><strong>Fairness(公平)</strong>: <ul><li>Are processing power/waiting time equally distributed?</li><li>Is there any process with excessively long waiting time? (<strong>starvation</strong>)</li></ul></li></ul><h4 id="6-调度算法"><a href="#6-调度算法" class="headerlink" title="6. 调度算法"></a>6. 调度算法</h4><ol><li>Overview:</li></ol><ul><li><strong>Algorithms</strong>:<ul><li>First Come First Served (FCFS) / First In First Out (FIFO) (<strong>Batch System</strong>)</li><li>Shortest job first (<strong>Batch System</strong>)</li><li>Round Robin (<strong>Interactive System</strong>)</li><li>Priority Queue (<strong>Interactive System</strong>)</li></ul></li><li>Performance measured used:<ul><li><strong>Average response time(平均响应时间)</strong>: the average of the time taken for all the processes to start</li><li><strong>Average turnaround time(平均周转时间)</strong>: the average time taken for all the processes to finish</li></ul></li></ul><ol start="2"><li><strong>First Come First Served</strong>:</li></ol><ul><li>Concept: a <strong>non-preemptive algorithm</strong> that operates as a <strong>strict queueing mechanism</strong> and schedules the processes in the same order that they were added to the queue</li><li>Advantages:<ul><li><strong>positional fairness</strong> and easy to implement</li></ul></li><li>Disadvantages:<ul><li><strong>Favours long processes</strong> over short ones (think of the supermarket checkout!)</li><li>Could <strong>compromise resource utilisation</strong>, i.e., CPU vs. I/O devices</li></ul></li></ul><ol start="3"><li><strong>Shortest Job First:</strong></li></ol><ul><li>Concept: A <strong>non-preemptive algorithm</strong> that starts processes in order of <strong>ascending processing time</strong> using a provided/known estimate of the processing</li><li>Advantages: results in the <strong>optimal turn around time</strong></li><li>Disadvantages:<ul><li><strong>Starvation</strong> might occur</li><li><strong>Fairness</strong> is compromised</li><li><strong>Processing times have to be known</strong> beforehand or estimated by</li></ul></li></ul><ol start="4"><li><strong>Round Robin:</strong></li></ol><ul><li>Concept: a <strong>preemptive version of FCFS</strong> that forces <strong>context switches</strong> at <strong>periodic intervals</strong> or <strong>time slices (Time quantum)</strong><ul><li>Processes run in the order that they were added to the queue</li><li>Processes are forcefully <strong>interrupted by the timer</strong></li></ul></li><li>Advantages:<ul><li>Improved <strong>response time</strong></li><li>Effective for general purpose <strong>time sharing systems</strong></li></ul></li><li>Disadvantages:<ul><li>Increased <strong>context switching</strong> and thus overhead</li><li>Can <strong>reduce to FCFS</strong> (只要时间片足够长，就是FCFS算法)</li></ul></li></ul><ol start="5"><li><strong>Priority Queues:</strong></li></ol><ul><li>Concept: A <strong>preemptive algorithm</strong> that schedules processes by priority (high to low)<ul><li>The process priority is saved in the <strong>process control block</strong></li></ul></li><li>Advantages:<ul><li>can <strong>prioritise I/O bound jobs</strong></li></ul></li><li>Disadvantages:<ul><li>low priority processes may suffer from <strong>starvation</strong> (with static priorities)</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
            <tag> Process Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Process</title>
      <link href="2020/10/10/Process/"/>
      <url>2020/10/10/Process/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>The simplified definition: “a process is a <strong>running instance</strong> of a program”<ul><li>A program is <strong>passive</strong> entity and “sits” on a disk, not doing anything.</li><li>A process has <strong>control structures</strong> associated with it, may be <strong>active</strong>, and may have <strong>resources</strong> assigned to it (e.g. I/O devices, memory, processor). It has a program, input, output and a state.</li><li>A single <strong>processor</strong> may be shared among several <strong>processes</strong>.</li></ul></li><li>A process is registered with the OS using its <strong>“control structures”</strong>: i.e. an entry in the OS’s <strong>process table</strong> to a <strong>process control blocks</strong> (PCB)</li><li>The <strong>process control block</strong> contains all information necessary to <strong>administer the process</strong> and is <strong>essential</strong> for <strong>context switching</strong> in <strong>multiprogramming systems</strong></li></ul><h4 id="2-Menory-Image-of-Processes"><a href="#2-Menory-Image-of-Processes" class="headerlink" title="2. Menory Image of Processes"></a>2. Menory Image of Processes</h4><ul><li>A <strong>process’ memory image</strong> contains:<ul><li>The program <strong>code</strong> (could be shared between multiple processes running the same code)</li><li>A <strong>data</strong> segment: process-specified data (input and output, global variable)</li><li>call <strong>stack</strong>: keep track of active subroutines (function parameters, local variables)</li><li><strong>Heap</strong>: hold intermediate computation data generated during run time</li></ul></li><li>Every process has its own <strong>logical address space</strong>, in which the <strong>stack</strong> and <strong>heap</strong> are placed at <strong>opposite sides</strong> to allow them to grow</li></ul><h4 id="3-Process-States-and-Transitions"><a href="#3-Process-States-and-Transitions" class="headerlink" title="3. Process States and Transitions"></a>3. Process States and Transitions</h4><ul><li><p>Diagram</p><img src="/2020/10/10/Process/Screen Shot 2020-12-21 at 1.50.28 PM.png" style="zoom:50%;"></li><li><p>State transitions include:</p><ul><li>1  <strong>New -&gt; ready</strong>: admit the process and commit to execution</li><li>2  <strong>Running -&gt; blocked</strong>: e.g. process is waiting for input or carried out a system call</li><li>3  <strong>Ready -&gt; running</strong>: the process is selected by the <strong>process sceduler</strong></li><li>4  <strong>Blocked -&gt; ready</strong>: event happens, e.g. I/O operation has finished</li><li>5  <strong>Running -&gt; ready</strong>: the process is preempted, e.g., by a <strong>timer interrupt</strong> or by <strong>pause</strong></li><li>6  <strong>Running -&gt; exit</strong>: process has finished, e.g. program ended or exception encountered</li></ul></li></ul><h4 id="4-Context-Switching-Multiprogramming"><a href="#4-Context-Switching-Multiprogramming" class="headerlink" title="4. Context Switching (Multiprogramming)"></a>4. Context Switching (Multiprogramming)</h4><ul><li><p>Modern computers are <strong>multiprogramming</strong> systems:</p></li><li><p>Assuming a <strong>single processor system</strong>, the instructions of individual processes are executed <strong>sequentially</strong></p><ul><li>CPU’s rapid switching back and forth from process to process is called <strong>multiprogramming</strong>.</li><li>Multiprogramming is achieved by <strong>alternating</strong> processes and <strong>context switching</strong></li><li><strong>True parallelism</strong> requires <strong>mutiple processors</strong></li></ul></li><li><p>When a <strong>context switch</strong> takes place, the system <strong>saves the state</strong> of the old process and <strong>loads the state</strong> of the new process (created <strong>overhead</strong>)</p><ul><li><strong>Saved</strong> =&gt; the process control block is <strong>updated</strong></li><li><strong>(Re-)started</strong> =&gt; the process control block <strong>read</strong></li></ul></li><li><p>A <strong>trade-off</strong> exists between <strong>“responsiveness”</strong> and <strong>“overhead”</strong></p><ul><li><strong>Short time slices</strong> result in <strong>good response time</strong> but <strong>low effective “utilisation”</strong></li><li><strong>Long time slices</strong> result in <strong>poor response time</strong> but <strong>better effective “utilisation”</strong></li></ul></li><li><p>The OS uses <strong>process control block</strong> and a <strong>process table</strong> to manage processes and maintain their information</p></li><li><p>A <strong>process control block</strong> contains three types of <strong>attributes</strong>:</p><ul><li><strong>Process identification</strong> (PID, UID, Parent PID)</li><li><strong>Process state information</strong> (user registers, program counters, stack pointer, program status word, memory management information, files, etc.)</li><li><strong>Process control information</strong> (process state, scheduling information, etc.)</li></ul></li><li><p><strong>Process control blocks</strong> are <strong>kernel data structures</strong>, i.e. they are <strong>protected</strong> and only accessible in <strong>kernel mode!</strong></p><ul><li>Allowing user applications to access them directly could <strong>compromise their integrity</strong></li><li>The <strong>operating system manages</strong> them on the user’s behalf through <strong>system calls</strong></li></ul></li><li><p>Switching Processes</p><ul><li><img src="/2020/10/10/Process/Screen Shot 2020-12-21 at 3.42.30 PM.png" style="zoom:50%;"></li><li><p>1  Save process state (program counter, registers)</p></li><li><p>2  Update PCB (running -&gt; ready)</p></li><li><p>3  Move PCB to appropriate queue (ready/blocked)</p></li><li><p>4  Run scheduler, select new process</p></li><li><p>5  Update to running state in PCB</p></li><li><p>6  Update memory structures</p></li><li><p>7  Restore process</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
            <tag> Context Switching </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树有关算法</title>
      <link href="2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/"/>
      <url>2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h4 id="1-二叉树定义-java"><a href="#1-二叉树定义-java" class="headerlink" title="1. 二叉树定义(java)"></a>1. 二叉树定义(java)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">// Definition for a binary tree node.</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TreeNode</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> val<span class="token punctuation">;</span>  TreeNode left<span class="token punctuation">;</span>  TreeNode right<span class="token punctuation">;</span>  <span class="token function">TreeNode</span><span class="token punctuation">(</span><span class="token keyword">int</span> x<span class="token punctuation">)</span> <span class="token punctuation">{</span> val <span class="token operator">=</span> x<span class="token punctuation">;</span> <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-二叉树翻转"><a href="#2-二叉树翻转" class="headerlink" title="2. 二叉树翻转"></a>2. 二叉树翻转</h4><img src="/2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/截屏2020-09-16 下午4.04.19.png" style="zoom:50%;"><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//简单递归即可实现</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> TreeNode <span class="token function">invertTree</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>root <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token keyword">return</span> null<span class="token punctuation">;</span>    TreeNode left <span class="token operator">=</span> <span class="token function">invertTree</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>left<span class="token punctuation">)</span><span class="token punctuation">;</span>    TreeNode right <span class="token operator">=</span> <span class="token function">invertTree</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>right<span class="token punctuation">)</span><span class="token punctuation">;</span>    root<span class="token punctuation">.</span>right <span class="token operator">=</span> left<span class="token punctuation">;</span>    root<span class="token punctuation">.</span>left <span class="token operator">=</span> right<span class="token punctuation">;</span>    <span class="token keyword">return</span> root<span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 当然还可以用四种遍历来解决这道题</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 总结 </tag>
            
            <tag> 未完待续 </tag>
            
            <tag> Java </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> 二叉树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序方法汇总</title>
      <link href="2020/09/15/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/"/>
      <url>2020/09/15/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h4 id="1-冒泡排序"><a href="#1-冒泡排序" class="headerlink" title="1. 冒泡排序"></a>1. 冒泡排序</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 整数数组升序</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> i<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>          nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>          nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环，通过依次比较相邻元素，若前一个大于后一个则交换位置，循环结束后，数组内最大值将位于数组最后一个位置；第二次循环，也依次比较相邻元素，除了最后一对（因为最后一个是最大值，前一个肯定比后一个小）,循环结束数组内倒数第二大的值将位于数组倒数第二个位置；……直到没有一对数字需要比较，此时数组以升序排列。</p><p>冒泡排序的时间复杂度为O(n<sup>2</sup>)，空间复杂度为O(1)，计算方法请参见“<a href="https://scycy2.github.io/2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/">算法复杂度</a>”一文。</p><h4 id="2-选择排序"><a href="#2-选择排序" class="headerlink" title="2. 选择排序"></a>2. 选择排序</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">SelectionSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> minIndex <span class="token operator">=</span> i<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 记录最小值的下标</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums<span class="token punctuation">[</span>minIndex<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>          minIndex <span class="token operator">=</span> j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// 交换最小值和当前位置</span>      <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>      nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>minIndex<span class="token punctuation">]</span><span class="token punctuation">;</span>      nums<span class="token punctuation">[</span>minIndex<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环找出当前数组最小值的下标，并放到数组开头；第二次循环找出除了数组第一个值的数组最小值，并交换到第二个位置；……直到数组最后两个值比较（并交换），此时数组按从小到大顺序排列。</p><p>选择排序的时间复杂度为O(n<sup>2</sup>)，空间复杂度为O(1)。</p><h4><span id="3">3. 直接插入排序</span></h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">InsertionSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 当前值与前面已排好序的值比较</span>      <span class="token keyword">int</span> j<span class="token punctuation">;</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span>j <span class="token operator">=</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> j <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> temp<span class="token punctuation">;</span> <span class="token operator">--</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        nums<span class="token punctuation">[</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 如果当前值小于前面的值，则将前面的值向后移</span>      <span class="token punctuation">}</span>      nums<span class="token punctuation">[</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 直到当前值大于等于前面的某个值或者前面没有值的时候，将该值插入</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环，获取当前值temp=nums[1]，将temp与nums[0]比较，若小于nums[0]，将nums[0]的值往后移变为nums[1]，然后temp插入到nums[1]前即nums[0]的位置；第二次循环（前两个值已排好序），获取temp=nums[2]，先与nums[1]比较，若大于则不变，小于则将nums[1]的值往后移变为nums[2]，再比较temp与nums[0]并重复之前的过程；……最终数组以升序排序。</p><p>插入排序的空间复杂度为O(1)，时间复杂度则与原数组排列顺序有关，如果原数组已按升序排序，则该算法的时间复杂度为O(n)（最好的情况）；若按降序排列，则时间复杂度为O(n<sup>2</sup>)（最差的情况）。</p><h4 id="4-快速排序"><a href="#4-快速排序" class="headerlink" title="4. 快速排序"></a>4. 快速排序</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// start为待排序数组开始的下标，end为结束下标</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token function">quickSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">,</span> <span class="token keyword">int</span> start<span class="token punctuation">,</span> <span class="token keyword">int</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> pivot <span class="token operator">=</span> nums<span class="token punctuation">[</span>start<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 取数组某个值(此处选第一个值)作为参照，比它小的放在它左边，大的放在右边</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> start<span class="token punctuation">;</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> end<span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> pivot<span class="token punctuation">)</span> <span class="token punctuation">{</span>        j<span class="token operator">--</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 从右往左，本就比参照大的数则不动</span>      <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;</span> pivot<span class="token punctuation">)</span> <span class="token punctuation">{</span>        i<span class="token operator">++</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 从左往右，本就比参照小的数不动</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&amp;&amp;</span> i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        i<span class="token operator">++</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 交换</span>        <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">></span> start<span class="token punctuation">)</span> nums <span class="token operator">=</span> <span class="token function">quickSort</span><span class="token punctuation">(</span>nums<span class="token punctuation">,</span> start<span class="token punctuation">,</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">if</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">&lt;</span> start<span class="token punctuation">)</span> nums <span class="token operator">=</span> <span class="token function">quickSort</span><span class="token punctuation">(</span>nums<span class="token punctuation">,</span> j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> nums<span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一趟排序将数组分为两部分，一部分小于参照值，另一部分则大于参照值；再分别对两部分进行排序（递归）。当两部分都有序时，则整个数组都为有序状态。</p><p>快速排序的空间复杂度为O(1)，其时间复杂度与选取的参照有关，若每次选取的参照可使数组等分，则经过log<sub>2</sub>n躺划分，可完成排序，此时时间复杂度为O(nlog<sub>2</sub>n)；若每次选取的参照为最大值或最小值，则需要经过n躺划分，可完成排序，此时时间复杂度为O(n<sup>2</sup>)。</p><h4 id="5-希尔排序"><a href="#5-希尔排序" class="headerlink" title="5. 希尔排序"></a>5. 希尔排序</h4><p>希尔排序是插入排序的一种，又称“缩小增量排序”。</p><p>希尔排序就是将数组根据下标的一定增量分组，然后对每一组进行直接插入排序；随后增量减少，每次增量不同，都进行一次直接插入排序，直到增量为1，此时就是上面所提到的<a href="#3">直接插入排序</a>。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">shellSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> n <span class="token operator">=</span> nums<span class="token punctuation">.</span>length<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> step <span class="token operator">=</span> n <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> step <span class="token operator">>=</span> <span class="token number">1</span><span class="token punctuation">;</span> step <span class="token operator">/=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 步长</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> step<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 内循环两层为插入排序</span>        <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> j <span class="token operator">=</span> i <span class="token operator">-</span> step<span class="token punctuation">;</span>          <span class="token keyword">while</span> <span class="token punctuation">(</span>j <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> temp<span class="token punctuation">)</span> <span class="token punctuation">{</span>            nums<span class="token punctuation">[</span>j <span class="token operator">+</span> step<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>          j <span class="token operator">-=</span> step<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        nums<span class="token punctuation">[</span>j <span class="token operator">+</span> step<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>      <span class="token punctuation">}</span>       <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环增量（步长）step为数组长度n的一半，此时根据增量将数组分为n/2组，分别为(nums[0], nums[step])，(nums[1], nums[1+step])，……(nums[step-1], nums[2*step-1])，然后对每组进行直接插入排序；第二次循环增量再减半，根据增量数组可分为多组，再对各组进行直接插入排序；……最后增量为1，即普通的直接插入排序。</p><p>希尔排序的空间复杂度为O(1)，但时间复杂度很难计算，查阅资料后可得希尔排序的平均时间复杂度为O(n<sup>3/2</sup>)（注意这里是<strong>平均</strong>)。</p><p>希尔排序较直接插入排序快是因为当增量大时，进行直接插入排序的元素少，速度快；当增量逐渐减少，此时数组已基本有序，此时直接插入排序对基本有序的序列排序效率很高。</p><p>这里有一个例子来自维基百科（个人认为维基百科将此过程通过列来表达更加清晰易懂）</p><p>此处待补充。。。。。。。。。。。。。。。。。。。。。。。</p><h4>6. 归并排序</h4><p>归并排序是建立在归并操作上的一种有效的、稳定的排序算法，该算法采用了分治法(Divide and Conquer)。</p><p>将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。</p><pre class="line-numbers language-java"><code class="language-java">Public <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">mergeSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arr<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> temp<span class="token punctuation">,</span> <span class="token keyword">int</span> start<span class="token punctuation">,</span> <span class="token keyword">int</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>start <span class="token operator">&lt;</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> mid <span class="token operator">=</span> <span class="token punctuation">(</span>start <span class="token operator">+</span> end<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>      <span class="token function">mergeSort</span><span class="token punctuation">(</span>arr<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> start<span class="token punctuation">,</span> mid<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token function">mergeSort</span><span class="token punctuation">(</span>arr<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> mid <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">merge</span><span class="token punctuation">(</span>arr<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> start<span class="token punctuation">,</span> mid<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arr<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> temp<span class="token punctuation">,</span> <span class="token keyword">int</span> start<span class="token punctuation">,</span> <span class="token keyword">int</span> mid<span class="token punctuation">,</span> <span class="token keyword">int</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> start<span class="token punctuation">;</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> mid <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> mid <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;=</span> mid<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token operator">++</span>i<span class="token punctuation">;</span>      <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>        temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token operator">++</span>j<span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token operator">++</span>k<span class="token punctuation">;</span>    <span class="token punctuation">}</span>        <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> mid<span class="token punctuation">)</span> <span class="token punctuation">{</span>      temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token operator">++</span>i<span class="token punctuation">;</span>      <span class="token operator">++</span>k<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>j <span class="token operator">&lt;=</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>      temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token operator">++</span>j<span class="token punctuation">;</span>      <span class="token operator">++</span>k<span class="token punctuation">;</span>    <span class="token punctuation">}</span>        System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>temp<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> arr<span class="token punctuation">,</span> start <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 未完待续 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树四种遍历方式</title>
      <link href="2020/09/13/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/"/>
      <url>2020/09/13/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h4 id="1-二叉树定义-java"><a href="#1-二叉树定义-java" class="headerlink" title="1. 二叉树定义(java)"></a>1. 二叉树定义(java)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">// Definition for a binary tree node.</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TreeNode</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> val<span class="token punctuation">;</span>  TreeNode left<span class="token punctuation">;</span>  TreeNode right<span class="token punctuation">;</span>  <span class="token function">TreeNode</span><span class="token punctuation">(</span><span class="token keyword">int</span> x<span class="token punctuation">)</span> <span class="token punctuation">{</span> val <span class="token operator">=</span> x<span class="token punctuation">;</span> <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-前、中、后序遍历-递归版"><a href="#2-前、中、后序遍历-递归版" class="headerlink" title="2. 前、中、后序遍历(递归版)"></a>2. 前、中、后序遍历(递归版)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> <span class="token function">preorderTraversal</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">)</span> <span class="token punctuation">{</span>    List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> res <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">helper</span><span class="token punctuation">(</span>root<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span>  <span class="token punctuation">}</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">helper</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">,</span> List <span class="token operator">&lt;</span>Integer<span class="token operator">></span> res<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>root <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">// res.add(root.val); // 前序遍历，先将root的值加入List中</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>root<span class="token punctuation">.</span>left <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">helper</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>left<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// res.add(root.val); // 中序遍历，先将左边节点的值加入List中，再加入root的值</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>root<span class="token punctuation">.</span>right <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token function">helper</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>right<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// res.add(root.val); // 后序遍历，先将左右节点的值加入List中，最后加入root的值</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-前、中、后序遍历-迭代版"><a href="#3-前、中、后序遍历-迭代版" class="headerlink" title="3. 前、中、后序遍历(迭代版)"></a>3. 前、中、后序遍历(迭代版)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//前序</span><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> <span class="token function">preorderTraversal</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">)</span> <span class="token punctuation">{</span>      List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> ans <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LinkedList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>root <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    LinkedList<span class="token operator">&lt;</span>TreeNode<span class="token operator">></span> stack <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LinkedList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 需要不断的增删，因此用LinkedList</span>    stack<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token operator">!</span>nodes<span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      TreeNode n <span class="token operator">=</span> stack<span class="token punctuation">.</span><span class="token function">pollLast</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//根据stack先进后出的原则，先从上到下获取所有左节点的值，再往右获取所有右节点的值</span>      ans<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>val<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>n<span class="token punctuation">.</span>right <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        stack<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>right<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>n<span class="token punctuation">.</span>left <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        stack<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>left<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>        <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 中序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-层次遍历"><a href="#4-层次遍历" class="headerlink" title="4. 层次遍历"></a>4. 层次遍历</h4>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 总结 </tag>
            
            <tag> 未完待续 </tag>
            
            <tag> Java </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法复杂度</title>
      <link href="2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
      <url>2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h3 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1. 时间复杂度"></a>1. 时间复杂度</h3><h4 id="1-1-大O表示法："><a href="#1-1-大O表示法：" class="headerlink" title="1.1 大O表示法："></a>1.1 大O表示法：</h4><p>用O(n)来体现算法的时间复杂度。</p><p>大O表示法O(f(n))中f(n)可以是1、n<sup>2</sup>、logn等，接下来看看如何推倒大O阶。</p><h4 id="1-2-推导大O阶规则："><a href="#1-2-推导大O阶规则：" class="headerlink" title="1.2 推导大O阶规则："></a>1.2 推导大O阶规则：</h4><p>a. <strong>用1来代替运行时间中的所有加法常数</strong></p><p>b. <strong>f(n)若是多项式，只保留最高阶项即可</strong></p><p>c. <strong>去掉最高阶项系数</strong></p><h4 id="1-3"><a href="#1-3" class="headerlink" title="1.3"></a>1.3</h4><ul><li><p>常数阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">;</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"O"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">--</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//每一句都只执行一次</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码执行次数为4次，根据规则a，其时间复杂度为O(1)。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    i<span class="token operator">++</span><span class="token punctuation">;</span><span class="token punctuation">}</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">Println</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码虽然循环了100次，但它的时间复杂度依然为O(1)，因为它的执行次数只是一个较大的常数.</p></li><li><p>线性阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>此段代码为for循环，其中<code>int i = 0</code>只执行一次，<code>i &lt; n</code>执行n次，<code>i++</code>执行n次，打印语句也执行了n次，因此这段代码执行次数为3n+1，根据规则，T(n) = O(n)。</p></li><li><p>对数阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//执行一次</span><span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>     i <span class="token operator">*=</span> <span class="token number">2</span><span class="token punctuation">;</span><span class="token punctuation">}</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//执行一次</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>中间循环体<code>i &lt; n</code>以及<code>i *= 2</code>都分别执行了log<sub>2</sub>n次，因为i每次翻倍增加，通过计算可知中间循环体循环了log<sub>2</sub>n次，一次这段代码执行次数为1+2log<sub>2</sub>n，根据规则，T(n) = O(logn)。</p></li><li><p>平方阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// for (int j = 0; j &lt; i + 1; j++)</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> j<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最内层循环体执行次数为xn<sup>2</sup>(x为某常数)，可得该段代码执行次数最高阶项为xn<sup>2</sup>，根据规则，T(n) = O(n<sup>2</sup>)。</p><p>同样可得立方阶，四次方阶……</p></li><li><p>碰到的较复杂的算法复杂度分析</p><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * Definition for singly-linked list. * public class ListNode { *     int val; *     ListNode next; *     ListNode(int x) { val = x; } * } */</span><span class="token comment" spellcheck="true">//合并K个有序链表</span><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> ListNode <span class="token function">mergeKLists</span><span class="token punctuation">(</span>ListNode<span class="token punctuation">[</span><span class="token punctuation">]</span> lists<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>lists<span class="token punctuation">.</span>length <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> null<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        ListNode ans <span class="token operator">=</span> lists<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> lists<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>lists<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token keyword">continue</span><span class="token punctuation">;</span>            ans <span class="token operator">=</span> <span class="token function">mergeTwoLists</span><span class="token punctuation">(</span>ans<span class="token punctuation">,</span> lists<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> ListNode <span class="token function">mergeTwoLists</span><span class="token punctuation">(</span>ListNode l1<span class="token punctuation">,</span> ListNode l2<span class="token punctuation">)</span> <span class="token punctuation">{</span>        ListNode head <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ListNode</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        ListNode curr <span class="token operator">=</span> head<span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span>l1 <span class="token operator">!=</span> null <span class="token operator">&amp;&amp;</span> l2 <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>l1<span class="token punctuation">.</span>val <span class="token operator">&lt;=</span> l2<span class="token punctuation">.</span>val<span class="token punctuation">)</span> <span class="token punctuation">{</span>                curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l1<span class="token punctuation">;</span>                curr <span class="token operator">=</span> curr<span class="token punctuation">.</span>next<span class="token punctuation">;</span>                l1 <span class="token operator">=</span> l1<span class="token punctuation">.</span>next<span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>                curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l2<span class="token punctuation">;</span>                curr <span class="token operator">=</span> curr<span class="token punctuation">.</span>next<span class="token punctuation">;</span>                l2 <span class="token operator">=</span> l2<span class="token punctuation">.</span>next<span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>l1 <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l2<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>l2 <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l1<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> head<span class="token punctuation">.</span>next<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>首先分析<code>mergeTwoLists</code>函数的时间复杂度，假设每个链表的长度为n，可简单地得出该函数的T(o) = O(n + n) = O(n) (为简便，只计算循环体执行次数)。再分析<code>mergeKLists</code>函数的时间复杂度，同样的每个链表长度都为n，第一次合并前两个链表，此时<code>ans</code> 的长度都为2n，第x次合并后，长度变为xn，第x次的时间代价为O(n + (x - 1) $\times$ n) = O(x $\times$ n)，此时总的时间代价可用求和公式计算O($\sum_{i=1}^k(i\times n)$) = O($\frac{(1+k)\times k}{2}\times n$) = O(k<sup>2</sup>$\times $n)，所以时间复杂度为O(k<sup>2</sup> $\times$ n)。</p></li></ul><h3 id="2-空间复杂度"><a href="#2-空间复杂度" class="headerlink" title="2. 空间复杂度"></a>2. 空间复杂度</h3><h4 id="2-1-空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S-n-来定义。"><a href="#2-1-空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S-n-来定义。" class="headerlink" title="2.1 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S(n)来定义。"></a>2.1 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S(n)来定义。</h4><h4 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h4><p>空间复杂度常用的有O(1), O(n), O(n<sup>2</sup>)</p><h4 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h4><ul><li><p>O(1)</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> i<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>经典的冒泡排序以及插入排序、选择排序等都是空间复杂度为O(1)的算法，因为它没有临时占用额外的存储空间，只在自己的数组里进行交换。</p></li><li><p>O(n)</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">climbStairs</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> dp <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">int</span><span class="token punctuation">[</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        dp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        dp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> dp<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        dp<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">>=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> dp<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是leetcode上一道算法题，需要你计算爬到n阶且每次只能爬1或2阶有多少种不同的方法。很显然这是一道动态规划的题目，因此我们可以new一个数组来保存爬1～n阶各有多少种方法，而到n阶的方法就是到n-1阶和到n-2阶的方法和。这里我们new了一个长度为n+1的数组，用了额外的空间来存储我们需要的数据，因此空间复杂度为O(n+1) = O(n)。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 算法复杂度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo-theme-matery主题部分优化</title>
      <link href="2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/"/>
      <url>2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>hexo-theme-matery下载与配置: <a href="https://github.com/blinkfox/hexo-theme-matery">https://github.com/blinkfox/hexo-theme-matery</a></p><h3 id="1-关于个人信息的添加及修改："><a href="#1-关于个人信息的添加及修改：" class="headerlink" title="1. 关于个人信息的添加及修改："></a>1. 关于个人信息的添加及修改：</h3><p>以自己的个人信息作说明：</p><p>原：<img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/yuan.png" style="zoom:50%;"></p><p>现：</p><img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/PersonalInfo.png" style="zoom:50%;"><p>首先找到<code>/hexo-theme-matery/layout/about.ejs</code> ，并找到代码如下：</p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"author"</span><span class="token operator">></span> <span class="token comment" spellcheck="true">//作者信息</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"post-statis hide-on-large-only"</span> data<span class="token operator">-</span>aos<span class="token operator">=</span><span class="token string">"zoom-in-right"</span><span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/post-statis'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//引用另一个文件，表示的是左边文章、分类、标签以及他们的数量</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"title"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>author <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//作者名字，在blog/_config.yml中配置</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"career"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> theme<span class="token punctuation">.</span>profile<span class="token punctuation">.</span>career <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//作者职业，在/hexo-theme-matery/layout/about.ejs中配置</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"social-link hide-on-large-only"</span> data<span class="token operator">-</span>aos<span class="token operator">=</span><span class="token string">"zoom-in-left"</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/social-link'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//右边github、email、QQ等的链接，也可自己减少或增加</span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因此想要修改或增加自己的信息，需要在相应的配置文件里进行修改或增加。</p><p>比如我自己做的修改，首先打开<code>hexo-theme-matery</code>下的<code>_config.yml</code>文件，并找到<code>profile</code>属性，</p><pre class="line-numbers language-yml"><code class="language-yml">profile:  avatar: /medias/头像.jpg  career: Undergraduate  school: University of Nottingham Ningbo China (UNNC)  major: Computer Science with Artificial Intelligence (4+0)  introduction: If you wish to succeed, you should use persistence as your good friend, experience as your reference, prudence as your brother and hope as your sentry.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在<code>profile</code>里修改了头像，职业，以及新增了学校和专业。</p><p>那么该如何将这些特性在网页里显示出来呢？我们需要在上面说的<code>about.ejs</code>这个文件里做相应的修改。</p><pre><code>&lt;div class=&quot;title&quot;&gt;&lt;%- config.author %&gt;&lt;/div&gt;&lt;div class=&quot;career&quot;&gt;&lt;%- theme.profile.career %&gt;&lt;/div&gt;&lt;div class=&quot;major&quot;&gt;&lt;%- theme.profile.major %&gt;&lt;/div&gt;&lt;div class=&quot;school&quot;&gt;&lt;%- theme.profile.school %&gt;&lt;/div&gt;</code></pre><p>只要加几个<code>div</code>块元素，就能显示你的其他信息。</p><h3 id="2-Post封面图片的修改"><a href="#2-Post封面图片的修改" class="headerlink" title="2. Post封面图片的修改"></a>2. Post封面图片的修改</h3><p>先找到<code>/hexo-theme/matery/layout/_partial/post_cover.ejs</code>中的代码</p><pre class="line-numbers language-js"><code class="language-js"><span class="token keyword">if</span> <span class="token punctuation">(</span>page<span class="token punctuation">.</span>img<span class="token punctuation">)</span> <span class="token punctuation">{</span>    featureimg <span class="token operator">=</span> <span class="token function">url_for</span><span class="token punctuation">(</span>page<span class="token punctuation">.</span>img<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 如果设置了img属性，那么封面图片就是你设置的图片</span><span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> hashCode <span class="token operator">=</span> <span class="token keyword">function</span> <span class="token punctuation">(</span>str<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>str <span class="token operator">&amp;&amp;</span> str<span class="token punctuation">.</span>length <span class="token operator">===</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">var</span> hash <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">var</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> len <span class="token operator">=</span> str<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> len<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            hash <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>hash <span class="token operator">&lt;</span><span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">-</span> hash<span class="token punctuation">)</span> <span class="token operator">+</span> str<span class="token punctuation">.</span><span class="token function">charCodeAt</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>            hash <span class="token operator">|</span><span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> hash<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> len <span class="token operator">=</span> theme<span class="token punctuation">.</span>featureImages<span class="token punctuation">.</span>length<span class="token punctuation">;</span>    <span class="token keyword">var</span> num <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">abs</span><span class="token punctuation">(</span><span class="token function">hashCode</span><span class="token punctuation">(</span>page<span class="token punctuation">.</span>title<span class="token punctuation">)</span> <span class="token operator">%</span> len<span class="token punctuation">)</span><span class="token punctuation">;</span>    featureimg <span class="token operator">=</span> theme<span class="token punctuation">.</span>jsDelivr<span class="token punctuation">.</span>url            <span class="token operator">?</span> theme<span class="token punctuation">.</span>jsDelivr<span class="token punctuation">.</span>url <span class="token operator">+</span> <span class="token function">url_for</span><span class="token punctuation">(</span>theme<span class="token punctuation">.</span>featureImages<span class="token punctuation">[</span>num<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token punctuation">:</span> <span class="token function">url_for</span><span class="token punctuation">(</span>theme<span class="token punctuation">.</span>featureImages<span class="token punctuation">[</span>num<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 否则将会采用作者的算法，根据title的长度来计算hashcode，再根据hashcode从默认的图片中选出一张。如果两篇文章的title都是四个字，那这两篇的封面就是同一张图片。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>根据代码可知，想要一篇post用特定的图片，就必须设置这篇post的img属性。如下</p><img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/屏幕快照 2020-07-20 下午2.41.07.png" style="zoom:50%;"><p>可将图片移至<code>hexo-theme-matery/source/medias/featureimages</code>下，并将img属性设置为<code>/medias/featureimages/‘图片名’</code>即可。</p>]]></content>
      
      
      <categories>
          
          <category> 博客优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/07/16/hello-world/"/>
      <url>2020/07/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 初始内容 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
