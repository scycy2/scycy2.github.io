<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MapReduce Paradigm</title>
      <link href="2021/09/17/MapReduce-Paradigm/"/>
      <url>2021/09/17/MapReduce-Paradigm/</url>
      
        <content type="html"><![CDATA[<h4 id="1-What-is-MapReduce"><a href="#1-What-is-MapReduce" class="headerlink" title="1. What is MapReduce?"></a>1. What is MapReduce?</h4><ul><li><p>Terms are borrowed from Functional Language (e.g., Lisp)</p></li><li><p>Example: Sum of Square</p><ul><li><p>(map square ‘(1 2 3 4))</p><ul><li>Output: (1 4 9 16)</li><li>processes each record sequentially and independently</li></ul></li><li><p>(reduce + ‘(1 4 9 16))</p><ul><li><p>(+ 16 (+ 9 (+ 4 1)))</p></li><li><p>Output: 30</p></li><li><p>processes set of all records in batches</p></li></ul></li></ul></li><li><p><strong>Map</strong></p><ul><li>Parallelly Process a large number of individual records to generate intermediate key/value pairs</li></ul></li><li><p><strong>Reduce</strong></p><ul><li>Each key assigned to one Reduce</li><li>Parallelly Processes and merges all intermediate values by partitioning keys</li></ul></li></ul><h4 id="2-MapReduce-Scheduling"><a href="#2-MapReduce-Scheduling" class="headerlink" title="2. MapReduce Scheduling"></a>2. MapReduce Scheduling</h4><ul><li><p>Programming MapReduce</p><ul><li><p>Externally: For user</p><ol><li>Write a Map program (short), write a Reduce program (short)</li><li>Submit job: wait for result</li><li>Need to know nothing about parallel/distributed programming</li></ol></li><li><p>Internally: For the Paradigm and Scheduler</p><ol><li>Parallelize Map</li><li>Transfer data from Map to Reduce</li><li>Parallelize Reduce</li><li>Implement Storage for Map input, Map output, Reduce input and Reduce output</li></ol><p>(Ensure that no Reduce starts before all Maps are finished. That is, ensure the <em><strong>barrier</strong></em> between the Map phase and Reduce phase)</p></li></ul></li><li><p>Inside MapReduce</p><ul><li>For the cloud:<ol><li>Parallelize Map: each map task is independent of the other<ul><li>All Map outpur records with same key assigned to same Reduce</li></ul></li><li>Transfer data from Map to Reduce:<ul><li>All Map output records with same key assigned to same Reduce task</li><li>use partitioning function</li></ul></li><li>Parallelize Reduce: each reduce task is independent of the other</li><li>Implement Storage for Map input, Map output, Reduce input, and Reduce output<ul><li>Map input: from distributed file system</li><li>Map output: to local disk (at Map node); uses local file system</li><li>Reduce input: from (multiple) remote disks; uses local file systems</li><li>Reduce output: to distributed file system</li></ul></li></ol></li></ul></li><li><p><strong>The YARN Scheduler</strong></p><ul><li><p>Used in Hadoop 2.x +</p></li><li><p>YARN = Yet Another Resource Negotiator</p></li><li><p>Treats each server as a collection of <em>containers</em></p><ul><li>Container = some CPU + some memory</li></ul></li><li><p>Has 3 main components</p><ul><li><p>Global <em>Resource Manager (RM)</em></p><ul><li>Scheduling</li></ul></li><li><p>Per-server <em>Node Manager (NM)</em></p><ul><li>Daemon and server-specific functions</li></ul></li><li><p>Per-application (job) <em>Application Master (AM)</em></p><ul><li><p>Container negotiation with RM and NMs</p></li><li><p>Detecting task failures of that job</p></li></ul></li></ul></li><li><img src="/2021/09/17/MapReduce-Paradigm/Screen Shot 2021-09-17 at 1.58.30 PM.png" style="zoom:50%;"></li></ul></li></ul><h4 id="3-MapReduce-Fault-Tolerance"><a href="#3-MapReduce-Fault-Tolerance" class="headerlink" title="3. MapReduce Fault-Tolerance"></a>3. MapReduce Fault-Tolerance</h4><ul><li>Server Failure<ul><li>NM heartbeats to RM<ul><li>If server fails, RM lets all affected Ams know, and AMs take action</li></ul></li><li>NM keeps track of each task running at its server<ul><li>If task fails while in-progress, mark the task as idle and restart it</li></ul></li><li>AM heartbeats to RM<ul><li>On failure, RM restarts AM, which then syncs up with its running tasks</li></ul></li></ul></li><li>RM Failure<ul><li>Use old checkpoints and bring up secondary RM</li></ul></li><li>Heartbeats also used to piggyback container requests<ul><li>Avoid extra messages</li></ul></li><li>Slow Servers<ul><li>The slowest machine slows the entire job down</li><li>Dueto Bad Disk, Network Bandwidth, CPU, or Memory</li><li>Keep track of “progress” of each task (% done)</li><li>Perform backup (replicated) execution of straggler task: task considered done when first replica complete. Called <strong>Speculative Execution</strong></li></ul></li><li>Locality<ul><li>Since cloud has hierarchical topology (e.g. racks)</li><li>GFS/HDFS stores 3 replicas of each of chunks<ul><li>Maybe on different racks</li></ul></li><li>Mapreduce attempts to schedule a map task on<ul><li>a machine that contains a replica of corresponding input data, or failing that,</li><li>on the same rack as a machine containing the input, or failing that,</li><li>Anywhere</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloud Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Clouds</title>
      <link href="2021/09/12/Introduction-to-Clouds/"/>
      <url>2021/09/12/Introduction-to-Clouds/</url>
      
        <content type="html"><![CDATA[<h4 id="1-What-is-a-Cloud"><a href="#1-What-is-a-Cloud" class="headerlink" title="1. What is a Cloud?"></a>1. What is a Cloud?</h4><ul><li><p>Cloud = Lots of storage + compute cycles nearby</p></li><li><p>A single-site cloud (aka “Datacenter”) consists of</p><ul><li>Compute nodes (grouped into racks)</li><li>Switches, connecting the racks</li><li>A network topology, e.g. hierarchical</li><li>Storage (backend) nodes connected to the network</li><li>Front-end for submitting jobs and receiving client requests</li><li>Software Services</li></ul></li><li><p>A geographically distributed cloud consists of</p><ul><li>Multiple such sites</li><li>Each site perhaps with a different structure and services</li></ul></li></ul><h4 id="2-What’s-new-in-today’s-clouds"><a href="#2-What’s-new-in-today’s-clouds" class="headerlink" title="2. What’s new in today’s clouds"></a>2. What’s new in today’s clouds</h4><ul><li>Four features new in today’s clouds<ul><li>Massive scale.</li><li>On-demand access: Pay-as-you-go, no upfront commitment.<ul><li>Anyone can access it.</li></ul></li><li>Data-intensive Nature: What was MBs has now become TBs, PBs, and XBs.<ul><li>Daily logs, forensics, Web data, etc.</li><li>Humans have data numbness: Wikipedia (large) compress is only about 10 GB!</li></ul></li><li>New Cloud Programming Paradigms: MapReduce/Hadoop, NoSQL/Cassandra/MongoDB and many others.<ul><li>High in accessibility and ease of programmability.</li><li>Lots of open-source</li></ul></li></ul></li></ul><h4 id="3-New-Aspects-of-Clouds"><a href="#3-New-Aspects-of-Clouds" class="headerlink" title="3. New Aspects of Clouds"></a>3. New Aspects of Clouds</h4><ol><li>Massive Scale (easy to understand)</li><li>On-demand Access: *aaS Classification<ul><li>HaaS: Hardware as a Service<ul><li>You get access to barebones hardware machines, do whatever you want with them</li><li>Not always a good idea because of security risks</li></ul></li><li>IaaS: Infrastructure as a Service<ul><li>You get access to flexible computing and storage infrastructure. Virtualization is one way of achieving this. Often said to subsume HaaS.</li></ul></li><li>PaaS: Platform as a Service<ul><li>You get access to flexible computing and storage infrastructure, coupled with a software platform (often tightly)</li></ul></li><li>SaaS: Software as a Service<ul><li>You get access to software services, when you need them. Often said to subsume SOA (Service Oriented Architectures).</li></ul></li></ul></li><li>Data-intensive Computing<ul><li>Computation-Intensive Computing<ul><li>Example areas: MPI-based, High-performance computing, Grids</li><li>Typically run on supercomputers</li></ul></li><li>Data-Intensive<ul><li>Typically store data at datacenters</li><li>Use compute nodes nearby</li><li>Compute nodes run computation services</li></ul></li><li>In data-intensive computing, the <strong>focus shifts from computation to the data</strong>: CPU utilization no longer the most important resource metric, instead I/O is (disk and/or network)</li></ul></li><li>New Cloud Programming Paradigms<ul><li>Easy to write and run highly parallel programs in new cloud programming paradigms</li></ul></li></ol><h4 id="4-Economics-of-Clouds"><a href="#4-Economics-of-Clouds" class="headerlink" title="4. Economics of Clouds"></a>4. Economics of Clouds</h4><ul><li>Two categories of clouds<ul><li>Can either be a public cloud, or private cloud</li><li>Private clouds are accessible only to company employees</li><li>Public clouds provide service to any paying customer</li></ul></li><li>To Outsource or Own?</li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloud Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning Optimization Methods</title>
      <link href="2021/09/03/Deep-Learning-Optimization-Methods/"/>
      <url>2021/09/03/Deep-Learning-Optimization-Methods/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Exponentially-Weighted-Moving-Average-EWMA"><a href="#1-Exponentially-Weighted-Moving-Average-EWMA" class="headerlink" title="1. Exponentially Weighted Moving Average (EWMA)"></a>1. Exponentially Weighted Moving Average (EWMA)</h4><ul><li><p>The Exponentially Weighted Moving Average (EWMA) is a quantitative or statistical measure used to model or describe a time series.</p></li><li><p>The moving average is designed as such that older observations are given lower weights. The weights fall exponentially as the data input gets older.</p></li><li><p>The only decision a user of the EWMA (denoted by $v$) must take is the parameter <strong>beta</strong>. The parameter decides how important the current observation is in the calculation of the $v$. The higher the value of alpha, the more closely the $v$ racks the original time series.</p></li><li><p>Suppose given a series of data $x_1, x_2, \cdots, x_n$，to fit a curve, we use following formula:</p><ul><li><p>$$<br>v_0 = 0\<br>v_1 = \beta \times v_0 + (1 - \beta) \times x_1\<br>v_2 = \beta \times v_1 + (1 - \beta) \times x_2\<br>\cdots\<br>v_t = \beta \times v_{t-1} + (1 - \beta) \times x_t<br>$$</p></li><li><img src="/2021/09/03/Deep-Learning-Optimization-Methods/Screen Shot 2021-09-11 at 11.31.19 AM.png" style="zoom:50%;"><p>$\beta = 0.9$</p></li></ul></li><li><p>$\beta = 0.98$, green line; $\beta = 0.5$, yellow line</p><img src="/2021/09/03/Deep-Learning-Optimization-Methods/Screen Shot 2021-09-11 at 11.35.13 AM.png" style="zoom:50%;"></li></ul><h4 id="2-Momentum"><a href="#2-Momentum" class="headerlink" title="2. Momentum"></a>2. Momentum</h4><ul><li><p>To solve the problem of large oscillating of mini-batch SGD when updating parameters.</p></li><li><p>Make the convergence speed faster.</p></li><li><p>Implementation details:</p><p>On iteration $t$:</p><p>​        Compute $dW, db$ on the current mini-batch</p><p>​        $v_{dW} = \beta v_{dw} + (1 - \beta)dW$</p><p>​        $v_{db} = \beta v_{db} + (1 - \beta)db$</p><p>​        $W = W - \alpha v_{dW}$</p><p>​        $b = b - \alpha v_{db}$</p></li><li><p>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</p></li><li><p>You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$.</p></li></ul><h4 id="3-RMSprop"><a href="#3-RMSprop" class="headerlink" title="3. RMSprop"></a>3. RMSprop</h4><ul><li><p>Blue: Momentum; Green: RMSprop</p><img src="/2021/09/03/Deep-Learning-Optimization-Methods/Screen Shot 2021-09-11 at 11.50.06 AM.png" style="zoom:50%;"></li><li><p>Implementation details:</p><p>On iteration $t$:</p><p>​        Compute $dW, db$ on the current mini-batch</p><p>​        $s_{dW} = \beta_2 s_{dw} + (1 - \beta_2)dW^2$</p><p>​        $s_{db} = \beta_2 s_{db} + (1 - \beta_2)db^2$</p><p>​        $W = W - \alpha \frac{dW}{\sqrt{s_{dW}} + \epsilon}$</p><p>​        $b = b - \alpha \frac{db}{\sqrt{s_{db}} + \epsilon}$</p></li><li><p>Generally, $\epsilon = 10^{-8}$ </p></li><li><p>Make the oscillating small</p></li></ul><h4 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4. Adam"></a>4. Adam</h4><ul><li><p>The combination of <strong>Momentum</strong> and <strong>RMSprop</strong></p></li><li><p>Implementation details:</p><p>$v_{dW} = 0, s_{dW} = 0, v_{db} = 0, s_{db} = 0$</p><p>On iteration $t$:</p><p>​        Compute $dW, db$ on the current mini-batch</p><p>​        $v_{dW} = \beta_1 v_{dW} + (1 - \beta_1)dW, v_{db} = \beta_1 v_{db} + (1 - \beta_1)db$</p><p>​        $s_{dW} = \beta_2 s_{dW} + (1 - \beta_2)dW^2, s_{db} = \beta_2 s_{db} + (1 - \beta_2)db^2$</p><p>​        $v_{dW}^{corected} = \frac{v_{dW}}{1 - \beta_1^t}, v_{db}^{corected} = \frac{v_{db}}{1 - \beta_1^t}$</p><p>​        $s_{dW}^{corrected} = \frac{s_{dW}}{1 - \beta_2^t}, s_{db}^{corrected} = \frac{s_{db}}{1 - \beta_2^t}$</p><p>​        $W = W - \alpha \frac{v_{dW}^{corrected}}{\sqrt{s_{dW}^{corrected}} + \epsilon}, b = b - \alpha \frac{v_{db}^{corrected}}{\sqrt{s_{db}^{corrected}} + \epsilon}$</p></li><li><p>Since the moving average at the beginning of iteration will lead to a large difference from the initial value, we need to correct the deviation of the values.</p></li><li><p>Generally, $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$, $\alpha$ need to be tuned.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Optimization Methods </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gradient Descent Methods</title>
      <link href="2021/09/01/Gradient-Descent-Methods/"/>
      <url>2021/09/01/Gradient-Descent-Methods/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Batch-Gradient-Descent"><a href="#1-Batch-Gradient-Descent" class="headerlink" title="1. (Batch) Gradient Descent"></a>1. (Batch) Gradient Descent</h4><ul><li><p>All the training examples do gradient descent together</p></li><li><p>Advantages: better performance</p></li><li><p>Disadvantages: if dataset is too big, the training process will be slow</p></li><li><p>Code example:</p><pre class="line-numbers language-python"><code class="language-python">X <span class="token operator">=</span> data_inputY <span class="token operator">=</span> labelsparameters <span class="token operator">=</span> initialize_parameters<span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Forward propagation</span>    a<span class="token punctuation">,</span> caches <span class="token operator">=</span> forward_propagation<span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Compute cost.</span>    cost <span class="token operator">+=</span> compute_cost<span class="token punctuation">(</span>a<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Backward propagation.</span>    grads <span class="token operator">=</span> backward_propagation<span class="token punctuation">(</span>a<span class="token punctuation">,</span> caches<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Update parameters.</span>    parameters <span class="token operator">=</span> update_parameters<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h4><ul><li><p>Computing gradients on just one training example at a time, rather than on the whole training set</p></li><li><p>Advantages: speed up convergence</p></li><li><p>Disadvantages: the parameters will “oscillate” toward the minimum rather than converge smoothly</p><p>![](Gradient-Descent-Methods/Screen Shot 2021-09-09 at 2.48.18 PM.png)</p></li><li><p>Code example:</p><pre class="line-numbers language-python"><code class="language-python">X <span class="token operator">=</span> data_inputY <span class="token operator">=</span> labelsparameters <span class="token operator">=</span> initialize_parameters<span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Forward propagation</span>        a<span class="token punctuation">,</span> caches <span class="token operator">=</span> forward_propagation<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Compute cost</span>        cost <span class="token operator">+=</span> compute_cost<span class="token punctuation">(</span>a<span class="token punctuation">,</span> Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Backward propagation</span>        grads <span class="token operator">=</span> backward_propagation<span class="token punctuation">(</span>a<span class="token punctuation">,</span> caches<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Update parameters.</span>        parameters <span class="token operator">=</span> update_parameters<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="3-Mini-batch-Gradient-Descent"><a href="#3-Mini-batch-Gradient-Descent" class="headerlink" title="3. Mini-batch Gradient Descent"></a>3. Mini-batch Gradient Descent</h4><ul><li><p>The number of training examples that do gradient descent at a time is $n (1\lt n\lt m)$</p></li><li><p>If $n = 1$, it becomes stochastic gradient descent; if $n=m$, it becomes batch gradient descent</p></li><li><p>Advantages: more smooth than SGD, faster than BGD</p><p>![](Gradient-Descent-Methods/Screen Shot 2021-09-09 at 3.04.00 PM.png)</p></li><li><p>Disadvantages: one more hyper parameter <strong>batch_size</strong> need to be tuned</p></li><li><p>Code example:</p><ul><li><p>First, random mini batches</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># GRADED FUNCTION: random_mini_batches</span><span class="token keyword">def</span> <span class="token function">random_mini_batches</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> mini_batch_size <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> seed <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Creates a list of random minibatches from (X, Y)        Arguments:    X -- input data, of shape (input size, number of examples)    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)    mini_batch_size -- size of the mini-batches, integer        Returns:    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)    """</span>        np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># To make your "random" minibatches the same as ours</span>    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>                  <span class="token comment" spellcheck="true"># number of training examples</span>    mini_batches <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># Step 1: Shuffle (X, Y)</span>    permutation <span class="token operator">=</span> list<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>permutation<span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">)</span>    shuffled_X <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> permutation<span class="token punctuation">]</span>    shuffled_Y <span class="token operator">=</span> Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> permutation<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span>        inc <span class="token operator">=</span> mini_batch_size    <span class="token comment" spellcheck="true"># Step 2 - Partition (shuffled_X, shuffled_Y).</span>    <span class="token comment" spellcheck="true"># Cases with a complete mini batch size only i.e each of 64 examples.</span>    num_complete_minibatches <span class="token operator">=</span> math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>m <span class="token operator">/</span> mini_batch_size<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># number of mini batches of size mini_batch_size in your partitionning</span>    <span class="token keyword">for</span> k <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_complete_minibatches<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># (approx. 2 lines)</span>        <span class="token comment" spellcheck="true"># mini_batch_X =  </span>        <span class="token comment" spellcheck="true"># mini_batch_Y =</span>        <span class="token comment" spellcheck="true"># YOUR CODE STARTS HERE</span>        mini_batch_X <span class="token operator">=</span> shuffled_X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> k <span class="token operator">*</span> inc <span class="token punctuation">:</span> <span class="token punctuation">(</span>k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> inc<span class="token punctuation">]</span>        mini_batch_Y <span class="token operator">=</span> shuffled_Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> k <span class="token operator">*</span> inc <span class="token punctuation">:</span> <span class="token punctuation">(</span>k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> inc<span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># YOUR CODE ENDS HERE</span>        mini_batch <span class="token operator">=</span> <span class="token punctuation">(</span>mini_batch_X<span class="token punctuation">,</span> mini_batch_Y<span class="token punctuation">)</span>        mini_batches<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mini_batch<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># For handling the end case (last mini-batch &lt; mini_batch_size i.e less than 64)</span>    <span class="token keyword">if</span> m <span class="token operator">%</span> mini_batch_size <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#(approx. 2 lines)</span>        <span class="token comment" spellcheck="true"># mini_batch_X =</span>        <span class="token comment" spellcheck="true"># mini_batch_Y =</span>        <span class="token comment" spellcheck="true"># YOUR CODE STARTS HERE</span>        mini_batch_X <span class="token operator">=</span> shuffled_X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> inc <span class="token operator">*</span> num_complete_minibatches <span class="token punctuation">:</span> m<span class="token punctuation">]</span>        mini_batch_Y <span class="token operator">=</span> shuffled_Y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> inc <span class="token operator">*</span> num_complete_minibatches <span class="token punctuation">:</span> m<span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># YOUR CODE ENDS HERE</span>        mini_batch <span class="token operator">=</span> <span class="token punctuation">(</span>mini_batch_X<span class="token punctuation">,</span> mini_batch_Y<span class="token punctuation">)</span>        mini_batches<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mini_batch<span class="token punctuation">)</span>        <span class="token keyword">return</span> mini_batches<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Second, gradient descent</p><pre class="line-numbers language-python"><code class="language-python">seed <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span>    seed <span class="token operator">=</span> seed <span class="token operator">+</span> <span class="token number">1</span>    minibatches <span class="token operator">=</span> random_mini_batches<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> mini_batch_size<span class="token punctuation">,</span> seed<span class="token punctuation">)</span>    <span class="token keyword">for</span> minibatch <span class="token keyword">in</span> minibatches<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Select a minibatch</span>        <span class="token punctuation">(</span>minibatch_X<span class="token punctuation">,</span> minibatch_Y<span class="token punctuation">)</span> <span class="token operator">=</span> minibatch        <span class="token comment" spellcheck="true"># Forward propagation</span>        AL<span class="token punctuation">,</span> caches <span class="token operator">=</span> forward_propagation<span class="token punctuation">(</span>minibatch_X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Compute cost</span>        cost <span class="token operator">=</span> compute_cost<span class="token punctuation">(</span>AL<span class="token punctuation">,</span> minibatch_Y<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Backward propagation</span>        grads <span class="token operator">=</span> backward_propagation<span class="token punctuation">(</span>AL<span class="token punctuation">,</span> minibatch_Y<span class="token punctuation">,</span> caches<span class="token punctuation">)</span>        parameters <span class="token operator">=</span> update_parameters<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p><strong>batch_size</strong> usually is a power of two, which is more conducive to GPU acceleration</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Optimization Methods </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural-Network Implementation</title>
      <link href="2021/08/24/Neural-Network-Implementation/"/>
      <url>2021/08/24/Neural-Network-Implementation/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true">#Initialize parameters for deep neural network</span><span class="token keyword">def</span> <span class="token function">initialize_parameters_deep</span><span class="token punctuation">(</span>layer_dims<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token triple-quoted-string string">"""  Argument:  layer_dims -- python array (list) containing the dimensions of each layer in network    Returns:  parameters -- python dictionary containing parameters "W1", "b1", ..., "WL", "bL":                                  Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])                                  bl -- bias vector of shape (layer_dims[l], 1)  """</span>  parameters <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  L <span class="token operator">=</span> len<span class="token punctuation">(</span>layer_dims<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># number of layers in the network</span>  <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> L<span class="token punctuation">)</span><span class="token punctuation">:</span>      parameters<span class="token punctuation">[</span><span class="token string">'W'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> layer_dims<span class="token punctuation">[</span>l<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>      parameters<span class="token punctuation">[</span><span class="token string">'b'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>              <span class="token keyword">assert</span><span class="token punctuation">(</span>parameters<span class="token punctuation">[</span><span class="token string">'W'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> layer_dims<span class="token punctuation">[</span>l <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token keyword">assert</span><span class="token punctuation">(</span>parameters<span class="token punctuation">[</span><span class="token string">'b'</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>layer_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">return</span> parameters<span class="token comment" spellcheck="true">#Implement the linear part of a layer's forward propagation.</span><span class="token keyword">def</span> <span class="token function">linear_forward</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    A -- activations from previous layer (or input data): (size of previous layer, number of examples)    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)    b -- bias vector, numpy array of shape (size of the current layer, 1)    Returns:    Z -- the input of the activation function, also called pre-activation parameter     cache -- a python tuple containing "A", "W" and "b" ; stored for computing the backward pass efficiently    """</span>    Z <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W<span class="token punctuation">,</span> A<span class="token punctuation">)</span> <span class="token operator">+</span> b    cache <span class="token operator">=</span> <span class="token punctuation">(</span>A<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        <span class="token keyword">return</span> Z<span class="token punctuation">,</span> cache<span class="token comment" spellcheck="true">#Implement the forward propagation for the LINEAR->ACTIVATION layer</span><span class="token keyword">def</span> <span class="token function">linear_activation_forward</span><span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">,</span> activation<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)    b -- bias vector, numpy array of shape (size of the current layer, 1)    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"    Returns:    A -- the output of the activation function, also called the post-activation value     cache -- a python tuple containing "linear_cache" and "activation_cache";             stored for computing the backward pass efficiently    """</span>        <span class="token keyword">if</span> activation <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>        Z<span class="token punctuation">,</span> linear_cache <span class="token operator">=</span> linear_forward<span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        A<span class="token punctuation">,</span> activation_cache <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>Z<span class="token punctuation">)</span>    <span class="token keyword">elif</span> activation <span class="token operator">==</span> <span class="token string">"relu"</span><span class="token punctuation">:</span>        Z<span class="token punctuation">,</span> linear_cache <span class="token operator">=</span> linear_forward<span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span>        A<span class="token punctuation">,</span> activation_cache <span class="token operator">=</span> relu<span class="token punctuation">(</span>Z<span class="token punctuation">)</span>            cache <span class="token operator">=</span> <span class="token punctuation">(</span>linear_cache<span class="token punctuation">,</span> activation_cache<span class="token punctuation">)</span>    <span class="token keyword">return</span> A<span class="token punctuation">,</span> cache  <span class="token comment" spellcheck="true">#Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation</span><span class="token comment" spellcheck="true">#For other computation, we can change the for loop</span><span class="token keyword">def</span> <span class="token function">L_model_forward</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    X -- data, numpy array of shape (input size, number of examples)    parameters -- output of initialize_parameters_deep()        Returns:    AL -- activation value from the output (last) layer    caches -- list of caches containing:                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)    """</span>    caches <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    A <span class="token operator">=</span> X    L <span class="token operator">=</span> len<span class="token punctuation">(</span>parameters<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>                  <span class="token comment" spellcheck="true"># number of layers in the neural network</span>        <span class="token comment" spellcheck="true"># Implement [LINEAR -> RELU]*(L-1). Add "cache" to the "caches" list.</span>    <span class="token comment" spellcheck="true"># The for loop starts at 1 because layer 0 is the input</span>    <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> L<span class="token punctuation">)</span><span class="token punctuation">:</span>        A_prev <span class="token operator">=</span> A         A<span class="token punctuation">,</span> cache <span class="token operator">=</span> linear_activation_forward<span class="token punctuation">(</span>A_prev<span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">)</span>        caches<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cache<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Implement LINEAR -> SIGMOID. Add "cache" to the "caches" list.</span>    AL<span class="token punctuation">,</span> cache <span class="token operator">=</span> linear_activation_forward<span class="token punctuation">(</span>A<span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"sigmoid"</span><span class="token punctuation">)</span>    caches<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cache<span class="token punctuation">)</span>              <span class="token keyword">return</span> AL<span class="token punctuation">,</span> caches  <span class="token comment" spellcheck="true">#Implement the cost function</span><span class="token keyword">def</span> <span class="token function">compute_cost</span><span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)    Returns:    cost -- cross-entropy cost    """</span>        m <span class="token operator">=</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Compute loss from aL and y.</span>    cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token operator">/</span>m<span class="token operator">*</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>AL<span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>Y<span class="token punctuation">,</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>AL<span class="token punctuation">)</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span>        cost <span class="token operator">=</span> np<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span>        <span class="token keyword">return</span> cost  <span class="token comment" spellcheck="true">#Implement the linear portion of backward propagation for a single layer (layer l)</span><span class="token keyword">def</span> <span class="token function">linear_backward</span><span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> cache<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    dZ -- Gradient of the cost with respect to the linear output (of current layer l)    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer    Returns:    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev    dW -- Gradient of the cost with respect to W (current layer l), same shape as W    db -- Gradient of the cost with respect to b (current layer l), same shape as b    """</span>    A_prev<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b <span class="token operator">=</span> cache    m <span class="token operator">=</span> A_prev<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    dW <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>m<span class="token operator">*</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> A_prev<span class="token punctuation">.</span>T<span class="token punctuation">)</span>    db <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>m<span class="token operator">*</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    dA_prev <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dZ<span class="token punctuation">)</span>        <span class="token keyword">return</span> dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db  <span class="token comment" spellcheck="true">#Implement the backward propagation for the LINEAR->ACTIVATION layer.</span><span class="token keyword">def</span> <span class="token function">linear_activation_backward</span><span class="token punctuation">(</span>dA<span class="token punctuation">,</span> cache<span class="token punctuation">,</span> activation<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    dA -- post-activation gradient for current layer l     cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"        Returns:    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev    dW -- Gradient of the cost with respect to W (current layer l), same shape as W    db -- Gradient of the cost with respect to b (current layer l), same shape as b    """</span>    linear_cache<span class="token punctuation">,</span> activation_cache <span class="token operator">=</span> cache        <span class="token keyword">if</span> activation <span class="token operator">==</span> <span class="token string">"relu"</span><span class="token punctuation">:</span>        dZ <span class="token operator">=</span> relu_backward<span class="token punctuation">(</span>dA<span class="token punctuation">,</span> activation_cache<span class="token punctuation">)</span>        dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db <span class="token operator">=</span> linear_backward<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> linear_cache<span class="token punctuation">)</span>    <span class="token keyword">elif</span> activation <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>        dZ <span class="token operator">=</span> sigmoid_backward<span class="token punctuation">(</span>dA<span class="token punctuation">,</span> activation_cache<span class="token punctuation">)</span>        dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db <span class="token operator">=</span> linear_backward<span class="token punctuation">(</span>dZ<span class="token punctuation">,</span> linear_cache<span class="token punctuation">)</span>        <span class="token keyword">return</span> dA_prev<span class="token punctuation">,</span> dW<span class="token punctuation">,</span> db<span class="token comment" spellcheck="true">#Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group</span><span class="token keyword">def</span> <span class="token function">L_model_backward</span><span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> caches<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    AL -- probability vector, output of the forward propagation (L_model_forward())    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)    caches -- list of caches containing:                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])        Returns:    grads -- A dictionary with the gradients             grads["dA" + str(l)] = ...              grads["dW" + str(l)] = ...             grads["db" + str(l)] = ...     """</span>    grads <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    L <span class="token operator">=</span> len<span class="token punctuation">(</span>caches<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># the number of layers</span>    m <span class="token operator">=</span> AL<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    Y <span class="token operator">=</span> Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>AL<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># after this line, Y is the same shape as AL</span>        <span class="token comment" spellcheck="true"># Initializing the backpropagation</span>    dAL <span class="token operator">=</span> <span class="token operator">-</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>Y<span class="token punctuation">,</span> AL<span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>divide<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> Y<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> AL<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Lth layer (SIGMOID -> LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]</span>    current_cache <span class="token operator">=</span> caches<span class="token punctuation">[</span>L<span class="token number">-1</span><span class="token punctuation">]</span>    dA_prev_temp<span class="token punctuation">,</span> dW_temp<span class="token punctuation">,</span> db_temp <span class="token operator">=</span> linear_activation_backward<span class="token punctuation">(</span>dAL<span class="token punctuation">,</span> current_cache<span class="token punctuation">,</span> <span class="token string">"sigmoid"</span><span class="token punctuation">)</span>    grads<span class="token punctuation">[</span><span class="token string">"dA"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dA_prev_temp    grads<span class="token punctuation">[</span><span class="token string">"dW"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dW_temp    grads<span class="token punctuation">[</span><span class="token string">"db"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> db_temp        <span class="token comment" spellcheck="true"># Loop from l=L-2 to l=0</span>    <span class="token keyword">for</span> l <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>range<span class="token punctuation">(</span>L<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># lth layer: (RELU -> LINEAR) gradients.</span>        <span class="token comment" spellcheck="true"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span>        current_cache <span class="token operator">=</span> caches<span class="token punctuation">[</span>l<span class="token punctuation">]</span>        dA_prev_temp<span class="token punctuation">,</span> dW_temp<span class="token punctuation">,</span> db_temp <span class="token operator">=</span> linear_activation_backward<span class="token punctuation">(</span>dA_prev_temp<span class="token punctuation">,</span> current_cache<span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">)</span>        grads<span class="token punctuation">[</span><span class="token string">"dA"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dA_prev_temp        grads<span class="token punctuation">[</span><span class="token string">"dW"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> dW_temp        grads<span class="token punctuation">[</span><span class="token string">"db"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> db_temp    <span class="token keyword">return</span> grads  <span class="token comment" spellcheck="true">#Update parameters using gradient descent</span><span class="token keyword">def</span> <span class="token function">update_parameters</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    params -- python dictionary containing your parameters     grads -- python dictionary containing your gradients, output of L_model_backward        Returns:    parameters -- python dictionary containing your updated parameters                   parameters["W" + str(l)] = ...                   parameters["b" + str(l)] = ...    """</span>    parameters <span class="token operator">=</span> params<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>    L <span class="token operator">=</span> len<span class="token punctuation">(</span>parameters<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span> <span class="token comment" spellcheck="true"># number of layers in the neural network</span>    <span class="token comment" spellcheck="true"># Update rule for each parameter. Use a for loop.</span>    <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">:</span>        parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">"W"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-</span> learning_rate <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token string">"dW"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> parameters<span class="token punctuation">[</span><span class="token string">"b"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">-</span> learning_rate <span class="token operator">*</span> grads<span class="token punctuation">[</span><span class="token string">"db"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> parameters<span class="token comment" spellcheck="true">#Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.</span><span class="token keyword">def</span> <span class="token function">L_layer_model</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> layers_dims<span class="token punctuation">,</span> learning_rate <span class="token operator">=</span> <span class="token number">0.0075</span><span class="token punctuation">,</span> num_iterations <span class="token operator">=</span> <span class="token number">3000</span><span class="token punctuation">,</span> print_cost<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Arguments:    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).    learning_rate -- learning rate of the gradient descent update rule    num_iterations -- number of iterations of the optimization loop    print_cost -- if True, it prints the cost every 100 steps        Returns:    parameters -- parameters learnt by the model. They can then be used to predict.    """</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    costs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                         <span class="token comment" spellcheck="true"># keep track of cost</span>        <span class="token comment" spellcheck="true"># Parameters initialization.</span>    parameters <span class="token operator">=</span> initialize_parameters_deep<span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Loop (gradient descent)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.</span>        AL<span class="token punctuation">,</span> caches <span class="token operator">=</span> L_model_forward<span class="token punctuation">(</span>X<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span>        cost <span class="token operator">=</span> compute_cost<span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>        grads <span class="token operator">=</span> L_model_backward<span class="token punctuation">(</span>AL<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> caches<span class="token punctuation">)</span>        parameters <span class="token operator">=</span> update_parameters<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span>                        <span class="token comment" spellcheck="true"># Print the cost every 100 iterations</span>        <span class="token keyword">if</span> print_cost <span class="token operator">and</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> i <span class="token operator">==</span> num_iterations <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Cost after iteration {}: {}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> np<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>cost<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> i <span class="token operator">==</span> num_iterations<span class="token punctuation">:</span>            costs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>        <span class="token keyword">return</span> parameters<span class="token punctuation">,</span> costs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Neural Network </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Advice for applying machine learning</title>
      <link href="2021/06/29/Advice-for-applying-machine-learning/"/>
      <url>2021/06/29/Advice-for-applying-machine-learning/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Evaluating-a-Learning-Algorithm"><a href="#1-Evaluating-a-Learning-Algorithm" class="headerlink" title="1. Evaluating a Learning Algorithm"></a>1. Evaluating a Learning Algorithm</h4><h5 id="1-1-Evaluating-a-Hypothesis"><a href="#1-1-Evaluating-a-Hypothesis" class="headerlink" title="1.1 Evaluating a Hypothesis"></a>1.1 Evaluating a Hypothesis</h5><ul><li><p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a <strong>training set</strong> and a <strong>test set</strong>. Typically, the training set consists of $70%$ of data and the test set is the remaining $30%$.</p></li><li><p>The new procedure using these two sets is then:</p><ul><li>Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set</li><li>Compute the test set error $J_{test}(\Theta)$</li></ul></li><li><p><strong>The test set error</strong></p><ul><li><p>For linear regression: $J_{test}(\Theta) = \frac{1}{2m_{test}}\Sigma_{i=1}^{m_{test}}(h_\Theta(x_{test}^{(i)})-y_{test}^{(i)})^2$</p></li><li><p>For classification ~ Misclassification error (aka $0/1$ misclassification error):<br>$$<br>err(h_\Theta(x), y) = \begin{cases}1\quad \text{if } h_\Theta \ge 0.5 \text{ and } y=0 \text{ or } h_\Theta \lt 0.5 \text{ and } y=1\ 0\quad \text{otherwise}\end{cases}<br>$$</p><p>$$<br>\text{Test Error} = \frac{1}{m_{test}}\Sigma_{i=1}^{m_{test}}err(h_\Theta(x_{test}^{(i)}), y_{test}^{(i)})<br>$$</p></li></ul></li></ul><h5 id="1-2-Model-Selection-and-Train-Validation-Test-Sets"><a href="#1-2-Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="1.2 Model Selection and Train/Validation/Test Sets"></a>1.2 Model Selection and Train/Validation/Test Sets</h5><ul><li><p>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could overfit and as a result the predication on the test set would be poor. The error of the hypothesis as measured on the dataset with which you trained the parameters will be lower than the error on any other data set.</p></li><li><p>Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of the hypothesis, test each degree of polynomial and look at the error result.</p></li><li><p>One way to break down the dataset into the three sets is:</p><ul><li>Training set: $60%$</li><li>Cross validation set: $20%$</li><li>Test set: $20%$</li></ul></li><li><p>Calculate three separate error values for the three different sets using the following method:</p><ol><li>Optimize the parameters in $\Theta$ using the training set for each polynomial degree.</li><li>Find the polynomial degree $d$ with the least error using the cross validation set.</li><li>Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);</li></ol></li><li><p>This way, the degree of the polynomial $d$ has not been trained using the test set.</p></li></ul><h4 id="2-Bias-vs-Variance"><a href="#2-Bias-vs-Variance" class="headerlink" title="2. Bias vs. Variance"></a>2. Bias vs. Variance</h4><h5 id="2-1-Diagnosing-Bias-vs-Variance"><a href="#2-1-Diagnosing-Bias-vs-Variance" class="headerlink" title="2.1 Diagnosing Bias vs. Variance"></a>2.1 Diagnosing Bias vs. Variance</h5><ul><li><p>Need to distinguish whether <strong>bias</strong> or <strong>variance</strong> is the problem contributing to bad predictions.</p></li><li><p>High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.</p></li><li><p>The training error will tend to <strong>decrease</strong> as we increase the degree $d$ of the polynomial.</p><p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase $d$ up to a point, and then it will <strong>increase</strong> as $d$ is increased, forming a convex curve.</p></li><li><p><strong>High bias (underfitting)</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$</p></li><li><p><strong>High variance (overfitting)</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$</p></li><li><p>Summurized in the figure below:</p><img src="/2021/06/29/Advice-for-applying-machine-learning/BiasAndVariance.png" style="zoom:50%;"></li></ul><h5 id="2-2-Regularization-and-Bias-Variance"><a href="#2-2-Regularization-and-Bias-Variance" class="headerlink" title="2.2 Regularization and Bias/Variance"></a>2.2 Regularization and Bias/Variance</h5><ul><li><img src="/2021/06/29/Advice-for-applying-machine-learning/RegularizationAndBias:Variance.png" style="zoom:50%;"></li><li><p>In the figure above, as $\lambda$ increases, the fit becomes more rigid. On the other hand, as $\lambda$ approaches $0$, the model tends to overfit the data.</p></li><li><p>In order to choose the model and the regularization term $\lambda$, need to:</p><ol><li>Create a list of lambdas (i.e. $\lambda \in {0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24}$);</li><li>Create a set of models with different degrees or any other variants.</li><li>Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$.</li><li>Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ <strong>without</strong> regularization or $\lambda = 0$.</li><li>Select the best combo that produces the lowest error on the cross validation set.</li><li>Using the best combo $\Theta$ and $\lambda$, apply in on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.</li></ol></li></ul><h5 id="2-3-Learning-Curves"><a href="#2-3-Learning-Curves" class="headerlink" title="2.3 Learning Curves"></a>2.3 Learning Curves</h5><ul><li>Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence<ul><li>As the training set gets larger, the error for a quadratic function increases.</li><li>The error value will plateau out after a certain $m$, or training set size.</li></ul></li><li><strong>Experiencing high bias</strong>:<ul><li><strong>Low training set size</strong>: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.</li><li><strong>Large traning set size</strong>: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta) \approx J_{CV}(\Theta)$.</li><li>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data will not <strong>(by itself)</strong> help much.</li></ul></li><li><strong>Experiencing high variance</strong>:<ul><li><strong>Low training set size</strong>: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.</li><li><strong>Large training set size</strong>: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) \lt J_{CV}(\Theta)$ but the difference between them remains significant.</li><li>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is likely to help.</li></ul></li></ul><h5 id="2-4-Deciding-what-to-do-next-revisited"><a href="#2-4-Deciding-what-to-do-next-revisited" class="headerlink" title="2.4 Deciding what to do next revisited"></a>2.4 Deciding what to do next revisited</h5><ul><li>The decision process can be broken down as follows:<ul><li><strong>Getting more training examples</strong>: Fixes high variance</li><li><strong>Trying smaller sets of features</strong>: Fixes high variance</li><li><strong>Adding features</strong>: Fixes high bias</li><li><strong>Adding polynomial features</strong>: Fixes high bias</li><li><strong>Decreasing $\lambda$</strong>: Fixes high bias</li><li><strong>Increasing $\lambda$</strong>: Fixes high variance</li></ul></li><li><strong>Diagnosing Neural Networks</strong><ul><li>A neural network with fewer parameters is <strong>prone to underfitting</strong>. It is also <strong>Computationally cheaper</strong>.</li><li>A large neural network with more parameters is <strong>prone to overfitting</strong>. It is also <strong>computationally expensive</strong>. In this case, use regularization (increase $\lambda$) to address overfitting.</li><li>Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using cross validation set. Then select the one that performs best.</li></ul></li><li><strong>Model Complexity Effects</strong>:<ul><li>Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</li><li>Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.</li><li>In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="2021/06/22/Support-Vector-Machine/"/>
      <url>2021/06/22/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Optimization-Objective"><a href="#1-Optimization-Objective" class="headerlink" title="1. Optimization Objective"></a>1. Optimization Objective</h4><ul><li><p>$$<br>\min_\theta C \sum_{i=1}^m\left[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})\right] + \frac{1}{2}\sum_{i=1}^n\theta_J^2<br>$$</p></li><li><p>$z = \theta^Tx$</p></li><li><p><img src="/2021/06/22/Support-Vector-Machine/cost1.png" style="zoom:50%;"> <img src="/2021/06/22/Support-Vector-Machine/cost0.png" style="zoom:50%;"></p></li><li><p>if $y = 1$, $\theta^Tx \ge 1$</p><p>if $y=0$, $\theta^Tx\le -1$</p></li></ul><h4 id="2-Decision-Boundary"><a href="#2-Decision-Boundary" class="headerlink" title="2. Decision Boundary"></a>2. Decision Boundary</h4><ul><li><p>$$<br>\begin{align*}<br>&amp; \min_\theta \frac{1}{2}\sum_{i=1}^n\theta_j^2\<br>&amp; s.t.\quad \theta^Tx^{(i)} \ge 1\quad \text{if}\ y^{(i)} = 1\<br>&amp; \quad \quad\ \ \ \theta^Tx^{(i)} \le -1\quad \text{if}\ y^{(i)} = 0<br>\end{align*}<br>$$</p></li><li><p>Linearly Separable case</p><img src="/2021/06/22/Support-Vector-Machine/LinearSeparable.png" style="zoom:50%;"></li><li><img src="/2021/06/22/Support-Vector-Machine/LinearSeparable1.png" style="zoom:50%;"></li></ul><h4 id="3-Kernel"><a href="#3-Kernel" class="headerlink" title="3. Kernel"></a>3. Kernel</h4><ul><li><p>Given $x$, compute new feature depending on proximity to landmarks $l^{(1)}, l^{(2)}, l^{(3)}, \cdots$</p></li><li><p>$f_1 = similarity(x, l^{(1)}) = \exp\left(-\frac{\Vert x-l^{(1)}\Vert^2}{2\sigma^2}\right)$</p><p>if $x \approx l^{(1)}: f_1 \approx \exp\left(-\frac{0^2}{2\sigma^2}\right) \approx 1$</p><p>If $x$ is far from $l^{(1)}: f_1 = \exp\left(-\frac{(\text{large number})^2}{2\sigma^2}\right) \approx 0$</p></li><li><p><strong>SVM with Kernels</strong></p><ul><li><p>Given $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})$</p></li><li><p>Choose $l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, \cdots, l^{(m)} = x^{(m)}$</p></li><li><p>Given example $x$:</p><ul><li>$f_1 = similarity(x, l^{(1)})$</li><li>$f_2 = similarity(x, l^{(2)})$</li><li>$\cdots$</li></ul></li><li><p>For training example $(x^{(i)}, y^{(i)})$</p><ul><li>$x^{(i)}\rightarrow {f_1^{(i)} = sim(x^{(i)}, l^{(1)})\ \vdots \ f_m^{(i)} = sim(x^{(i)}), l^{(m)}}$</li><li>$f^{(i)} = x^{(i)}\f_0^{(i)} = 1$</li></ul></li><li><p>Hypothesis: Given $x$, compute features $f\in R^{m+1}$</p><ul><li><p>Predict “$y=1$” if $\theta^Tf \ge 0$</p></li><li><p>Training<br>$$<br>\min_\theta C \sum_{i=1}^m\left[y^{(i)}cost_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost_0(\theta^Tf^{(i)})\right] + \frac{1}{2}\sum_{i=1}^n\theta_J^2<br>$$</p></li></ul></li></ul></li></ul><h4 id="4-SVM-parameters"><a href="#4-SVM-parameters" class="headerlink" title="4. SVM parameters"></a>4. SVM parameters</h4><ul><li>$C(=\frac{1}{\lambda})$<ul><li>Large $C$: Lower bias, high variance.</li><li>Small $C$: Higher bias, low variance.</li></ul></li><li>$\sigma^2$<ul><li>Large $\sigma^2$: Features $f_i$ vary more smoothly. Higher bias, lower variance.</li><li>Small $\sigma^2$: Features $f_i$ vary less smoothly. Lower bias, higher variance.</li></ul></li></ul><h4 id="5-Logistic-regression-vs-SVMs"><a href="#5-Logistic-regression-vs-SVMs" class="headerlink" title="5. Logistic regression vs. SVMs"></a>5. Logistic regression vs. SVMs</h4><p>$n = \text{number of features }(x\in R^{n+1}), m = \text{number of training examples}$</p><ul><li>If $n$ is large (relative to $m$):<ul><li>Use logistic regression, or SVM without a kernel (“linear kernel”)</li></ul></li><li>If $n$ is small, $m$ is intermediate:<ul><li>Use SVM with Gaussian kernel</li></ul></li><li>If $n$ is small, $m$ is large:<ul><li>Create/add more features, then use logistic regression or SVM without a kernel</li></ul></li><li>Neural network likely to work well for most of these settings, but may be slower to train.</li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network</title>
      <link href="2021/06/16/Neural-Network/"/>
      <url>2021/06/16/Neural-Network/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Model-Representation"><a href="#1-Model-Representation" class="headerlink" title="1. Model Representation"></a>1. Model Representation</h4><ul><li><p>Input: features $x_1\cdots x_n$</p></li><li><p>Output: the result of the hypothesis function.</p></li><li><img src="/2021/06/16/Neural-Network/Neural Network Rep1.png" style="zoom:50%;"></li><li><p>$x_0$ input node is called the “bias unit”. It is always equal to $1$.</p></li><li><p>The same logistic function as in classification, $\frac{1}{1+e^{-\theta^Tx}}$, which is also called sigmoid (logistic) <strong>activation</strong> function.</p></li><li><p>“theta” parameters are called “weights”.</p></li><li><p>A simplistic representation:<br>$$<br>[x_0x_1x_2]\rightarrow [\ ]\rightarrow h_\theta(x)<br>$$</p></li><li><p>Inpur nodes (layer 1), known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p></li><li><p>The intermediate layers of nodes between the input and output layers called the “hidden layers”.</p></li><li><img src="/2021/06/16/Neural-Network/Neural Network Rep2.png" style="zoom:50%;"></li><li><p>We label these intermediate or “hidden” layer nodes $a_0^2\cdots a_n^2$ and call them “activation units”.</p></li><li><p>$$<br>\begin{align*}<br>&amp; a_i^{(j)} = \text{activation}\ \text{of unit}\ i\ \text{in layer}\ j\<br>&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer}\ j \text{ to layer}\ j+1<br>\end{align*}<br>$$</p></li><li><p>$$<br>[x_0x_1x_2x_3]\rightarrow \left[a_1^{(2)}a_2^{(2)}a_3^{(2)}\right]\rightarrow h_\theta(x)<br>$$</p></li><li><p>The values for each of the “activation” nodes is obtained as follows:<br>$$<br>\begin{align*}<br>&amp;&amp; a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_2 + \Theta_{12}^{(1)}x_1 + \Theta_{13}^{(1)}x_3)\<br>&amp;&amp; a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_2 + \Theta_{22}^{(1)}x_1 + \Theta_{23}^{(1)}x_3)\<br>&amp;&amp; a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_2 + \Theta_{32}^{(1)}x_1 + \Theta_{33}^{(1)}x_3)\<br>&amp;&amp; h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})<br>\end{align*}<br>$$</p></li><li><p>The dimensions of matrices of weights is determined as follows:</p><p><strong>If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1}\times (s_j+1)$</strong>.</p><p>The $+1$ comes from the addition in $\Theta^{(j)}$ of the “bias nodes”, $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will.</p></li></ul><h4 id="2-Vectorized-Representation"><a href="#2-Vectorized-Representation" class="headerlink" title="2. Vectorized Representation"></a>2. Vectorized Representation</h4><ul><li><p>Setting $x = a^{(1)}$</p><p>$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$</p></li><li><p>Multiply matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$ (where $s_j$ is the number of activation nodes) by vector $a^{(j-1)}$ with height $n+1$. This gives vector $z^{(j)}$ with height $s_j$. Now the vector of activation nodes for layer $j$ as follows:<br>$$<br>a^{(j)} = g(z^{(j)})<br>$$<br>where function $g$ can be applied element-wise to vector $z^{(j)}$</p></li><li><p>Add a bias unit (equal to $1$) to layer $j$ after we have computed $a^{(j)}$. This will be element $a_0^{(j)}$ and will be equal to $1$.</p></li><li><p>Final result $h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)}) = g(\Theta^{(j)}a^{(j)})$</p></li></ul><h4 id="3-Cost-Function"><a href="#3-Cost-Function" class="headerlink" title="3. Cost Function"></a>3. Cost Function</h4><ul><li><p>First define a few variables need to use:</p><ul><li>$L = \text{total number of layers in the network}$</li><li>$s_l = \text{number of units (not counting bias unit) in layer } l$</li><li>$K = \text{number of output units/classes}$</li></ul></li><li><p>In neural networks, we may have many output nodes. Denote $h_\theta(x)_k$ as being a hypothesis that results in the $k^{th}$ output.</p></li><li><p>The cost function for neural networks is going to be generalization of the one used for logistic regression.<br>$$<br>J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k-1}^{K} \left[y_{k}^{(i)} \log((h_\Theta(x^{(i)}))<em>k) + (1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))<em>k)\right] + \frac{\lambda}{2m} \sum</em>{l=1}^{L-1} \sum</em>{i=1}^{s_l} \sum_{j=1}^{s_l+1}(\Theta_{j, i}^{(i)})^2<br>$$<br>We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.</p></li><li><p>In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.</p></li><li><p>Note:</p><ul><li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer.</li><li>the triple sum simply adds up the squares of all the individual $\Theta$s in the entire network.</li><li>the $i$ in the triple sum does <strong>not</strong> refer to training example $i$.</li></ul></li></ul><h4 id="4-Backpropagation-Algorithm"><a href="#4-Backpropagation-Algorithm" class="headerlink" title="4. Backpropagation Algorithm"></a>4. Backpropagation Algorithm</h4><ul><li><p>“Backpropagation” is neural-network terminology for minimizing the cost function.</p></li><li><p>Compute the partial derivative of $J(\Theta)$:<br>$$<br>\frac{\partial}{\partial \Theta_{i, j}^{(l)}}J(\Theta)<br>$$</p></li><li><p>Procedure:</p><ul><li>Given training examples set ${(x^{(1)}, y^{(1)})\cdots(x^{(m)}, y^{(m)})}$</li><li>Set $\Delta_{ij}^{(l)} = 0$ for all $(l, i, j)$</li><li>For $i = 1$ to $m$:<ul><li>Set $a^{(1)} = x^{(i)}$</li><li>Perform forward propagation to compute $a^{(l)}$ for $l = 2,3,\cdots, L$</li><li>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$</li><li>Compute $\delta^{(L-1)}, \delta^{(L-2)}, \cdots, \delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T\delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1-a^{(l)})$<ul><li>The delta values of layer $l$ are calculated by multiplying the delta values in the next layer with the theta matrix of layer $l$. We then element-wise multiply that with a function called $g^{\prime}$, or g-prime, which is the derivative of the activation function $g$ evaluated with the input values given by $z^{(l)}$.</li></ul></li><li>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$</li></ul></li><li>$D_{ij}^{(l)} := \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)} \text{ if } j \ne 0$</li><li>$D_{ij}^{(l)} := \Delta_{ij}^{(l)} \text{ if } j = 0$</li></ul></li><li><p>Intuitively, $\delta_j^{(l)}$ is the “error” for $a_j^{(l)}$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function: $\delta_j^{(l)} = \frac{\partial}{\partial z_j^{(l)}}cost(t)$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Overfitting</title>
      <link href="2021/06/09/Overfitting/"/>
      <url>2021/06/09/Overfitting/</url>
      
        <content type="html"><![CDATA[<h4 id="1-The-Problem-of-Overfitting"><a href="#1-The-Problem-of-Overfitting" class="headerlink" title="1. The Problem of Overfitting"></a>1. The Problem of Overfitting</h4><ul><li><strong>Underfitting</strong>, or high bias, is when the form of our hypothesis function $h$ maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.</li><li>At the other extreme, <strong>overfitting</strong>, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</li><li>Two main options to address the issue of overfitting:<ul><li>Reduce the number of features:<ul><li>Manually select which features to keep.</li><li>Use a model selection algorithm.</li></ul></li><li>Regularization<ul><li>Keep all the features, but reduce the magnitude of parameters $\theta_j$.</li><li>Regularization works well when we have a lot of slightly useful features.</li></ul></li></ul></li></ul><h4 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h4><ul><li><p>If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.</p></li><li><p>$$<br>\min_\theta\frac{1}{2m}\Sigma_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \Sigma_{j=1}^n \theta_j^2<br>$$</p></li><li><p>The $\lambda$, or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.</p></li><li><p>Using the above cost function with extra summation, we can smooth the output of our hypothesis function to reduce overfitting.</p></li><li><p>If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p></li></ul><h4 id="3-Regularized-Linear-Regression"><a href="#3-Regularized-Linear-Regression" class="headerlink" title="3. Regularized Linear Regression"></a>3. Regularized Linear Regression</h4><ul><li><p><strong>Gradient Descent</strong><br>$$<br>\begin{align*}<br>&amp; Repeat\ {\<br>&amp; \quad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\<br>&amp; \quad\theta_j := \theta_j - \alpha\left[ \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\right) + \frac{\lambda}{m}\theta_j\right] \quad\quad j\in{1, 2\cdots n}\<br>&amp; }<br>\end{align*}<br>$$</p></li><li><p>The term $\frac{\lambda}{m}\theta_j$ performs regularization. With some manipulation, the update rule can also be represented as:<br>$$<br>\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\Sigma_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$<br>The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than $1$. Intuitively reduce the value of $\theta_j$ by some amount on every update.</p></li><li><p><strong>Normal Equation</strong><br>$$<br>\theta = (X^TX+\lambda L)^{-1}X^Ty\quad<br>where\ L = \begin{bmatrix} 0 \ \ &amp; 1 \ \ &amp;\ &amp; 1 \ \ &amp; \ &amp; \ &amp; \ddots \ \ &amp; \ &amp; \ &amp;\ &amp; 1\end{bmatrix}<br>$$<br>$L$ is a matrix with $0$ at the top left and $1$’s down the diagonal, with $0$’s everywhere else. It should have dimension $(n+1)\times (n+1)$. Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number $\lambda$.</p></li><li><p>When we add the term $\lambda L$, then $X^TX + \lambda L$ becomes invertible.</p></li></ul><h4 id="4-Regularized-Logistic-Regression"><a href="#4-Regularized-Logistic-Regression" class="headerlink" title="4. Regularized Logistic Regression"></a>4. Regularized Logistic Regression</h4><ul><li><p>Cost function<br>$$<br>J(\theta) = -\frac{1}{m}\Sigma_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i )}))] + \frac{\lambda}{m}\Sigma_{j=1}^n\theta_j^2<br>$$<br>The second sum, $\Sigma_{j=1}^n\theta_j^2$ <strong>means to explicitly exclude</strong> the bias term $\theta_0$.</p></li><li><p><strong>Gradient Descent</strong><br>$$<br>\begin{align*}<br>&amp; Repeat\ {\<br>&amp; \quad\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}\<br>&amp; \quad\theta_j := \theta_j - \alpha\left[ \left( \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\right) + \frac{\lambda}{m}\theta_j\right] \quad\quad j\in{1, 2\cdots n}\<br>&amp; }<br>\end{align*}<br>$$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression -- Classification</title>
      <link href="2021/06/08/Logistic-Regression-Classification/"/>
      <url>2021/06/08/Logistic-Regression-Classification/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Classification"><a href="#1-Classification" class="headerlink" title="1. Classification"></a>1. Classification</h4><ul><li>The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.</li><li><strong>Binary classification problem</strong><ul><li>$y$ can take on only two values, $0$ and $1$. (Multi-class later)</li><li>$0$ is called the negative class, and $1$ the positive class, and they are sometimes also denoted by the symbols “-“ and “+”.</li><li>Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the <strong>label</strong> for the training example.</li></ul></li></ul><h4 id="2-Hypothesis-Representation"><a href="#2-Hypothesis-Representation" class="headerlink" title="2. Hypothesis Representation"></a>2. Hypothesis Representation</h4><ul><li><p>By using the “Sigmoid Function”, also called the “Logistic Function”:<br>$$<br>\begin{align*}<br>&amp; h_{\theta}(x) = g(\theta^Tx)\<br>&amp; z = \theta^Tx\<br>&amp; g(z) = \frac{1}{1+e^{-z}}<br>\end{align*}<br>$$</p></li><li><p>The function $g(z)$, maps any real number to the $(0,1)$ interval, making it useful for transforming an arbitrary-valued function into a function suited for classification.</p></li><li><p>$h_{\theta}(x)$ will give us the <strong>probability</strong> that our output is $1$. The probability that the prediction is $0$ is just the complement of the probability that it is $1$.<br>$$<br>\begin{align*}<br>&amp; h_{\theta}(x) = P(y=1\mid x;\theta) = 1 - P(y=0\mid x;\theta)\<br>&amp; P(y=0\mid x;\theta) + P(y=1\mid x;\theta) = 1<br>\end{align*}<br>$$</p></li></ul><h4 id="3-Decision-Boundary"><a href="#3-Decision-Boundary" class="headerlink" title="3. Decision Boundary"></a>3. Decision Boundary</h4><ul><li><p>$$<br>\theta^Tx\ge 0 \Rightarrow y=1\<br>\theta^Tx\lt 0 \Rightarrow y=0<br>$$</p></li><li><p>The <strong>decision boundary</strong> is the line that separates the area where $y = 1$ and where $y = 0$. It is created by the hypothesis function.</p></li><li><p>The input to the sigmoid function $g(z)$ (e.g. $\theta^TX$) doesn’t need to be linear, and could be a function that describes a circle or any shape to fit our data.</p></li></ul><h4 id="4-Cost-Function"><a href="#4-Cost-Function" class="headerlink" title="4. Cost Function"></a>4. Cost Function</h4><ul><li><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p></li><li><p>Instead, our cost function for logistic regression looks like:<br>$$<br>\begin{align*}<br>&amp; J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})\<br>&amp; Cost(h_\theta(x), y) = -\log(h_\theta(x))\quad\quad if\ y = 1\<br>&amp; Cost(h_\theta(x), y) = -\log(1-h_\theta(x))\ if\ y = 0<br>\end{align*}<br>$$</p></li><li><p>$$<br>\begin{align*}<br>&amp; Cost(h_{\theta}(x), y) = 0\ if\ h_\theta(x) = y\<br>&amp; Cost(h_\theta(x), y) \rightarrow \infin\ if\ y = 0\ and\ h_\theta(x)\rightarrow 1\<br>&amp; Cost(h_\theta(x), y) \rightarrow \infin\ if\ y = 1\ and\ h_\theta(x) \rightarrow 0<br>\end{align*}<br>$$</p></li><li><p>If the correct answer ‘$y$’ is $0$, then the cost function will be $0$ if our hypothesis function also ouputs $0$. If the hypothesis approaches $1$, then the cost function will approach infinity. If the correct answer ‘$y$’ is $1$, the case is reverse.</p></li><li><p>Not that writing the cost function in this way guarantees that $J(\theta)$ is convex for logistic regression.</p></li></ul><h4 id="5-Simplified-Cost-Function"><a href="#5-Simplified-Cost-Function" class="headerlink" title="5. Simplified Cost Function"></a>5. Simplified Cost Function</h4><ul><li><p>Compress the cost function’s two conditional cses into one case:<br>$$<br>Cost(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))<br>$$</p></li><li><p>Cost function<br>$$<br>J(\theta) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))]<br>$$</p></li><li><p>A vectorized implementation:<br>$$<br>h = g(X\theta)\<br>J(\theta) = \frac{1}{m}(-y^T\log(h) - (1-y)^T\log(1-h))<br>$$</p></li></ul><h4 id="6-Gradient-Descent"><a href="#6-Gradient-Descent" class="headerlink" title="6. Gradient Descent"></a>6. Gradient Descent</h4><ul><li><p>$$<br>\begin{align*}<br>&amp; Repeat\ { \<br>&amp; \theta_j := \theta_j - \frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\<br>&amp; }<br>\end{align*}<br>$$</p></li><li><p>A vectorized implementation:<br>$$<br>\theta := \theta - \frac{\alpha}{m}X^T(g(X\theta) - \vec{y})<br>$$</p></li></ul><h4 id="7-Advanced-Optimization"><a href="#7-Advanced-Optimization" class="headerlink" title="7. Advanced Optimization"></a>7. Advanced Optimization</h4><ul><li>“Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize $\theta$ that can be used instead of gradient descent.</li></ul><h4 id="8-Multiclass-Classification-One-vs-all"><a href="#8-Multiclass-Classification-One-vs-all" class="headerlink" title="8. Multiclass Classification: One-vs-all"></a>8. Multiclass Classification: One-vs-all</h4><ul><li><p>Instead of $y = {0, 1}$, we will expand the definition so that $y = {0, 1\cdots n}$.</p></li><li><p>Since $y = {0, 1\cdots n}$, we divide the problem into $n+1$ binary classification problems; in each one, predict the probability that ‘$y$’ is a member of one of our classes.</p></li><li><p>$$<br>\begin{align*}<br>&amp; y\in {0, 1\cdots n}\<br>&amp; h_\theta^{(0)}(x) = P(y = 0\mid x;\theta)\<br>&amp; h_\theta^{(1)}(x) = P(y = 1\mid x;\theta)\<br>&amp; \cdots\<br>&amp; h_\theta^{(n)}(x) = P(y = n\mid x;\theta)\<br>&amp; prediction = \max_{i}(h_\theta^{(i)}(x))<br>\end{align*}<br>$$</p></li><li><p>We are basically choosing one class and then lumping all the others into a single second class. Do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Priority Queue</title>
      <link href="2021/06/02/Priority-Queue/"/>
      <url>2021/06/02/Priority-Queue/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A priority queue is an abstract data type for storing a collection of prioritized elements that supports<ul><li>arbitrary element insertion,</li><li>removal of elements in order of priority (the element with the first priority can be removed at any time)</li></ul></li></ul><h4 id="2-Priority-Queue-ADT"><a href="#2-Priority-Queue-ADT" class="headerlink" title="2. Priority Queue ADT"></a>2. Priority Queue ADT</h4><ul><li>A priority queue stores a collection of entries</li><li>Each entry is a pair (key, value)</li><li><strong>Entry ADT</strong><ul><li>An <em>entry</em> in a priority queue is simply a key-value pair</li><li>Priority queues store entries to allow for efficient insertion and removal based on keys</li></ul></li></ul><h4 id="3-Sequence-based-Priority-Queue"><a href="#3-Sequence-based-Priority-Queue" class="headerlink" title="3. Sequence-based Priority Queue"></a>3. Sequence-based Priority Queue</h4><ul><li>Implementation with an unsorted list<ul><li><em>insert</em> takes $O(1)$ time since we can insert the item at the beginning or end of the sequence</li><li><em>removeMin</em> and <em>min</em> take $O(n)$ time since we have to traverse the entire sequence to find the smallest key</li></ul></li><li>Implementation with a sorted list<ul><li><em>insert</em> takes $O(n)$ time since we have to find the place where to insert the item</li><li><em>removeMin</em> and <em>min</em> take $O(1)$ time, since the smallest key is at the beginning</li></ul></li></ul><h4 id="4-Priority-Queue-Sorting"><a href="#4-Priority-Queue-Sorting" class="headerlink" title="4. Priority Queue Sorting"></a>4. Priority Queue Sorting</h4><ul><li>Use a priority queue to sort a list of comparable elements<ul><li>Insert the elements one by one with a series of <em>insert</em> operations</li><li>Remove the elements in sorted order with a series of <em>removeMin</em> operations</li></ul></li><li>Selection-Sort<ul><li>Selection-sort is the variation of PQ-sort where the prioroty queue is implemented with an unsorted sequence</li><li>Running time of Selection-sort:<ul><li>Inserting the elements into the priority queue with $n$ insert operations takes $O(n)$ time</li><li>Removing the elements in sorted order from the priority queue with $n$ <em>removeMin</em> operations takes time proportional to $1+2+\cdots + n$</li></ul></li><li>Selection-sort runs in $O(n^2)$ time</li></ul></li><li>Insertion-Sort<ul><li>Insertion-sort is the variation of PQ-sort where the priority queue is implemented with a sorted sequence</li><li>Running time of Insertion-sort:<ul><li>Inserting the elements into the priority queue with $n$ <em>insert</em> operations takes time proportional to $1+2+\cdots +n$</li><li>Removing the elements in sorted order from the priority queue with a series of $n$ <em>removeMin</em> operations takes $O(n)$ time</li></ul></li><li>Insertion-sort runs in $O(n^2)$ time</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="2021/06/01/Linear-Regression/"/>
      <url>2021/06/01/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h5 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h5><ul><li>$x^{(i)}$ to denote the “input” variables, also called input features.</li><li>$y^{(i)}$ to denote the “output” or target variable that we are trying to predict.</li><li>A pair $(x^{(i)}, y^{(i)})$ is called a training example.</li><li>The dataset that we’ll be using to learn - a list of $m$ training examples $(x^{(i)}, y^{(i)}); i = 1, \cdots, m$ - is called a training set.</li><li>Note that the superscript “$(i)$” in the notation is simply an index into the training set, and has nothing to do with exponentiation.</li><li>$X$ to denote the space of input values.</li><li>$Y$ to denote the space of output values.</li></ul><h4 id="1-Model-Representation"><a href="#1-Model-Representation" class="headerlink" title="1. Model Representation"></a>1. Model Representation</h4><ul><li>Given a training set, to learn a function $h: X\rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. $h$ is called a hypothesis.</li><li>When the target variable that we’re trying to predict is continuous, we call the learning problem a regression problem.</li></ul><h4 id="2-Cost-Function-Loss-Function"><a href="#2-Cost-Function-Loss-Function" class="headerlink" title="2. Cost Function (Loss Function)"></a>2. Cost Function (Loss Function)</h4><ul><li><p>This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from $x$’s and the actual output $y$’s.</p></li><li><p>$$<br>J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y_i} - y_i)^2 = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2<br>$$</p></li><li><p>This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $(\frac{1}{2})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\frac{1}{2}$ term.</p></li><li><p>As a goal, we should try to minimize the cost function.</p></li></ul><h4 id="3-Gradient-Descent"><a href="#3-Gradient-Descent" class="headerlink" title="3. Gradient Descent"></a>3. Gradient Descent</h4><ul><li><p>The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter $\alpha$, which is called the learning rate.</p></li><li><p>A smaller $\alpha$ would result in a smaller step and a larger $\alpha$ results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J(\theta_0, \theta_1)$.</p></li><li><p>The gradient descent algorithm is:</p><ul><li><p>repeat until convergence:<br>$$<br>\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)<br>$$<br>where</p><p>$j = 0, 1$ represents the feature index number.</p></li></ul></li><li><p>At each iterarion $j$, one should simultaneously update the parameters $\theta_1, \theta_2, \cdots, \theta_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation.</p></li><li><p>We should adjust our parameter $\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p></li><li><p>Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed. As we approach a local optimum, gradient descent will automatically take smaller steps. So, no need to decrese $\alpha$ over time.</p></li></ul><h4 id="4-Multiple-Features"><a href="#4-Multiple-Features" class="headerlink" title="4. Multiple Features"></a>4. Multiple Features</h4><ul><li><p>Linear regression with multiple variables is also known as <strong>“multivariate linear regression”</strong>.</p></li><li><p>$x^{(i)}_j = $ value of feature $j$ in the $i^{th}$ training example</p></li><li><p>$x^{(i)} = $ the input (features) of the $i^{th}$ training example</p></li><li><p>$m = $ the number of training examples</p></li><li><p>$n = $ the number of features</p></li><li><p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:<br>$$<br>h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \cdots + \theta_nx_n<br>$$</p></li><li><p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:<br>$$<br>h_{\theta}(x) = \begin{bmatrix}\theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n\end{bmatrix} \begin{bmatrix}x_0\x_1\ \vdots\ x_n\end{bmatrix} = \theta^Tx<br>$$</p></li></ul><h4 id="5-Gradient-Descent-for-Multiple-Variables"><a href="#5-Gradient-Descent-for-Multiple-Variables" class="headerlink" title="5. Gradient Descent for Multiple Variables"></a>5. Gradient Descent for Multiple Variables</h4><ul><li>repeat until convergence:<br>$$<br>\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\times x_j^{(i)}\quad for\ j := 0\cdots n<br>$$</li></ul><h4 id="6-Gradient-Descent-in-Practice-Feature-Scaling"><a href="#6-Gradient-Descent-in-Practice-Feature-Scaling" class="headerlink" title="6. Gradient Descent in Practice - Feature Scaling"></a>6. Gradient Descent in Practice - Feature Scaling</h4><ul><li><p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p></li><li><p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally: $-1 \le x_{(i)} \le 1$ or $-0.5 \le x_{(i)} \le 0.5$. These aren’t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p></li><li><p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown this formula:<br>$$<br>x_i := \frac{x_i - \mu_i}{s_i}<br>$$<br>Where $\mu_i$ is the <strong>average</strong> of all the values for feature $(i)$ and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.</p></li></ul><h4 id="7-Gradient-Descent-in-Practice-Learning-Rate"><a href="#7-Gradient-Descent-in-Practice-Learning-Rate" class="headerlink" title="7. Gradient Descent in Practice - Learning Rate"></a>7. Gradient Descent in Practice - Learning Rate</h4><ul><li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent. If $J(\theta)$ ever increases, then you probably need to decrease $\alpha$.</li><li><strong>Automatic convergence test.</strong> Declare convergence if $J(\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it’s difficult to choose this threshold value.</li><li>It has been proven that if learning rate $\alpha$ is sufficiently small, then $J(\theta)$ will decrease on every iteration.</li><li>To summarize:<ul><li>If $\alpha$ is too small: slow convergence.</li><li>If $\alpha$ is too large: may not decrease on every iteration and thus may not converge.</li></ul></li></ul><h4 id="8-Features-and-Polynomial-Regression"><a href="#8-Features-and-Polynomial-Regression" class="headerlink" title="8. Features and Polynomial Regression"></a>8. Features and Polynomial Regression</h4><ul><li>We can improve our features and the form of our hypothesis function in a couple different ways.</li><li>We can <strong>combine</strong> multiple features into one.</li><li><strong>Polynomial Regression</strong><ul><li>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</li><li>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</li><li>For example, if our hypothesis function is $h_{\theta}(x) = \theta_0 + \theta_1 x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$.</li><li>One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.</li></ul></li></ul><h4 id="9-Normal-Equation"><a href="#9-Normal-Equation" class="headerlink" title="9. Normal Equation"></a>9. Normal Equation</h4><ul><li><p>A second way of minimizing $J$.</p></li><li><p>In the “Normal Equation” method, we will minimize $J$ by explicitly taking its derivatives with respect to the $\theta_j$’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$</p></li><li><p>There is <strong>no need</strong> to do feature scaling with the normal equation.</p></li><li><p>The comparison of gradient descent and the normal equation:</p><img src="/2021/06/01/Linear-Regression/Screen Shot 2021-06-04 at 10.02.03 PM.png" style="zoom:50%;"></li><li><p>With the normal equation, computing the inversion has complexity $O(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, when $n$ exceeds $10,000$ it might be a good time to go from a normal solution to an iterative process.</p></li><li><p><strong>Noninvertibility</strong></p><ul><li>If $X^TX$ is <strong>noninvertible</strong>, the common causes might be having:<ul><li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li><li>Too many features (e.g. $m\le n$). In this case, delete some features or use “regularization”.</li></ul></li><li>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> self-study notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Red-Black Tree</title>
      <link href="2021/05/26/Red-Black-Tree/"/>
      <url>2021/05/26/Red-Black-Tree/</url>
      
        <content type="html"><![CDATA[<h2 align="center">Red-Black Trees</h2><h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A red-black tree is a binary search tree that satisfies the following properties:<ul><li><strong>Root Property</strong>: the root is black</li><li><strong>External Property</strong>: every leaf is black</li><li><strong>Internal/Red Property</strong>: the children of a red node are black</li><li><strong>Depth Property</strong>: all the leaves have the same <em>black depth</em>, defined as the number of proper ancestors that are black</li></ul></li></ul><h4 id="2-From-Red-Black-to-2-4-Trees"><a href="#2-From-Red-Black-to-2-4-Trees" class="headerlink" title="2. From Red-Black to $(2,4)$ Trees"></a>2. From Red-Black to $(2,4)$ Trees</h4><ul><li>Given a red-black tree, we can construct a corresponding $(2,4)$ tree:<ul><li>merge every red node $w$ into its parent, storing the entry from $w$ at its parent</li><li>the children of $w$ become ordered children of the parent</li></ul></li><li>Depth Property:<ul><li>$(2,4)$ Tree: all the external nodes have the same depth</li><li>Red-Black Tree: all the leaves have the same <em>black depth</em></li></ul></li><li>A red-black tree is a representation of a $(2,4)$ tree by means of a binary tree whose nodes are colored red or black</li><li>In comparison with its associated $(2,4)$ tree, a red-black tree has<ul><li>same logarithmic time performance</li><li>simpler implementation with a single node type</li></ul></li></ul><h4 id="3-Height-of-a-Red-Black-Tree"><a href="#3-Height-of-a-Red-Black-Tree" class="headerlink" title="3. Height of a Red-Black Tree"></a>3. Height of a Red-Black Tree</h4><ul><li>Theorem: A red-black tree storing $n$ items has height $O(\log n)$<ul><li>Proof:<ul><li>The height of a red-black tree is at most twice the height of its associated $(2,4)$ tree, which is $O(\log n)$</li></ul></li></ul></li></ul><h4 id="4-Search"><a href="#4-Search" class="headerlink" title="4. Search"></a>4. Search</h4><ul><li>The search algorithm for a red-black tree is the same as that for a binary search tree</li><li>Searching in a red-black tree takes $O(\log n)$ time</li></ul><h4 id="5-Insertion"><a href="#5-Insertion" class="headerlink" title="5. Insertion"></a>5. Insertion</h4><ul><li><p>To insert $(k, o)$, we execute the insertion algorithm for binary search trees and color <strong>red</strong> the newly inserted node $z$ <em>unless it is the root</em></p><ul><li>We preserve the root, external, and depth properties</li><li>If the parent $v$ of $z$ is black, we also preserve the internal property and we are done</li><li>Else ($v$ is red) we have a <strong>double red</strong> (i.e., a violation of the internal property), which requires a reorganization of the tree</li></ul></li><li><p><strong>Remedying a Double Red</strong></p><ul><li>Consider a double red with child $z$ and parent $v$, and let $w$ be the sibling of $v$</li><li>Case1: $w$ is black<ul><li>The double red is an incorrect replacement of a 4-node</li><li><strong>Restructuring</strong>: we change the 4-node replacement</li></ul></li><li>Case2: $w$ is red<ul><li>The double red corresponds to an overflow</li><li><strong>Recoloring</strong>: we perform the equivalent of a <strong>split</strong></li></ul></li></ul></li><li><p><strong>Restructuring</strong></p><ul><li><p>A restructuring remedies a child-parent double red when the parent red node has a black sibling</p></li><li><p>It is equivalent to restoring the <em>correct replacement</em> of a 4-node</p></li><li><p>The internal property is restored and the other properties are preserved</p></li><li><p>There are four restructuring configrations depending on whether the double red nodes are left or right children</p><img src="/2021/05/26/Red-Black-Tree/Screen Shot 2021-05-10 at 7.22.25 PM.png" style="zoom:50%;"></li></ul></li><li><p><strong>Recoloring</strong></p><ul><li>A recoloring remedies a child-parent double red when the parent red node has a red sibling</li><li>The parent $v$ and its sibling $w$ become black and the grandparent $u$ becomes red, unless it is the root</li><li>It is equivalent to performing a split on a 5-node</li><li><em>The double red violation may propagate to the grangparent $u$</em></li></ul></li><li><p>Analysis of Insertion</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Algorithm insert(k, o)1. We search for key k to locate the insertion node z2. We add the new entry (k, o) at node z and color z red3. while doubleRed(z)        if isBlack(sibling(parent(z)))            z <- restructure(z)            return        else sibling(parent(z)) is red            z <- recolor(z)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Recall that a red-black tree has $O(\log n)$ height</p></li><li><p>Step 1 takes $O(\log n)$ time because we visit $O(\log n)$ nodes</p></li><li><p>Step 2 takes $O(1)$ time</p></li><li><p>Step 3 takes $O(\log n)$ time because we perform</p><ul><li>$O(\log n)$ recoloring, each taking $O(1)$ time, and</li><li>at most one restructuring taking $O(1)$ time</li></ul></li><li><p>Thus, an insertion in a red-black tree takes $O(\log n)$ time</p></li></ul></li></ul><h4 id="6-Deletion"><a href="#6-Deletion" class="headerlink" title="6. Deletion"></a>6. Deletion</h4><ul><li>To perform operation $remove(k)$, we first execute the deletion algorithm for binary search trees</li><li>Let $v$ be the internal node removed, $w$ the external node removed, and $r$ the sibling of $w$<ul><li>If $v$ was red, the resulting tree remains a valid red-black tree</li><li>If $v$ was black and $r$ was red, we color $r$ black and we are done</li><li>Else ($v$ and $r$ were both black) we color $r$ <em><strong>double black</strong></em>, to preserve the depth property</li></ul></li><li><strong>Remedying a Double Black</strong><ul><li>The algorithm for remedying a double black node $r$ with sibling $y$ considers three cases</li><li>Case1: $y$ is black and has a red child<ul><li>We perform a <strong>restructuring</strong>, equivalent to a <strong>transfer</strong>, and we are done.</li></ul></li><li>Case2: sibling $y$ of $r$ is black and its children are both black<ul><li>We perform a <strong>recoloring</strong>, equivalent to a <strong>fusion</strong>, which may propagate up the double black violation</li></ul></li><li>Case3: $y$ is red<ul><li>We perform an <strong>adjustment</strong>, equivalent to choosing a different representation of a 3-node, after which either Case 1 or Case 2 applies</li></ul></li></ul></li><li>Deletion in a red-black tree takes $O(\log n)$ time</li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Formal Languages</title>
      <link href="2021/05/15/Formal-Languages/"/>
      <url>2021/05/15/Formal-Languages/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Basic-Concepts"><a href="#1-Basic-Concepts" class="headerlink" title="1. Basic Concepts"></a>1. Basic Concepts</h4><ul><li><p><strong>Alphabet</strong>: a finite, nonempty set $\Sigma$ of symbols.</p></li><li><p><strong>String</strong>: a finite sequence of symbols from $\Sigma$</p></li><li><p>substring, prefix and suffix</p></li><li><p>The empty string has length 0 and is written as $\lambda$</p><ul><li>It is a prefix, substring and suffix of any given string</li><li>Operations (suppose <strong>v = aba</strong> and <strong>w = abaaa</strong>):<ul><li>Concatenation: <strong>vw = abaabaaa</strong></li><li>Reverse: <strong>w<sup>R</sup> = aaaba</strong></li><li>Repetition: <strong>v<sup>2</sup> = abaaba</strong> and <strong>v<sup>0</sup> = $\lambda$</strong></li></ul></li></ul></li><li><p>If $\Sigma$ is an alphabet, then we use $\Sigma^{<em>}$ to denote the set of strings obtained by concatenating zero or more symbols from $\Sigma$. The set $\Sigma^{</em>}$ always contains $\lambda$.</p><ul><li>$\Sigma^{*}$ = the set of <strong>all strings</strong> formed by concatenating zero or more symbols in $\Sigma$</li><li>$\Sigma^{+}$ = the set of all <strong>non-empty</strong> strings formed by concatenating symbols in $\Sigma$</li><li>$\Sigma^{+} = \Sigma^{*} - {\lambda}$</li></ul></li><li><p>A <strong>formal language L</strong> is any subset of $\Sigma^{*}$</p><ul><li>The <strong>complement</strong> of <strong>L</strong> is: $\overline{L} = \Sigma^{*} - L$</li><li>Reverse: $L^{R} = {w^{R}, w\in L}$</li><li>Concatenation: $L_{1}L_{2} = {xy: x\in L_{1}, y \in L_{2}}$</li><li>$L^{n}$ as $L$ concatenated with itself $n$ times<ul><li>$L^{0} = {\lambda}$</li><li>$L^{1} = L$</li></ul></li><li><strong>Star-closure</strong>: $L^{*} = L^{0}\bigcup L^{1}\bigcup L^{2}\cdots$</li><li><strong>Positive-closure</strong>: $L^{+} = L^{1}\bigcup L^2\cdots$</li><li>空集和${\lambda}$都是一种语言</li></ul></li><li><p>A grammar G is defined as a quadruple $G = (V, T, S, P)$ (the sets $V$ and $T$ are non-empty and disjoint):</p><ul><li>$V$: a finite set of objects called <strong>variables</strong> or non-terminal symbols</li><li>$T$: a finite set of objects called <strong>terminal symbols</strong></li><li>$S\in V$: a special symbol called the <strong>start</strong> symbol</li><li>$P$: a finite set of <strong>productions</strong></li><li>Production rules are of the form: $x\rightarrow y$, where $x$ is an element of $(V\cup T)^+$ and $y$ is in $(V\cup T)^*$</li></ul></li><li><p>For a given grammar G, the language generated by G, L(G), is the set of all strings derived from the start  symbol.</p><ul><li>$L(G) = {w\in T^*: S \stackrel*\Rightarrow w}$</li><li>If $w\in L(G)$, then the sequence $S\Rightarrow w_1 \Rightarrow w_2 \Rightarrow \cdots \Rightarrow w_n \Rightarrow w$ is a <strong>derivation</strong> of the sentence $w$. The strings $S, w_1, w_2, \cdots, w_n$, which contain variables as well as terminals, are called <strong>sentential forms</strong> of the derivation</li></ul></li><li><p>Automata</p><ul><li>An automaton is an abstract model of a digital computer</li><li>An automaton consists of<ul><li>An input mechanism</li><li>A control unit</li><li>Possibly, a storage mechanism</li><li>Possibly, an output mechanism</li></ul></li></ul></li></ul><h4 id="2-Finite-Automata"><a href="#2-Finite-Automata" class="headerlink" title="2. Finite Automata"></a>2. Finite Automata</h4><h5 id="2-1-Deterministic-Finite-Acceptors"><a href="#2-1-Deterministic-Finite-Acceptors" class="headerlink" title="2.1 Deterministic Finite Acceptors"></a>2.1 Deterministic Finite Acceptors</h5><ul><li><p>A <strong>deterministic finite acceptor</strong> or <strong>dfa</strong> is defined by the quintuple<br>$$<br>M = (Q, \Sigma, \delta, q_0, F)<br>$$<br>where</p><p>​        $Q$ is a finite set of <strong>internal states</strong>,</p><p>​        $\Sigma$ is a finite set of symbols called the <strong>input alphabet</strong>,</p><p>​        $\delta : Q\times \Sigma \rightarrow Q$ is a total function called the <strong>transition function</strong>,</p><p>​        $q_0 \in Q$ is the <strong>initial state</strong>,</p><p>​        $F\subseteq Q$ is a set of <strong>finite states</strong>.</p></li><li><p>Extended Transition Function: $\delta^*$<br>$$<br>\delta^*: Q \times \Sigma^* \rightarrow Q<br>$$<br>For any given $q \in Q, w \in \Sigma^*, a \in \Sigma$:</p><p>​    $\delta^*(q,\lambda) = q$</p><p>​    $\delta^*(q, wa) = \delta(\delta^*(q, w), a)$</p></li><li><p>The language accepted by a DFA $M = (Q, \Sigma, \delta, q_0, F)$ is the set of all strings on $\Sigma$ accepted by $M$, i.e. the set of all strings $w$ such that $\delta^*(q_0, w)$ results in a final state:<br>$$<br>L(M) = {w \in \Sigma^* : \delta^*(q_0, w) \in F}<br>$$</p></li><li><p><strong>Theorem 2.1</strong></p><ul><li>Let $M = (Q, \Sigma, \delta, q_0, F)$ be a deterministic finite accepter, and let $G_M$ be its associated transition graph. Then for every $q_i, q_j \in Q$, and $w \in \Sigma^+$, $\delta^*(q_i, w) = q_j$ if and only if there is in $G_M$ a walk with label $w$ from $q_i$ to $q_j$</li></ul></li><li><p>Notes</p><ul><li>Finite accepters are characterized by having no temporary storage</li><li>Since an input file cannot be rewritten, a finite automaton is severely limited in its capacity to “remember” things during the computation</li><li>A finite amount of information can be retained in the control unit by placing the unit into a specific state</li><li>But since the number of such states is finite, a finite automaton can only deal with situations in which the information to be stored at any time is strictly bounded</li></ul></li><li><p>Regular Languages</p><ul><li>A language $L$ is <strong>regular</strong> if and only if there is a DFA that accepts $L$</li></ul></li></ul><h5 id="2-2-Nondeterministic-Finite-Accepter-NFA"><a href="#2-2-Nondeterministic-Finite-Accepter-NFA" class="headerlink" title="2.2 Nondeterministic Finite Accepter (NFA)"></a>2.2 Nondeterministic Finite Accepter (NFA)</h5><ul><li><p>A <strong>nondeterministic finite accepter</strong> or <strong>nfa</strong> is defined by the quintuple<br>$$<br>M = (Q, \Sigma, \delta, q_0, F)<br>$$<br>where $Q, \Sigma, q_0, F$ are defined as for deterministic finite accepter, but<br>$$<br>\delta : Q \times (\Sigma \cup {\lambda}) \rightarrow 2^Q<br>$$<br>where $2^Q$ is the set of all subsets of $Q$ (i.e., the power set of $Q$)</p></li><li><p>The basic differences between deterministic and nondeterministic finite automata are:</p><ul><li>In an NFA, a (state, symbol) combination may lead to several states <u>simultaneously</u></li><li>An NFA may have <u>undefined transitions</u></li><li>If a transition is labeled with the empty string as its input symbol, the NFA may change states <u>without consuming input</u></li></ul></li><li><p>The language $L$ accepted by an nfa $M = (Q, \Sigma, \delta, q_0, F)$ is defined as the set of all strings accepted in the above sense. Formally,<br>$$<br>L(M) = {w \in \Sigma^* : \delta^*(q_0, w) \cap F \neq \emptyset}<br>$$</p></li></ul><h5 id="2-3-Equivalence-of-Deterministic-and-Nondeterministic-Finite-Accepters"><a href="#2-3-Equivalence-of-Deterministic-and-Nondeterministic-Finite-Accepters" class="headerlink" title="2.3 Equivalence of Deterministic and Nondeterministic Finite Accepters"></a>2.3 Equivalence of Deterministic and Nondeterministic Finite Accepters</h5><ul><li><strong>Theorem 2.2</strong><ul><li>Let $L$ be the language accepted by a nondeterministic finite accepter $M_N = (Q_N, \Sigma, \delta_N, q_0, F_N)$. Then there exists a deterministic finite accepter $M_D = (Q_D, \Sigma, \delta_N, {q_0}, F_D)$ such that $L = L(M_D)$</li><li>For <u>any</u> nondeterministic finite accepter, there is an equivalent deterministic finite accepter.</li><li>Therefore, <em>every language accepted by a nondeterministic finite accepter is also regular.</em></li></ul></li><li>Procedure: NFA-to-DFA Conversion<ol><li>Beginning with the start state, define input transitions for the DFA as follows:<ul><li>If the NFA input transition leads to a single state, replicate for the DFA.</li><li>If the NFA input transition leads to more than one state, create a new state in the DFA labeled ${q_i, \cdots, q_j}$, where $q_i, \cdots, q_j$ are all the states the NFA transition can lead to.</li><li>If the NFA input transition is not defined, the corresponding DFA transition should lead to a trap state.</li></ul></li><li>Repeat step 1 for all newly created DFA states, until no new states are created.</li><li>Any DFA state containing an NFA final state in its label should be labeled as final.</li><li>If the NFA accepts the empty string, label the start DFA state a final state.</li></ol></li></ul><h4 id="3-Regular-Languages-and-Regular-Grammars"><a href="#3-Regular-Languages-and-Regular-Grammars" class="headerlink" title="3. Regular Languages and Regular Grammars"></a>3. Regular Languages and Regular Grammars</h4><h5 id="3-1-Regular-Expressions"><a href="#3-1-Regular-Expressions" class="headerlink" title="3.1 Regular Expressions"></a>3.1 Regular Expressions</h5><ul><li><p>Regular Expressions are defined recursively. For any alphabet $\Sigma$:</p><ol><li>Primitive regular expressions:<ul><li>the empty set $\emptyset$</li><li>the empty string $\lambda$</li><li>any symbol of the alphabet $a \in \Sigma$</li></ul></li><li>If $r_1$ and $r_2$ are regular expressions, then so are:<ul><li>the union $r_1 + r_2$</li><li>concatenation $r_1 \cdot r_2$ or $r_1r_2$</li><li>star closure $r_1^*$</li></ul></li><li>Any string resulting from a <em>finite</em> number of these operations on primitive regular expressions is also a regular expression.</li></ol></li><li><p>Languages Associated with Regular Expressions</p><ul><li>A regular expressions $r$ denotes a language $L(r)$</li><li>Assuming that $r_1$ and $r_2$ are regular expressions:<ol><li>The regular expression $\emptyset$ denotes the empty set</li><li>The regular expression $\lambda$ denotes the set ${\lambda}$</li><li>For any $a$ in the alphabet $\Sigma$, the regular expression $a$ denotes the set ${a}$</li><li>The regular expression $r_1 + r_2$ denotes $L(r_1) \cup L(r_2)$</li><li>The regular expression $r_1 \cdot r_2$ denotes $L(r_1)L(r_2)$</li><li>The regular expression $(r_1)$ denotes $L(r_1)$</li><li>The regular expression $r_1^*$ denotes $(L(r_1))^*$</li></ol></li></ul></li></ul><h5 id="3-2-Regular-Expressions-and-Regular-Languages"><a href="#3-2-Regular-Expressions-and-Regular-Languages" class="headerlink" title="3.2 Regular Expressions and Regular Languages"></a>3.2 Regular Expressions and Regular Languages</h5><ul><li><p><strong>Theorem 3.1</strong></p><ul><li>For any regular expression $r$, there is a nondeterministic finite automaton that accepts the language denoted by $r$.</li></ul></li><li><p>Since nondeterministic and deterministic accepters are equivalent, for any regular expression $r$, the language $L(r)$ is also regular.</p></li><li><p><strong>Primitive regular expressions</strong></p><ul><li>We can construct simple automata that accept the languages associated with:<ul><li>the empty set</li><li>the empty string</li><li>any individual symbol $a \in \Sigma$</li></ul></li></ul><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-03-23 at 11.56.19 AM.png" style="zoom:50%;"><ul><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-03-23 at 12.03.27 PM.png" style="zoom:50%;"></li><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-03-23 at 12.04.22 PM.png" style="zoom:50%;"></li><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-03-23 at 12.04.52 PM.png" style="zoom:50%;"></li><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-03-23 at 12.05.25 PM.png" style="zoom:50%;"></li></ul></li><li><p>Regular Expressions for Regular Languages</p><ul><li><strong>Theorem 3.2</strong><ul><li>For every regular language $L$, it is possible to construct a corresponding regular expression $r$ satisfying $L = L(r)$</li></ul></li></ul></li></ul><h5 id="3-3-Regular-grammars"><a href="#3-3-Regular-grammars" class="headerlink" title="3.3 Regular grammars"></a>3.3 Regular grammars</h5><ul><li><p>A grammar $G = (V, T, S, P)$ is said to be <strong>right-linear</strong> if all productions are of the form<br>$$<br>A \rightarrow xB,\<br>A \rightarrow x,<br>$$<br>where $A, B \in V$, and $x \in T^*$. A grammar is said to be <strong>left-linear</strong> if all productions are of the form<br>$$<br>A \rightarrow Bx,\<br>A \rightarrow x.<br>$$</p></li><li><p>In a <em><u>right-linear grammar</u></em>, at most one variable symbol appears on the right side of any production. If it occurs, it is the <em><strong>rightmost</strong></em> symbol.</p></li><li><p>In a <em><u>left-linear grammar</u></em>, at most one variable symbol appears on the right side of any production. If it occurs, it is the <em><strong>leftmost</strong></em> symbol.</p></li><li><p>A <strong>regular grammar</strong> is one that is either right-linear or left-linear.</p></li><li><p>Right-Linear grammars Generate Regular Languages</p><ul><li><strong>Theorem 3.3</strong><ul><li>For any given right-linear grammar $G$, the language $L(G)$ is regular</li></ul></li><li>The algorithm for constructing an NFA to accept the language generated by a given right-linear grammar $G$:<ul><li>Label the NFA start state with $S$ and a final state $V_f$</li><li>For every variable symbol $V_i$ in $G$, create an NFA state and label it $V_i$</li><li>For each production of the form $A \rightarrow aB$, label a transition from state $A$ to $B$ with symbol $a$</li><li>For each production of the form $A \rightarrow a$, label a transition from state $A$ to $V_f$ with symbol a:<ul><li>You need to add intermediate states for productions with more than one terminal on the right hand side.</li></ul></li></ul></li></ul></li><li><p>Right-Linear grammars for Regular Languages</p><ul><li><strong>Theorem 3.4</strong><ul><li>If $L$ is a regular language on the alphabet $\Sigma$, then there exists a right-linear grammar $G = (V, \Sigma, S, P)$ such that $L = L(G)$</li></ul></li><li>There is an algorithm that, given any DFA $M$ accepting a regular language $L$, constructs a right-linear grammar $G$ which generates the same language:<ul><li>Each state in the DFA corresponds to a variable symbol in $G$</li><li>For each DFA transition from state $A$ to state $B$ labeled with symbol $a$, there is a production of the form $A \rightarrow aB$ in $G$</li><li>For each final state $F_i $ in the DFA, there is a corresponding production $F_i \rightarrow \lambda$ in $G$</li></ul></li></ul></li><li><p>Equivalence of Regular Languages and Regular grammars</p><ul><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-03-27 at 12.46.52 PM.png" style="zoom:50%;"></li></ul></li></ul><h4 id="4-Properties-of-Regular-Languages"><a href="#4-Properties-of-Regular-Languages" class="headerlink" title="4. Properties of Regular Languages"></a>4. Properties of Regular Languages</h4><ul><li><p>Closure Properties</p><ul><li><strong>Theorem 4.1</strong><ul><li>If $L_1$ and $L_2$ are regular languages, so are the languages that result from the following operations:<ul><li>$L_1 \cup L_2$</li><li>$L_1 \cap L_2$</li><li>$L_1L_2$</li><li>$\overline{L_1}$</li><li>$L_1^*$</li></ul></li><li>In other words, the family of regular language is <strong>closed</strong> under union, intersection, concatenation, complementation, and star-closure.</li></ul></li></ul></li><li><p>Closure under Reversal</p><ul><li><strong>Theorem 4.2</strong><ul><li>If $L$ is a regular language, so is $L^R$</li></ul></li></ul></li><li><p>A Membership Algorithm for Regular Languages</p><ul><li>We say that a regular language is given in a <strong>standard representation</strong> if and only if it is described by one of the following:<ul><li>a finite automaton,</li><li>a regular expression,</li><li>or a regular grammar</li></ul></li><li><strong>Theorem 4.5</strong> confirms the existence of a <strong>membership</strong> algorithm for reguklar languages</li><li>To determine if an arbitrary string $w$ is in a regular language $L$, we assume we are given a standard representation of $L$m which we then convert to a DFA that accepts $L$</li><li>Simulate the operation of the DFA while processing $w$ as the input string</li><li>If the machine halts in a final state after processing $w$, then $w\in L$, otherwise, $w\notin L$.</li></ul></li><li><p>Determining whether a regular language is empty, finite or infinite</p><ul><li><strong>Theorem 4.6</strong> confirms the existence of an <em><strong>algorithm</strong></em> to determine if a regular language is <em><strong>empty, finite, or infinite</strong></em></li><li>Given the <em><strong>transition graph</strong></em> of a DFA that accepts $L$,<ul><li>If there is a simple path from the start state to any final state, $L$ is not empty (since it contains, at least, the corresponding string)</li><li>If a path form the start state to a final state includes a vertex which is the base of some cycle, $L$ is infinite (otherwise, $L$ is finite)</li></ul></li></ul></li><li><p>Determining whether two regular languages are equal</p><ul><li><p><strong>Theorem 4.7</strong> confirms the existence of an algorithm to determine if two regular languages $L_1$ and $L_2$ are equal:</p><ul><li><p>Define the language $L = (L_1 \cap \overline{L_2}) \cup (\overline{L_1} \cap L_2)$</p></li><li><p>By closure properties, $L$ is regular</p></li><li><p>So we can construct a DFA $M$ to accept it, and by Theorem 4.6, we can determine whether $L$ is emoty or not.</p></li><li><p>$L_1$ and $L_2$ are equal if and only if $L$ is empty</p></li></ul></li></ul></li><li><p>Identifying Non-regular Languages</p><ul><li><p><strong>Pigeonhole Principle:</strong> If we put $n$ objects into $m$ boxes (pigeonholes), and if $n &gt; m$, then at least one box must have more than one item in it.</p></li><li><p>This simple principle is the basis of most of the methods for proving non-regularity of languages.</p></li><li><p><strong>Basic observation:</strong> Although regular languages can be <em>infinite</em>, their associated automata have <em>finite</em> memory.</p></li></ul></li><li><p>The Pumping Lemma</p><ul><li>Suppose that $M = (Q, \Sigma, \delta, q_0, F)$ is a DFA with $n$ states that accepts a language $L$:<ul><li>If it accepts a string $x$ such that $|x| \ge n$, then by the time $n$ symbols have been read, $M$ must have entered some state more than once;</li><li>In other words, there must be two different prefixes $u$ and $uv$ such that $\delta^*(q_0, u) = \delta^*(q_0, uv)$</li><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-05-03 at 4.16.49 PM.png" style="zoom:50%;"></li></ul></li><li>This implies that there are many more strings in $L$, because we can traverse the loop $v$ any number of times (including leaving it out altogether).</li><li>In other words, all of the string $uv^iw$ for $i \ge 0$ are in $L$</li><li>This fact is known as the <em><strong>Pumping Lemma for Regular Languages</strong></em>.</li><li><strong>Theorem (<em>pumping property</em>):</strong> Suppose that $L$ is a language over $\Sigma$. If $L$ is accepted by the DFA $M = (Q, \Sigma, \delta, q_0, F)$, then there is an integer $n$ so that for every $x$ in $L$ satisfying $|x| \ge n$, there are three strings $u, v$, and $w$ such that $x = uvw$ and:<ul><li>$|uv| \le n$;</li><li>$v &gt; 0 (i.e. v \ne \lambda) $</li><li>For every $i \ge 0$, the string $uv^iw$ belongs to $L$</li></ul></li><li>The way we found $n$ was to take the number of states in an FA accepting $L$.</li><li>In many applications we don’t need to know this, only that there is such an $n$.</li><li>The statement of the Pumping lemma is as follows:<ul><li>If $L$ is regular $\Rightarrow$ the pumping property holds</li></ul></li><li>In practice, we use the contrapositive:<ul><li>If the pumping property does not hold $\Rightarrow$ the language $L$ is not regular</li></ul></li><li>Thus, the most common application of the pumping lemma is to show that a language is not regular</li><li>The proof is by contradiction. We suppose that the language can be accepted by an FA, and we let $n$ be the integer in the pumping lemma</li><li>Then we choose a string $x$ with $|x| \ge n$ to which we can apply the lemma so as to get a contradiction.</li><li>Note that the Pumping lemma only provides a <em><strong>necessary</strong></em> condition, but not a sufficient one.<ul><li>If $L$ is regular $\Rightarrow$ the pumping property holds</li></ul></li><li>In other words, it does not say anything about the pumping property when L is not regular</li><li>In fact, <em><strong>there are languages that do satisfy the pumping lemma, but are not regular</strong></em>, e.g.:<ul><li>$L = {a^ib^jc^j\mid i\ge 1\ and\ j \ge 0}\cup{b^jc^k\mid j\ge 0\ and\ k\ge 0}$</li></ul></li></ul></li></ul><h4 id="5-Context-Free-Languages"><a href="#5-Context-Free-Languages" class="headerlink" title="5. Context Free Languages"></a>5. Context Free Languages</h4><h5 id="5-1-Context-Free-Grammars"><a href="#5-1-Context-Free-Grammars" class="headerlink" title="5.1 Context-Free Grammars"></a>5.1 Context-Free Grammars</h5><ul><li><p>Definition:</p><ul><li><p>We call $G = (V, \Sigma, S, P)$ a <strong><em>context-free grammar</em> (CFG)</strong> if all productions in $P$ have the form:</p><ul><li>$A\rightarrow x$</li></ul><p>in which $A\in V$, and $x\in (V\cup\Sigma)^*$.</p></li><li><p>We say that $L$ is a <strong><em>context-free language</em> (CFG)</strong> if and only if there is a context-free grammar $G$ such that $L = L(G)$</p><ul><li>In other words, a language is context-free if it is generated by a context-free grammar.</li></ul></li></ul></li><li><p>No Restriction on the Right Side</p><ul><li>A context-free grammar has <em>no restrictions</em> on the right side of its productions.</li><li>Therefore, the class of CFLs includes the class of regular languages as a proper subset.</li></ul></li><li><p>Restriction on the Left Side</p><ul><li>Note that, a context-free grammar does impose a restriction on the <em>left side</em> of productions:<ul><li><strong>the left side must be a single variable</strong></li></ul></li></ul></li></ul><h5 id="5-2-Leftmost-and-Rightmost-Derivations"><a href="#5-2-Leftmost-and-Rightmost-Derivations" class="headerlink" title="5.2 Leftmost and Rightmost Derivations"></a>5.2 Leftmost and Rightmost Derivations</h5><ul><li>In a <em>leftmost derivation</em> (LMD), at each step, the leftmost variable in a sentential form is replaced.</li><li>In a <em>rightmost derivation</em> (RMD), at each step, the rightmost variable in a sentential form is replaced.</li></ul><h5 id="5-3-Derivation-Trees"><a href="#5-3-Derivation-Trees" class="headerlink" title="5.3 Derivation Trees"></a>5.3 Derivation Trees</h5><ul><li>In a <em>derivation tree</em> or <em>parse tree</em>,<ul><li>the root is labeled $S$</li><li>internal nodes are labeled with a variable occurring on the left side of a production</li><li>the children of a node contain the symbols on the corresponding right side of a production</li></ul></li><li>In a full derivation tree, the root node represents the start variable $S$.</li><li>Any interior node and its children represent a production $A\rightarrow x$ used in the derivation;<ul><li>the node represents $A$, and the children, from left to right, represent the symbols in $x$.</li></ul></li><li>Each leaf node represents a symbol or $\lambda$.</li><li>The string derived is read off from left to right, ignoring $\lambda’$s.<ul><li>The <em><strong>yield</strong></em> of a derivation tree is the string of terminals produced by a leftmost depth-first traversal of the tree.</li></ul></li></ul><h5 id="5-4-Sentential-Forms-and-Derivation-Trees"><a href="#5-4-Sentential-Forms-and-Derivation-Trees" class="headerlink" title="5.4 Sentential Forms and Derivation Trees"></a>5.4 Sentential Forms and Derivation Trees</h5><ul><li><strong>Theorem 5.1</strong> states that:<ul><li>Given a context-free grammar $G$, for every string $w\in L(G)$, there exists a derivation tree whose yield is $w$.</li><li>The converse is also true: the yield of any derivation tree formed with productions from $G$ is in $L(G)$</li><li><strong>Note:</strong> Derivation trees show which productions are used in obtaining a sentence, but <em>do not give the order of their application</em>.</li></ul></li></ul><h5 id="5-5-Parsing-and-Membership"><a href="#5-5-Parsing-and-Membership" class="headerlink" title="5.5 Parsing and Membership"></a>5.5 Parsing and Membership</h5><ul><li><strong>The <em>parsing</em> problem</strong>: given a grammar $G$ and a string $w$, find a sequence of derivations using the productions in $G$ to produce $w$</li><li>To ensure that the problem may ne solved (regardless of efficiency) we need to require grammars to be given in a suitable shape</li><li>Exhausive parsing is guaranteed to yield all strings eventually, but many fail to stop for strings not in $L(G)$, unless we restrict the productions in the grammar.</li><li>Questions about the strings generated by a CFg are sometimes easier to answer if the productions have a retricted form:<ul><li>For example, if we know that a grammar has:<ul><li>no $\lambda$-productions ($A\rightarrow \lambda$),</li><li>and no unit productions ($A\rightarrow B$),</li></ul></li><li>we can deduce that no derivation of a string $x$ can take more than $2\vert x\vert - 1$.</li><li>We could then, in principle, determine whether $x$ can be derived by considering derivations no longer than $2\vert x\vert - 1$.</li></ul></li><li>There exists an algorithm which, for every CFG $G = (V, \Sigma, S, P)$, produces a CFG $G_1 = (V, \Sigma, S, P_1)$ which has no $\lambda$-productions, no unit productions of the form $A\rightarrow B$, and for which:<ul><li>$L(G_1) = L(G)-{\lambda}$</li></ul></li><li>With this conversion, the parsing problem may be solved in an exhaustive, top-down, but not very efficient fashion:</li><li><strong><u>Theorem 5.2</u></strong>: Exhaustive parsing is guaranteed to yield all strings eventually, but may fail to stop for strings not in $L(G)$, <em>unless if the grammar has no $\lambda$-productions or unit productions</em>.</li></ul><h5 id="5-6-Parsing-and-Ambiguity"><a href="#5-6-Parsing-and-Ambiguity" class="headerlink" title="5.6 Parsing and Ambiguity"></a>5.6 Parsing and Ambiguity</h5><ul><li>If $G$ is a CFG, then for any $x\in L(G)$ these three statements are equivalent:<ul><li>$x$ has more than one derivation tree.</li><li>$x$ has more than one LMD.</li><li>$x$ has more than one RMD.</li></ul></li><li>Thus, a CFG $G$ is <em><strong>ambiguous</strong></em> if and only if, for at least one $x\in L(G)$, $x$ has more than one LMD (or RMD).</li></ul><h5 id="5-7-Derivation-Trees-and-Ambiguity"><a href="#5-7-Derivation-Trees-and-Ambiguity" class="headerlink" title="5.7 Derivation Trees and Ambiguity"></a>5.7 Derivation Trees and Ambiguity</h5><ul><li><p>A classic example of ambiguity is <strong>the dangling <em>else</em></strong>.</p></li><li><p>In C, an if-statement can be defined by:</p><ul><li>$S\rightarrow if\ (E)\ S\ \mid\ if\ (E)\ S\ else\ S\ \mid\ OS$  (Where $OS$ stands for “other statement”).</li></ul></li><li><p>Consider the statement</p><p>$if\ (e1)\ if\ (e2)\ f();\ else\ g();$</p><ul><li>In C, the <em>else</em> should belong to the second $if$, but this grammar does not rule out interpreting the statement with $else$ belonging to the first $if$.</li></ul></li><li><p>Clearly the grammar given is ambiguous, <em>but there are equivalent grammars that allow only the correct interpretation</em>. For example:</p><p>$S\rightarrow S_1\ \mid\ S_2$</p><p>$S_1\rightarrow \ if\ (E)\ S_1\ else\ S_1\ \mid\ OS$</p><p>$S_2\rightarrow\ if\ (E)\ S\ \mid\ if\ (E)\ S_1\ else\ S_2$</p></li><li><p>These rules generate the same strings as the original ones and are unambiguous.</p></li></ul><h5 id="5-8-Ambiguous-Languages"><a href="#5-8-Ambiguous-Languages" class="headerlink" title="5.8 Ambiguous Languages"></a>5.8 Ambiguous Languages</h5><ul><li>For some languages, it is always possible to find an unambiguous grammar.</li><li>However, there are <em><strong>inherently ambiguous</strong></em> languages, for which every possible grammar is ambiguous.</li></ul><h5 id="5-9-Pumping-lemma-for-CFGs"><a href="#5-9-Pumping-lemma-for-CFGs" class="headerlink" title="5.9 Pumping lemma for CFGs"></a>5.9 Pumping lemma for CFGs</h5><ul><li>For every CFL $L$ there exists a natural number $n$ such that for any string $u\in L$ with $|u|\ge n$, there are $v, w, x, y, z$ such that $u = vwxyz$ and:<ol><li>$|wxy|\le n$</li><li>$|wy|\gt 0$</li><li>$\forall i\in N : vw^ixy^iz\in L$</li></ol></li></ul><h4 id="6-Pushdown-Automata"><a href="#6-Pushdown-Automata" class="headerlink" title="6. Pushdown Automata"></a>6. Pushdown Automata</h4><h5 id="6-1-Introduction"><a href="#6-1-Introduction" class="headerlink" title="6.1 Introduction"></a>6.1 Introduction</h5><ul><li><p>CFLs do not have anything similar to regular expressions.</p></li><li><p>But they do have a machine model, i.e., pushdown automata.</p></li><li><p>A pushdown automaton is essentially a finite automaton with a <em><strong>stack</strong></em> added as storage.</p></li><li><p>As there is no limit on the size of the stack, pushdown automata do not have bounded memory limitation of finite automata.</p></li><li><p>Pushdown automata are equivalent to CFGs, as long as we allow them to be nondeterministic.</p></li><li><p>The language family associated with deterministic pushdown automata is a proper subset of the context-free languages.</p></li><li><p>There are CFLs (such as $A^nB^n$) that cannot be recognized by finite automata</p></li><li><p>This is because the memory of a finite automata is restricted to a finite set of states, whereas the recognition of a CFL may require storing an unbounded amount of information.</p></li><li><p>Consider the following languages:</p><ul><li>$A^nB^m = {a^nb^m\mid n, m \ge 0}$, which is regular;</li><li>$A^nB^n = {a^nb^n\mid n\ge 0}$, which is not regular.</li></ul></li><li><p>To recognize strings in $A^nB^m$, all we need to do is to check that $a$’s appear before $b$’s, a task which can be done by a DFA.</p></li><li><p>In constrast, for $A^nB^n$, we must not only check that all $a$’s precede the first $b$, we must also <em><strong>count</strong></em> the number of $a$’s.</p></li><li><p>Since $n$ is unbounded, this counting cannot be done with a finite memory.</p></li><li><p>As the example of $A^nB^n$ shows, for the machine model of CFLs, we need a machine that can <em><strong>count without limit</strong></em>.</p></li><li><p>But, that is not enough. For instance, consider $WW^R = {ww^R\mid w\in {a, b}^*$, which is a CFL.</p></li><li><p>$WW^R$ shows that we need more than unlimited counting ability:</p><ul><li>We need the ability to store and match a sequence of symbols in <em><strong>reverse</strong></em> order.</li></ul></li><li><p>This suggests that we might try a <em><strong>stack</strong></em> as a storage mechanism, allowing unbounded storage that is restricted to operating like a stack.</p></li><li><p>This gives us a class of machines called pushdown automata (PDA), which we use as a model of computation to process context-free languages.</p></li></ul><h5 id="6-2-Nondeterministic-Pushdown-Automata"><a href="#6-2-Nondeterministic-Pushdown-Automata" class="headerlink" title="6.2 Nondeterministic Pushdown Automata"></a>6.2 Nondeterministic Pushdown Automata</h5><ul><li><p>Each move of the control unit:</p><ul><li>reads a symbol from the input file;</li><li>changes the contents of the stack (<em>through the usual stack operations</em>).</li></ul></li><li><p>Each move of the control unit is determined by:</p><ul><li>the current input symbol</li><li>and the symbol currently on top of the stack.</li></ul></li><li><p>The result of the move is a new state of the control unit and a change in the top of the stack.</p></li><li><p>A <em>nondeterministic pushdown accepter</em> (NPDA) $M = (Q, \Sigma, \Gamma, \delta, q_0, z, F)$ is defined by:</p><ul><li><p>$Q$: the finite set of internal states of the control unit</p></li><li><p>$\Sigma$: the finite set of input alphabet</p></li><li><p>$\Gamma$: the finite set of stack alphabet</p></li><li><p>$\delta$: the transition function with the type signature:<br>$$<br>\delta: Q\times (\Sigma \cup {\lambda})\times \Gamma\rightarrow P_f(Q\times \Gamma^*)<br>$$<br>$P_f(Q\times \Gamma^*)$ is the set of <em>finite</em> subsets of $(Q\times \Gamma^*)$</p></li><li><p>$q_0\in Q$: the initial state of the control unit</p></li><li><p>$z\in \Gamma$: the stack start symbol</p></li><li><p>$F\subseteq Q$: the set of final states</p></li><li><p>The arguments of $\delta$ are:</p><ul><li>the current state of the control unit</li><li>the current input symbol</li><li>and the current symbol on <em>top</em> of the stack</li></ul></li><li><p>The result is a finite set of pairs $(q, x)$, where:</p><ul><li>$q$ is the next state of the control unit</li><li>and $x$ is a <em><strong>string</strong></em> that is put on top of the stack <em><strong>in place of the single symbol</strong></em> there before.</li></ul></li><li><p>$\lambda$-transition: When the second argument of $\delta$ is $\lambda$, i.e., a move that does not consume an input symbol.</p></li><li><p>$\delta$ Always needs a stack symbol; i.e., no move is possible if the stack is empty.</p></li><li><p>Finally, the requirement that the elements of the range of $\delta$ be a <em><strong>finite</strong></em> subset is necessary because:</p><ul><li>$Q\times \Gamma^*$ is an infinite set and therefore has infinite subsets.</li><li>While an NPDA may have several choices for its moves, <em>this choice must be restricted to a finite set of possibilities</em>.</li></ul></li><li><p><strong>Note</strong>: If a particular transition is not defined, the corresponding (state, symbol, stack top) configuration represents a <em>dead</em> state.</p></li></ul></li><li><p>Transition Graphs</p><ul><li>Label the edges of the graph with three things:<ul><li>the current input symbol</li><li>the symbol at the top of the stack</li><li>and the string that replaces the top of the stack</li></ul></li></ul></li><li><p>Instantaneous Descriptions</p><ul><li><p>While transition graphs are convenient for <em>describing</em> NPDAs, they are not so suitable for formal reasoning.</p></li><li><p>To trace the operation of an NPDA, we must keep track of:</p><ul><li>the current state of the control unit,</li><li>the unread part of the input string,</li><li>and the stack contents.</li></ul></li><li><p><strong><u>Instantaneous Description</u></strong>: The triplet $(q, w, u)$ in which:</p><ul><li>$q$ is the state of the control unit,</li><li>$w$ is the unread part of the input string,</li><li>and $u$ is the stack contents:<ul><li>with the leftmost symbol indicating the top of the stack</li></ul></li></ul><p>is called an instantaneous description of a pushdown automaton.</p></li><li><p>A move from one instantaneous description to another will be denoted by the symbol $\vdash$</p></li><li><p>Thus $(q_1, aw, bx)\vdash (q_2, w, yx)$ is possible if and only if:<br>$$<br>(q_2, y)\in \delta (q_1, a, b)<br>$$</p></li><li><p>Moves involving an arbitrary number of steps will be denoted by $\vdash^*$.</p></li></ul></li></ul><h5 id="6-3-The-Language-Accepted-by-an-NPDA"><a href="#6-3-The-Language-Accepted-by-an-NPDA" class="headerlink" title="6.3 The Language Accepted by an NPDA"></a>6.3 The Language Accepted by an NPDA</h5><ul><li><p>The language accepted by an NPDA is the set of all strings that cause the NPDA to halt in a final state, after starting in $q_0$ with only the stack symbol $z$ on the stack.</p></li><li><p><em>The final contents of the stack are irrelevant</em>.</p></li><li><p>As was the case with nondeterministic finite automata, the string is accepted if <em>at least one</em> of the computations cause the NPDA to halt in a final state.</p></li><li><p>In other words, it does not require all the possible traces to end up in a final state.</p></li><li><p><strong><u>Definition 7.2</u></strong>: Let $M = (Q, \Sigma, \Gamma, \delta, q_0, z, F)$ be an NPDA. The language accepted by $M$ is the set:<br>$$<br>L(M) = {w\in \Sigma^<em>\mid (q_0, w, z) \vdash^</em> (p, \lambda, u), p\in F, u \in \Gamma^*}<br>$$</p></li></ul><h5 id="6-4-NPDAs-and-CFLs"><a href="#6-4-NPDAs-and-CFLs" class="headerlink" title="6.4 NPDAs and CFLs"></a>6.4 NPDAs and CFLs</h5><ul><li><p>A context-free grammar is in <em>Greibach Normal Form</em> if, in all of its productions, the right side consists of a single terminal follows by any number of variables</p></li><li><p><strong><u>Definition 6.5</u></strong>: A context-free grammar $G = (V, T, S, P)$ is said to be in <em><strong>Greibach normal form</strong></em> if all productions have the form<br>$$<br>A\rightarrow ax<br>$$<br>where $a\in T$ and $x\in V^*$.</p></li><li><p><strong><u>Theorem 6.7</u></strong>: For every context-free grammar $G$ there exists a grammar $\hat{G}$ in Greibach normal form satisfying:<br>$$<br>L(\hat{G}) = L(G) - {\lambda}<br>$$</p></li></ul><h5 id="6-5-NPDAs-for-CFLs"><a href="#6-5-NPDAs-for-CFLs" class="headerlink" title="6.5 NPDAs for CFLs"></a>6.5 NPDAs for CFLs</h5><ul><li><p><strong><u>Theorem 7.1</u></strong> For any given $\lambda$-free context-free language $L$, there exists an NPDA $M$ such that<br>$$<br>L(M) = L<br>$$</p></li><li><p>The constructive proof of Theorem 7.1 provides an algorithm that can be used to build the corresponding NPDA, for any language specified by a grammar $G$ in Greibach normal form.</p></li><li><p>The resulting NPDA simulates grammar derivations by:</p><ul><li>keeping variables on the stack</li><li>while making sure that the input symbol matches the terminal on the right side of the production.</li></ul></li></ul><h5 id="6-6-Construction-of-an-NPDA-from-a-Grammar-in-Greibach-Normal-Form"><a href="#6-6-Construction-of-an-NPDA-from-a-Grammar-in-Greibach-Normal-Form" class="headerlink" title="6.6 Construction of an NPDA from a Grammar in Greibach Normal Form"></a>6.6 Construction of an NPDA from a Grammar in Greibach Normal Form</h5><ul><li>The NPDA $M = ({q_0, q_1, q_f}, T, V\cup{z}, \delta, q_0,z {q_f})$ has:<ul><li>three states $Q = {q_0, q_1, q_f}$, with $q_0$ as the initial state, and $q_f$ as the only final state</li><li>input alphabet equal to the grammar terminal symbols $T$,</li><li>and stack alphabet equal to the grammar variable $V$, plus $z$, with the assumption that $z\notin V$</li></ul></li><li>The transition function contains the following:<ul><li>A rule that pushes $S$ on the stack and switches control to $q_1$ without consuming input:<ul><li>$\delta (q_0, \lambda, z) = {(q_1, Sz)}$</li></ul></li><li>For every production of the form $A\rightarrow au$, we have<ul><li>$(q_1, u)\in \delta (q_1, a, A)$</li></ul></li><li>A rule that switches the control unit to the final state $q_f$ when there is no more input, and the stack is empty:<ul><li>$\delta(q_1, \lambda, z) = {(q_f, z)}$</li></ul></li><li>If $\lambda \in L$, we add the transition $\delta(q_0, \lambda, z) = {(q_f, z)}$</li></ul></li></ul><h5 id="6-7-CFGs-for-PDAs"><a href="#6-7-CFGs-for-PDAs" class="headerlink" title="6.7 CFGs for PDAs"></a>6.7 CFGs for PDAs</h5><ul><li><strong><u>Theorem 7.2</u></strong>: If $L = L(M)$ for some NPDA $M$, then there exists a context-free grammar $G$ such that $L(G) = L(M)$. In other words, $L$ is a context-free language.</li><li>Proof: The basic idea behind the proof is to reverse the process in the proof of Theorem 7.1</li><li>The aim is to construct a grammar that simulates the moves of the NPDA $M$.</li><li>In particular:<ul><li>the content of the stack should be reflected in the variable part of sentential forms in derivations,</li><li>while the processed input is the terminal prefix of the sentential forms.</li></ul></li><li>Theorem 7.1 and 7.2 show that the following are equivalent, in the sense that they generate the same class of languages, i.e., context-free languages:<ul><li>Context-free grammars</li><li>NPDAs</li></ul></li><li>Which specification one chooses depends on the purpose:<ul><li>For specifying programming language constructs, grammars are more appropriate, as they are easier to understand by human beings.</li><li>For conputational purposes (e.g., compilation of a program) the machine model, i.e., NPDA, is more appropriate.</li></ul></li></ul><h5 id="6-8-Deterministic-Pushdown-Automata-DPDA"><a href="#6-8-Deterministic-Pushdown-Automata-DPDA" class="headerlink" title="6.8 Deterministic Pushdown Automata (DPDA)"></a>6.8 Deterministic Pushdown Automata (DPDA)</h5><p>A <em>deterministic pushdown accepter</em> (DPDA) never has more than one choice in its moves.</p><ul><li>For every $q\in Q, a\in \Sigma \cup {\lambda}$ and $b\in \Gamma$:<ul><li>$\delta (q, a, b)$ contains at most one element;</li><li>If $\delta(q, \lambda, b)$ is not empty, then $\delta(q, c, b)$ must be empty for every input symbol $c\in \Sigma$.<ul><li>when a $\lambda$-move is possible for some configuration, no input-consuming alternative is available.</li></ul></li></ul></li><li>Unlike the case for finite automata, a $\lambda$-transition does not necessarily mean the automaton is nondeterministic:<ul><li>Since the top of the stack plays a role in determining the next move, the presence of $\lambda$-transitions does not automatically imply nondeterminism.</li></ul></li><li>Also, some transitions of a DPDA may be to the empty set, that is, undefined, so there may be dead configurations.</li><li>This does not affect the definition either:<ul><li>The only criterion for determinism is that, at all times, <em>at most</em> one possible move exists.</li></ul></li></ul><h5 id="6-9-Deterministic-Context-Free-Languages-DCFLs"><a href="#6-9-Deterministic-Context-Free-Languages-DCFLs" class="headerlink" title="6.9 Deterministic Context-Free Languages (DCFLs)"></a>6.9 Deterministic Context-Free Languages (DCFLs)</h5><ul><li><p>A context-free language $L$ is <em>deterministic</em> if there is a DPDA $M$ such that:<br>$$<br>L = L(M)<br>$$</p></li><li><p>There are, however, languages which are context-free, but not DCFL.</p><ul><li>This shows that, <em><strong>deterministic and nondeterministic pushdown automata are not equivalent.</strong></em></li></ul></li></ul><h5 id="6-10-Importance-of-DCFLs"><a href="#6-10-Importance-of-DCFLs" class="headerlink" title="6.10 Importance of DCFLs"></a>6.10 Importance of DCFLs</h5><ul><li>The importance of deterministic context-free languages lies in the fact that they can be <em><strong>parsed efficiently</strong></em>.</li><li>Think of the pushdown automaton as a parsing device:<ul><li>Since there is no backtracking involved, we can easily write a computer program for it, and we may expect that it will work efficiently.</li><li>Since there may be $\lambda$-transitions involved, we cannot immediately claim that this will yield a linear-time parser, but it puts us on the right track, nevertheless.</li></ul></li></ul><h4 id="7-Turing-Machines"><a href="#7-Turing-Machines" class="headerlink" title="7. Turing Machines"></a>7. Turing Machines</h4><h5 id="7-1-The-Standard-Turing-Machine"><a href="#7-1-The-Standard-Turing-Machine" class="headerlink" title="7.1 The Standard Turing Machine"></a>7.1 The Standard Turing Machine</h5><ul><li>A standard Turing machine has unlimited storage in the form of a tape:<ul><li>The tape consists of an infinite number of cells,</li><li>with each cell capable of storing one symbol.</li></ul></li><li>The read-write head can travel in <em><strong>both</strong></em> directions on the tape, processing one symbol per move.</li><li>In a standard Turing machine, the tape acts as the:<ul><li>input,</li><li>output,</li><li>and storage medium.</li></ul></li><li>A control function causes the machine to change states and possibly overwrite the tape contents:<ul><li>Deterministic and non-deterministic Turing machines turn out to be as powerful as each other.</li></ul></li><li>The input string is surrounded by blanks, so the input alphabet is considered a proper subset of the tape alphabet.</li></ul><h5 id="7-2-Definition-of-a-Turing-Machine"><a href="#7-2-Definition-of-a-Turing-Machine" class="headerlink" title="7.2 Definition of a Turing Machine"></a>7.2 Definition of a Turing Machine</h5><ul><li><strong><u>Definition 9.1</u></strong>: A <em>Turing Machine</em> $M = (Q, \Sigma, \Gamma, \delta, q_0, \Box, F)$ is defined by:<ul><li>$Q$: a finite set of internal states</li><li>$\Sigma$: the input alphabet</li><li>$\Gamma$: the tape alphabet</li><li>$\delta$: $Q\times \Gamma\rightarrow Q\times \Gamma \times {L, R}$: the transition function</li><li>$\Box \in \Gamma$: a special symbol called the blank</li><li>$q_0 \in Q$: the initial state</li><li>$F \subseteq Q$: the set of final states</li></ul></li><li>In the definition of a Turing machine, we assume that: $\Sigma \subseteq \Gamma - {\Box}$.</li><li>In other words, the input alphabet is a subset of the tape alphabet, and the blank symbol is not in the input alphabet.</li><li>Transition Function $\delta$<ul><li>$\delta: Q\times \Gamma \rightarrow Q \times \Gamma \times {L, R}$</li><li>Input to the transition function $\delta$ consists of:<ul><li>the current state of the control unit</li><li>and the current tape symbol</li></ul></li><li>Output of $\delta$ consists of:<ul><li>a new state,</li><li>a new tape symbol,</li><li>and location of the next symbol to be read (left or right)</li></ul></li><li>In general, $\delta$ is a partial function, so that some (state, symbol) input combinations may be undefined.</li></ul></li></ul><h5 id="7-3-Transition-Graphs-for-Turing-Machines"><a href="#7-3-Transition-Graphs-for-Turing-Machines" class="headerlink" title="7.3 Transition Graphs for Turing Machines"></a>7.3 Transition Graphs for Turing Machines</h5><ul><li>Label the edges of the graph with three items:<ul><li>the current tape symbol,</li><li>the symbol that replaces it,</li><li>and the direction in which the read-write head is to move.</li></ul></li></ul><h5 id="7-4-A-Turing-machine-that-Never-Halts"><a href="#7-4-A-Turing-machine-that-Never-Halts" class="headerlink" title="7.4 A Turing machine that Never Halts"></a>7.4 A Turing machine that Never Halts</h5><ul><li>It is possible for a Turing machine to never halt on certain inputs.</li><li>The machine runs forever with the read-write head moving alternately right and left but making no modifications to the tape.</li><li>In fact, it should be clear that this particular machine will run forever, <em>regardless of the initial information on its tape</em>, with the read-write head moving alternately right then left but making no modifications to the tape.</li><li>In the analogy with programming terminology, we say that the Turing machine is in an <em><strong>infinite loop</strong></em>.</li></ul><h5 id="7-5-Instantaneous-Description"><a href="#7-5-Instantaneous-Description" class="headerlink" title="7.5 Instantaneous Description"></a>7.5 Instantaneous Description</h5><ul><li><p>The most convenient way to exhibit a sequence of configurations of a Turing machine uses the idea of an <em>instantaneous description</em>.</p></li><li><p>Any configuration is completely determined by:</p><ul><li>the current state of the control unit,</li><li>the contents of the tape,</li><li>and the position of the read-write head.</li></ul></li><li><p>Use the notation in which $x_1qx_2$ or $a_1a_2\cdots a_{k-1}qa_ka_{k+1}\cdots a_n$ is the instantaneous description of a machine in state $q$ with the tape depicted in the figure below.</p><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-05-06 at 5.25.49 PM.png" style="zoom:50%;"></li><li><p>This convention is chosen so that the position of the read-write head is over the cell containing the symbol immediately following $q$.</p></li><li><p>The instantaneous description gives only a finite amount of information to the right and left of the read-write head.</p></li><li><p>The unspecified part of the tape is assumed to contain all blanks.</p></li><li><p>Normally, such blanks are irrelevant and are not shown explicitly in the instantaneous description.</p></li><li><p>If the position of blanks is relevant to the discussion, however, the blank symbol may appear in the instantaneous description.</p></li></ul><h5 id="7-6-The-Language-Accepted-by-a-Turing-Machine"><a href="#7-6-The-Language-Accepted-by-a-Turing-Machine" class="headerlink" title="7.6 The Language Accepted by a Turing Machine"></a>7.6 The Language Accepted by a Turing Machine</h5><ul><li><p><strong><u>Definition 9.3</u></strong>: Let $M = (Q, \Sigma, \Gamma, \delta, q_0, \Box, F)$ be a Turing machine. Then the language accepted by $M$ is:<br>$$<br>L(M) = {w\in \Sigma^+\mid q_0w\vdash^* x_1q_fx_2,\ for\ some\ q_f\in F, x_1, x_2\in \Gamma^*}<br>$$</p></li><li><p>The language accepted by a Turing machine is the set of all strings which cause the machine to halt in a final state, when started in its standard initial configuration.</p></li><li><p>A string is rejected if</p><ul><li>The machine halts in a nonfinal state, or</li><li><em><strong>The machine never halts</strong></em></li></ul></li><li><p>$M$ is said to halt starting from some initial configuration $x_1q_ix_2$ if<br>$$<br>x_1q_ix_2\vdash^* y_1q_jay_2<br>$$<br>for any $q_j$ and $a$, for which $\delta(q_j, a)$ is undefined.</p></li><li><p>Turing machines are more powerful than the previous classes of automata that we have studied (i.e., FAs, and PDAs).</p></li></ul><h5 id="7-7-Turing-Machines-as-Transducers"><a href="#7-7-Turing-Machines-as-Transducers" class="headerlink" title="7.7 Turing Machines as Transducers"></a>7.7 Turing Machines as Transducers</h5><ul><li><p>Turing machines provide an abstract model for digital computers, acting as a transducer that transforms input into output.</p></li><li><p>A <em>Turing machine transducer</em> implements a function that treats the original contents of the tape as its input and the final contents of the tape as its output.</p></li><li><p>A function is <em>Turing-computable</em> if it can be carried out by a Turing machine capable of processing all values in the function domain.</p></li><li><p><strong><u>Definition 9.4</u></strong>: A function $f$ with domain $D$ is said to be Turing-computable if there exists some Turing machine $M = (Q, \Sigma, \Gamma, \delta, q_0, \Box, F)$ such that, for all $w \in D$:<br>$$<br>q_0w\vdash^*q_ff(w)<br>$$<br>for some final state $q_f\in F$, <em><strong>and for any input $w\notin D$, the machine $M$ does not halt in a final state.</strong></em></p></li><li><p>Turing machines are the most powerful model of computation as transducers as well.</p></li><li><p>In fact, all the common mathematical functions are Turing-computable, e.g.:</p><ul><li>Arithmetic operators, Exponentiation, Integer logarithm;</li><li>Comparison;</li><li>String manipulation;</li><li>Etc.</li></ul></li></ul><h5 id="7-8-Combining-Turing-Machines"><a href="#7-8-Combining-Turing-Machines" class="headerlink" title="7.8 Combining Turing Machines"></a>7.8 Combining Turing Machines</h5><ul><li>By combining Turing Machines that perform simple tasks, complex algorithms can be implemented.</li></ul><h5 id="7-9-Universal-Turing-Machines"><a href="#7-9-Universal-Turing-Machines" class="headerlink" title="7.9 Universal Turing Machines"></a>7.9 Universal Turing Machines</h5><ul><li>The TMs we have studied so far have been special-purpose computers capable of executing a single algorithm.</li><li>We can consider a “universal” Turing machine, which can execute a program stored in its memory:<ul><li>It receives an input string that specifies both:<ul><li>the algorithm it is to execute</li><li>and the input that is to be provided to the algorithm</li></ul></li></ul></li></ul><h5 id="7-10-The-Church-Turing-Thesis"><a href="#7-10-The-Church-Turing-Thesis" class="headerlink" title="7.10 The Church-Turing Thesis"></a>7.10 The Church-Turing Thesis</h5><ul><li>To say that the TM is a general model of computation implies that:<ul><li><em>“any algorithmic procedure that can be carried out at all, by a human computer or a team of humans or an electronic computer, can be carried out by a TM”</em>.</li></ul></li><li>An acceptance of the Church-Turing Thesis leads to a <em><strong>definition of an algorithm</strong></em>:<ul><li>An <em>algorithm</em> for a function $f: D\rightarrow R$ is a Turing machine $M$, which given any $d\in D$ on its tape, eventually halts with the correct answer $f(d)\in R$ on its tape.</li></ul></li><li>The nature of the model makes it seems that a TM can execute any algorithm a human can.</li><li>Apparent enhancements to the TM have been shown not to increase its power.</li><li>Other models of computation proposed have either been less powerful or equivalent to Turing machines.</li><li>No one has ever suggested any kind of <em><strong>effective</strong></em> computation that cannot be implemented on a TM</li><li>From now on, we will consider that, by definition, <em><strong>an “algorithmic procedure” is what a Turing machine can do</strong></em>.</li></ul><h5 id="7-11-The-Chomsky-Hierarchy"><a href="#7-11-The-Chomsky-Hierarchy" class="headerlink" title="7.11 The Chomsky Hierarchy"></a>7.11 The Chomsky Hierarchy</h5><ul><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-05-14 at 12.41.00 PM.png" style="zoom:50%;"></li><li><img src="/2021/05/15/Formal-Languages/Screen Shot 2021-05-14 at 12.41.10 PM.png" style="zoom:50%;"></li></ul><h4 id="8-Easy-wrong-topic"><a href="#8-Easy-wrong-topic" class="headerlink" title="8. Easy wrong topic"></a>8. Easy wrong topic</h4><ul><li><p>Is every formal language regular? No. $L = {a^nb^n : n \ge 0}$ is not regular.</p></li><li><p>$L = \emptyset$ is regular language.</p></li><li><p>$G = (V, T, S, P)$, $S \rightarrow aSb \mid S$, $L(G)$ is regular, because the production cannot halt, the language is empty.</p></li><li><p>Every finite language is regular. Therefore, every non-regular language is infinite.</p></li><li><p>Context Free contains regular languages, regular languages contain finite language</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language and Computation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pattern Matching</title>
      <link href="2021/04/15/Pattern-Matching/"/>
      <url>2021/04/15/Pattern-Matching/</url>
      
        <content type="html"><![CDATA[<h4 id="1-What-is-pattern-matching"><a href="#1-What-is-pattern-matching" class="headerlink" title="1. What is pattern matching?"></a>1. What is pattern matching?</h4><ul><li>Given:<ul><li>A text string of length $n$</li><li>A pattern string of length $m\lt n$</li></ul></li><li>Determine:<ul><li>Whether the pattern is a <em><strong>substring</strong></em> of the text;</li><li>Find all indices at which the pattern begins.</li></ul></li></ul><h4 id="2-Brute-force-algorithm"><a href="#2-Brute-force-algorithm" class="headerlink" title="2. Brute-force algorithm"></a>2. Brute-force algorithm</h4><ul><li>Compare the pattern $P$ with the text $T$ for each possible shift of $P$ relative to $T$, until either<ul><li>A match is found, or</li><li>All placements of the pattern have been tried.</li></ul></li><li>Brute-force pattern matching runs in time $O(nm)$ in worst case.</li></ul><h4 id="3-Boyer-Moore-algorithm"><a href="#3-Boyer-Moore-algorithm" class="headerlink" title="3. Boyer-Moore algorithm"></a>3. Boyer-Moore algorithm</h4><ul><li><p>Boyer-Moore heuristics</p><ul><li><em><strong>Looking-glass heuristic</strong></em>: Compare $P$ with a subsequence of $T$ from  <em><strong>right to left</strong></em>.</li><li><em><strong>Character-jump heuristic</strong></em>: When a mismatch occurs at $T[i] = c$<ul><li>If $P$ contains $c$, shift $P$ to align the <em><strong>last (right-most)</strong></em> occurrence of $c$ in $P$ with $T[i]$</li><li>Else, shift $P$ to align $P[0]$ with $T[i+1]$</li></ul></li></ul></li><li><p>Last-occurrence function</p><ul><li>Objective: speed up the search by <em><strong>look-up table</strong></em>.</li><li>BM algorithm <em><strong>preprocesses the pattern $P$</strong></em> and the alphabet $S$ to build the last-occurrence function $L$ mapping $S$ to integers, where $L(c)$ is defined as<ul><li>The largest index $i$ such that $P[i] = c$ or</li><li>$-1$ if no such index exists.</li></ul></li></ul></li><li><p>The Boyer-Moore algorithm</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Algorithm BoyerMooreMatch(T, P, Alphabet)    L = lastOccurenceFunction(P, Alphabet)    i = m - 1    j = m - 1    repeat        if T[i] == P[j] // matching            if j == 0                return i // match at i            else                i = i - 1                j = j - 1        else // character-jump            l = L[T[i]]            i = i + m - min(j, 1 + l)            j = m - 1 //rightmost    until i > n - 1    return -1 // no match<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>Analysis</p><ul><li>Boyer-Moore’s algorithm runs in time $O(nm)$ in worst case</li><li>The worst case may occur in images and DNA sequences but unlikely in English text.</li><li>Boyer-Moore’s algorithm is <em><strong>significantly faster</strong></em> than the brute-force algorithm on English text.</li></ul></li><li><p>Key innovations of BM algorithm</p><ul><li><em><strong>Pre-indexing mismatched Text characters in Pattern.</strong></em></li></ul></li></ul><h4 id="4-KMP-algorithm"><a href="#4-KMP-algorithm" class="headerlink" title="4. KMP algorithm"></a>4. KMP algorithm</h4><ul><li><p>Key idea of KMP algorithm: precompute <em><strong>self-overlaps</strong></em> between portion of the pattern, so when a mismatch occurs, we know the maximum amount to shift the pattern.</p></li><li><p>Failure function</p><ul><li><p>Indicate the shift of the pattern upon a failed comparison.</p><ul><li>$f(k)$: the length of <em><strong>the longest prefix</strong></em> of the pattern that is a suffix of the substring pattern$[1,2,\cdots,k]$. pattern$[0]$ is excluded as we shift at least one character.</li><li>Mismatch at pattern$[k+1]$, $f(k)$ tells how many preceding characters can be reused to restart the pattern.</li></ul></li><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Algorithm failureFunction(P)    F[0] = 0    i = 1    j = 0    while i < m        if P[i] == P[j]            F[i] = j + 1            i = i + 1            j = j + 1        else if j > 0 then            j = F[j-1]        else            F[i] = 0 //no match            i = i + 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>Time complexity</p><ul><li>$O(|P|+|T|)$.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hash Table</title>
      <link href="2021/04/11/Hash-Table/"/>
      <url>2021/04/11/Hash-Table/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li><strong>Hash tables</strong> are a concrete data structure which is suitable for implementing maps.</li><li>Basic idea: convert each key into an index.</li><li>Look-up of keys, insertion and deletion in a hash table usually runs in $O(1)$ time.</li></ul><h4 id="2-Hash-Tables-and-Hash-Functions"><a href="#2-Hash-Tables-and-Hash-Functions" class="headerlink" title="2. Hash Tables and Hash Functions"></a>2. Hash Tables and Hash Functions</h4><ul><li>A <strong>hash table</strong> for a given key type consists of<ul><li>Array $A$ (called table) of size $N$</li><li>Hash function $h$</li></ul></li><li>A <strong>hash function</strong> $h$ maps keys of a given type to integers in a fixed interval $[0, N-1]$</li><li>When implementing a map with a hash table, the goal is to store item $(k, o)$ at index $i = h(k)$ in the array $A$</li></ul><h4 id="3-Hash-Functions"><a href="#3-Hash-Functions" class="headerlink" title="3. Hash Functions"></a>3. Hash Functions</h4><ul><li>A hash function is usually specified as the <em>composition</em> of two function:<ul><li><strong>Hash code</strong>: $h_1: keys \rightarrow integers$</li><li><strong>Compression function</strong>: $h_2: integers \rightarrow [0, N - 1]$</li></ul></li><li>The hash code is applied first, and the compression function is applied next on the result, i.e., $h(x) = h_2(h_1(x))$</li><li>The goal of the hash function is to <strong>“disperse”</strong> the keys in an apparently <strong>random</strong> way</li></ul><h4 id="4-Collision-Handling"><a href="#4-Collision-Handling" class="headerlink" title="4. Collision Handling"></a>4. Collision Handling</h4><ul><li>Collisions occur when different elements are mapped to the same cell.</li><li><strong>Separate Chaining</strong>: let each cell in the table point to a list of entries  that map there<ul><li>Separate chaining is simple, but requires additional mempry outside the table</li></ul></li><li><strong>Linear Probing</strong><ul><li><strong>Open addressing</strong>: the colliding item is placed in a different cell of the table</li><li><strong>Linear probing</strong>: handles collisions by placing the colliding item in the next (circularly) available table cell</li><li>Accessing a cell of the array is referred to as a “probe”</li><li>Disadvantage: Colliding items lump together, causing future collisions to cause a longer sequence of probes</li></ul></li><li>Updates with Linear Probing<ul><li>To handle insertions and deletions, we introduce a special object, called <em><strong>DEFUNCT</strong></em>, which replaces deleted elements</li></ul></li></ul><h4 id="5-Double-Hashing"><a href="#5-Double-Hashing" class="headerlink" title="5. Double Hashing"></a>5. Double Hashing</h4><ul><li><p>Double hashing uses a secondary hash function $d(k)$ and handles collisions by placing an item in the first available cell of the series<br>$$<br>(h(k) + jd(k))\mod N<br>$$<br>for $j = 0, 1, \cdots, N-1$</p></li><li><p>The secondary hash function $d(k)$ cannot have zero values</p></li><li><p>Linear probing is a special case where $d(k) = 1$</p></li><li><p>The table size $N$ must be a <em>prime</em> to allow probing of all the cells</p></li><li><p>Common choice of compression function for the secondary hash function:<br>$$<br>d(k) = q - (k\mod q)<br>$$<br>where</p><ul><li>$q &lt; N$</li><li>$q$ is a <em>prime</em></li></ul></li><li><p>The possible values for $d(k)$ are $1, 2, \cdots, q$</p></li></ul><h4 id="6-Performance-of-Hashing"><a href="#6-Performance-of-Hashing" class="headerlink" title="6. Performance of Hashing"></a>6. Performance of Hashing</h4><ul><li>In the worst case, searches, insertions and removals on a hash table take $O(n)$ time</li><li>The worst case occurs when all the keys inserted into the map collide</li><li>The load factor $\alpha = n/N$ affects the performance of a hash table</li><li>The expected running time of all the map ADT operations in a hash table is $O(1)$</li></ul><h4 id="7-Summary"><a href="#7-Summary" class="headerlink" title="7. Summary"></a>7. Summary</h4><ul><li><p>Let $n$ denote the number of entries in the map, and we assume that the bucket array supporting the hash table is maintained such that its capacity is proportional to the number of entries in the map.</p></li><li><style type="text/css">.tg  {border-collapse:collapse;border-spacing:0;}.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;  overflow:hidden;padding:10px 5px;word-break:normal;}.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}.tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}</style><table class="tg"><thead align="center">  <tr>    <th class="tg-7btt" rowspan="2">Method</th>    <th class="tg-7btt" rowspan="2">Unsorted<br>List</th>    <th class="tg-7btt" colspan="2">Hash Table</th>  </tr>  <tr>    <td class="tg-c3ow">expected</td>    <td class="tg-c3ow">worst case</td>  </tr></thead><tbody align="center">  <tr>    <td class="tg-c3ow">get</td>    <td class="tg-c3ow">O(n)</td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(1)</span></td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(n)</span></td>  </tr>  <tr>    <td class="tg-c3ow">put</td>    <td class="tg-c3ow">O(n)</td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(1)</span></td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(n)</span></td>  </tr>  <tr>    <td class="tg-c3ow">remove</td>    <td class="tg-c3ow">O(n)</td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(1)</span></td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(n)</span></td>  </tr>  <tr>    <td class="tg-c3ow">size, isEmpty</td>    <td class="tg-c3ow">O(1)</td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(1)</span></td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(1)</span></td>  </tr>  <tr>    <td class="tg-c3ow">entrySet, keySet, values</td>    <td class="tg-c3ow">O(n)</td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(n)</span></td>    <td class="tg-c3ow"><span style="font-weight:normal;font-style:normal;text-decoration:none">O(n)</span></td>  </tr></tbody></table></li><li><p>Why disperse?</p><ul><li>To reduce numbers of collisions</li></ul></li><li><p>Why random?</p><ul><li>Random means no pattern</li><li>If there is an obvious pattern, then the incoming data may have a matching pattern that leads to many collisions.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graph</title>
      <link href="2021/04/10/Graph/"/>
      <url>2021/04/10/Graph/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition-Graph"><a href="#1-Definition-Graph" class="headerlink" title="1. Definition: Graph"></a>1. Definition: Graph</h4><ul><li>$G$ is an ordered triple $G = (V, E, f)$<ul><li>$V$ is a set of nodes, points, or vertices.</li><li>$E$ is a set, whose elements are known as edges or lines.</li><li>$f$ is a function maps each element of $E$ to an unordered pair of vertices in $V$</li></ul></li></ul><h4 id="2-Definition-vertex-amp-edge"><a href="#2-Definition-vertex-amp-edge" class="headerlink" title="2. Definition: vertex &amp; edge"></a>2. Definition: vertex &amp; edge</h4><ul><li><p>Vertex</p><ul><li>Basic element</li><li>Drawn as a <em><strong>node</strong></em> or a dot</li><li>Vertex set of $G$ is usually denoted by $V(G)$, or $V$</li></ul></li><li><p>Edge</p><ul><li>A set of two elements</li><li>Drawn as a <em><strong>line</strong></em> connecting two vertices, called end vertices, or endpoints</li><li>The <em><strong>edge</strong></em> set of $G$ is usually denoted by $E(G)$, or $E$</li></ul></li></ul><h4 id="3-Simple-graphs"><a href="#3-Simple-graphs" class="headerlink" title="3. Simple graphs"></a>3. Simple graphs</h4><ul><li><em>Simple graphs</em> are graphs without <em><strong>multiple edges</strong></em> or <em><strong>self-loops</strong></em>.</li></ul><h4 id="4-Directed-and-undirected-graphs"><a href="#4-Directed-and-undirected-graphs" class="headerlink" title="4. Directed and undirected graphs"></a>4. Directed and undirected graphs</h4><ul><li>Graphs can be<ul><li>Undirected (edges don’t have direction),</li><li>Directed (edges have direction).</li></ul></li><li>Undirected graphs can be represented as directed graphs where for each edge $(X, Y)$ there is a corresponding edge $(Y, X)$</li><li>Directed edge $(A, B)$.</li><li>Undirected edge ${A, B}$.</li></ul><h4 id="5-Weighted-and-unweighted-graphs"><a href="#5-Weighted-and-unweighted-graphs" class="headerlink" title="5. Weighted and unweighted graphs"></a>5. Weighted and unweighted graphs</h4><ul><li>Graphs can also be<ul><li>unweighted</li><li>weighted (edges have weights)</li></ul></li></ul><h4 id="6-Adjacency-path-amp-reachability"><a href="#6-Adjacency-path-amp-reachability" class="headerlink" title="6. Adjacency, path &amp; reachability"></a>6. Adjacency, path &amp; reachability</h4><ul><li>$A\longrightarrow B \longrightarrow C \longrightarrow D$</li><li>Node $B$ is <em><strong>adjacent</strong></em> to $A$ is there is an edge from $A$ to $B$.</li><li>A <em><strong>path</strong></em> from $A$ to $D$: a sequence of vertices, such that there is an edge from $A$ to $B$, $B$ to $C$, $\cdots$, from $C$ to $D$.</li><li>Vertex $B$ is <em><strong>reachable</strong></em> from $A$ is there is a path from $A$ to $B$.</li></ul><h4 id="7-Cycle-acyclic-amp-connected"><a href="#7-Cycle-acyclic-amp-connected" class="headerlink" title="7. Cycle, acyclic &amp; connected"></a>7. Cycle, acyclic &amp; connected</h4><ul><li><em><strong>Cycle</strong></em>: A path from a vertex to itself.</li><li>Graph is <em><strong>acyclic</strong></em> if it does not have cycles.</li><li>Graph is <em><strong>connected</strong></em> if there is a path between every pair of vertices.</li><li>Graph is <em><strong>strongly connected</strong></em> if there is a path in <em><strong>both directions</strong></em> between every pair of vertices</li></ul><h4 id="8-Degree"><a href="#8-Degree" class="headerlink" title="8. Degree"></a>8. Degree</h4><ul><li>Number of edges incident on a node.</li></ul><h4 id="9-Degree-directed-graphs"><a href="#9-Degree-directed-graphs" class="headerlink" title="9. Degree (directed graphs)"></a>9. Degree (directed graphs)</h4><ul><li>In-degree: Number of edges entering,</li><li>Out-degree: Number of edges leaving,</li><li>Degree = indeg + outdeg.</li></ul><h4 id="10-Degree-simple-facts"><a href="#10-Degree-simple-facts" class="headerlink" title="10. Degree: simple facts"></a>10. Degree: simple facts</h4><ul><li>If $G$ is a graph with $m$ edges, $\Sigma deg(v) = 2m = 2|E|$</li><li>If $G$ is a digraph, $\Sigma indeg(v) = \Sigma outdeg(v) = |E|$</li><li>The number of <em><strong>odd-degree nodes</strong></em> is even</li><li>Let $G$ be a simple graph with $n$ vertices and $m$ edges<ul><li>If $G$ is undirected, $m\le n(n-1)/2, C_n^{2}$</li><li>If $G$ is directed, $m\le n(n-1)$</li></ul></li><li>Let $G$ be an undirected graph with $n$ vertices and $m$ edges<ul><li>If $G$ is connected, $m\ge n-1$</li><li>If $G$ is a tree, $m = n-1$</li></ul></li></ul><h4 id="11-Data-structures-for-graphs"><a href="#11-Data-structures-for-graphs" class="headerlink" title="11. Data structures for graphs"></a>11. Data structures for graphs</h4><ul><li><p>Static implementation: adjacency matrix</p><ul><li><p>Store node in the array: each node is associated with an integer (array index).</p></li><li><p>Represent information about the edges using a two-dimensional array, where<br>$$<br>array[i][j] = 1<br>$$<br>iff there is an edge from node with index $i$ to the node with index $j$.</p></li><li><p>Disadvantages of adjacency matrices</p><ul><li><em><strong>Sparse graohs</strong></em><ul><li>Very few edges for a large number of vertices.</li><li>Result in many zero entries in adjacency matrix.</li><li>Waste space and makes many algorithms less efficient.</li></ul></li><li>Matrix representation is <em><strong>inflexible</strong></em>, if the number of nodes in the graph may change, (especially if we don’t know the maximal size of the graph).</li></ul></li></ul></li><li><p>Adjacency list</p><ul><li>For every vertex, keep a list of adjacent vertices.</li><li>Keep a list of vertices, or keep vertices in a Map as keys and lists of adjacent vertices as values.</li></ul></li><li><p>Edge list</p><ul><li>Represent the graph as a list of edges.</li></ul></li></ul><h4 id="12-Graph-traversals"><a href="#12-Graph-traversals" class="headerlink" title="12. Graph traversals"></a>12. Graph traversals</h4><ul><li><p>Graph traversal: visit all vertices and edges in a graph in a systematic way.</p><ul><li>Breadth-first search.</li><li>Depth-first search.</li></ul></li><li><p>What is it used for?</p><ul><li>Search for a certain <em><strong>node</strong></em> in a graph.</li><li>Search for a certain <em><strong>edge</strong></em> in a graph.</li><li>Search for a <em><strong>path</strong></em> between two nodes.</li><li>Check if a graph is <em><strong>connected</strong></em>.</li><li>Check if a graph contains <em><strong>loops</strong></em>.</li></ul></li><li><p>Breadth-first search</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">BFS starting from vertex v,Create a queue Q,Mark v as visited and put v into Q,While Q is non-empty,  Remove the head u of Q,  Mark and enqueue all (unvisited) neighbours of u.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>The BFS order is that those closet to the start point $A$ occur earliest.</p></li><li><p>The BFS search tends to “go broad (wide, flat)”.</p></li></ul></li><li><p>Depth-first search</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">DFS starting from vertex v.Create a stack S.Mark v as visited and push v onto S.While S is non-empty,    Peek at the top u of S,    If u has an (unvisited) neighbour w,        mark w and push it onto S,    else        pop S.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>The DFS search tends to “go deep”.</p></li></ul></li><li><p>Detect cycles in a directed graph</p></li><li><ul><li><p>Idea: If we encounter a vertex which is already on the <em>stack</em>, we find a loop</p><ul><li>Stack contains vertices on a path, and if we see the same vertex again, the path must contain a cycle.</li></ul></li><li><p>Instead of visited and unvisited, use three colors:</p><ul><li>White = unvisited</li><li>Grey = on the stack</li><li>Black = finished</li></ul></li><li><p>Modified DFS for cycle detection</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Modified DFS starting from v.All vertices coloured white.Create a stack S.Colour v grey and push v onto S.While S is non-empty    peek at the top u of S    if u has a grey neighbour,        a cycle is detected,    else if u has a white neighbour w,        colour w grey and push it onto S,    else        colour u black and pop S.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul></li><li><p>$firstUnmarkedAdj()$</p><ul><li><p>Adjacency list implementation.</p></li><li><p>Assume that we have a method which returns the first unmarked vertex adjacent to a given one:</p><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">GraphNode firstUnmarkedAdj(GraphNode v)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>Implementation</p><ul><li>Keep <em><strong>a pointer into the adjacency list</strong></em> of each vertex<ul><li>Do not start to traverse the list of adjacent vertices from the beginning each time.</li></ul></li><li>Or use the same iterator for this list, so when we call $next()$ it returns the next element in the list.<ul><li>Again does not start from the beginning.</li></ul></li></ul></li></ul></li><li><p>Pseudocode for breadth-first search</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Starting from vertex ss.marked = true; // marked is a field in GraphNodeQueue Q = new Queue(); //FIFOQ.enqueue(s); // add s into queuewhile (!Q.isempty()) {    v = Q.dequeue(); //remove the first element    u = firstUnmarkedAdj(v);    while (u != null) {        u.marked = true;        Q.enqueue(u);        u = firstUnmarkedAdj(v);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Complexity of BFS</p><ul><li>Assume an adjacency list representation, $|V|$ is the number of vertices, $|E|$ is the number of edges.<ul><li><em><strong>Each vertex</strong></em> is enqueued and dequeued at most once.</li><li>Scanning for all <em><strong>adjacent vertices</strong></em> takes $O(|E|)$ time, since sum of lengths of adjacency lists is $|E|$.</li><li>Gice a $O(|V|+|E|)$ time complexity.</li></ul></li></ul></li></ul></li><li><p>Pseudocode for DFS</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">s.marked = true;Stack S = new Stack(); //FILOS.push(s); //push s into stackwhile (!S.isempty()) {    v = S.peek(); //topest element in stack    u = firstUnmarkedAdj(v);    if (u == null) {        S.pop(); //no element, remove    }else {        u.marked = true;        S.push(u); //push u into stack    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Complexity of DFS</p><ul><li><em><strong>Each vertex</strong></em> is pushed on the stack and popped at most once.</li><li>For every vertec we check what the next unvisited neighbour is.</li><li>In our implementation, we traverse the <em><strong>adjacency list</strong></em> only once. This gives $O(|V| + |E|)$ again.</li></ul></li></ul></li><li><p>BFS vs. DFS</p><ul><li><table><thead><tr><th align="center"></th><th align="center">BFS</th><th align="center">DFS</th></tr></thead><tbody><tr><td align="center">Implementation</td><td align="center">Queue, size $</td><td align="center">V</td></tr><tr><td align="center">Space complexity</td><td align="center">$O(</td><td align="center">V</td></tr><tr><td align="center">Time complexity</td><td align="center">$O(</td><td align="center">V</td></tr><tr><td align="center">Application</td><td align="center">Shortest path</td><td align="center">Cycle detection</td></tr></tbody></table></li></ul></li></ul><h4 id="13-Dijkstra’s-algorithm"><a href="#13-Dijkstra’s-algorithm" class="headerlink" title="13. Dijkstra’s algorithm"></a>13. Dijkstra’s algorithm</h4><ul><li><p>An algorithm for solving the single-source shortest path problem.</p><ul><li><em><strong>Greedy</strong></em> algorithm</li><li>Compute <em><strong>distances</strong></em> from a source vertex to all others</li></ul></li><li><p>Assumptions:</p><ul><li>The graph is <em><strong>connected</strong></em>.</li><li>The edges are <em><strong>undirected</strong></em>.</li><li>The edge weights are <em><strong>nonnegative</strong></em>.</li></ul></li><li><p>Grow a “cloud” of vertices, beginning with $s$.</p></li><li><p>Store $d(v)$ representing the distance of $v$ from $s$.</p></li><li><p>At each step</p><ul><li>Add to the cloud the vertex $u$ outside the cloud with the <em>shortest distance</em> $d(u)$.</li><li><em>Update</em> the distances of the vertices <em>adjacent to $u$</em>.</li></ul></li><li><p>Edge relaxation</p><ul><li><p>Consider an edge $e = (u, z)$ such that</p><ul><li>$u$ is the vertex most recently added to the cloud</li><li>$z$ is not in the cloud</li></ul></li><li><p>The relaxation of edge $e$ updates distance $d(z)$ as follows:</p><p>$d(z)\leftarrow \min{d(z), d(u) + weight(e)}$</p></li></ul></li><li><p>Implementation</p><p>To find the shortest paths from the start vertex $s$:</p><ul><li>Priority queue PQ: vertices to be processed.</li><li>An array: Shortest distances from $s$ to every vertex.<ul><li>Initially set to be infinity for all but $s$, and $0$ for $s$.</li></ul></li><li>Order the queue by its distance. (shortest in front).</li><li>Loop while there are vertices in the queue PQ:<ul><li>Dequeue a vertex $u$.</li><li>Update the distances by Edge Relaxation.</li><li>Re-order the queue.</li></ul></li></ul></li><li><p>Modified Dijkstra’s algorithm</p><ul><li><p>To make Dijkstra’s algorithm to <strong>return the path itself</strong>, not just the distance:</p><ul><li><p>In addition to distances, <em><strong>maintain a path</strong></em> (list of vertices) for every vertex.</p></li><li><p>In the beginning, paths are empty.</p></li><li><p>When assigning $dist(s, v) = dist(s, u) + weight(u, v)$ also assign $path(v) = path(u)$.</p></li><li><p>When dequeuing a vertex, add in to its path.</p></li></ul></li></ul></li><li><p>Complexity</p><ul><li><p>Assume that the priority queue is implemented as a heap.</p></li><li><p>At each step (dequeuing a vertex $u$ and recomputing distances) we do $O(|E_u|\times \log (|V|))$ work, where $E_u$ is the set of edges with source $u$. (Up-heap, find the min).</p></li><li><p>We do this for every vertex, so total complexity is $O((|V|+|E|)\times \log(|V|))$.</p></li><li><p>Really similar to BFS and DFS, but instead of choosing some successor, we <em><strong>reorder</strong></em> a priority queue at each step, hence the $\log(|V|)$ factor.</p></li><li><p>Adaptable priority queue using unsorted sequence</p><ul><li>Complexity: $O(|V|^2+|E|)$</li></ul></li><li><p>Fibonacci heap</p><ul><li>Complexity: $O(|V|\times \log(|V|) + |E|)$.</li></ul></li></ul></li></ul><h4 id="14-Directed-acyclic-graphs-有向无环图"><a href="#14-Directed-acyclic-graphs-有向无环图" class="headerlink" title="14. Directed acyclic graphs (有向无环图)"></a>14. Directed acyclic graphs (有向无环图)</h4><ul><li>Informal definition: directed graph without directed cycles.</li></ul><h4 id="15-Topological-ordering"><a href="#15-Topological-ordering" class="headerlink" title="15. Topological ordering"></a>15. Topological ordering</h4><ul><li>Definition: A topological ordering of graph $G$ is an ordering of its $n$ vertices such that for every edge $(v_i, v_j)$ of $G$, it is the case that $i\lt j$.<ul><li>Any directed path in $G$ traverses vertices in increasing order.</li><li>May be more than $1$ ordering.</li></ul></li><li>$G$ has a topological ordering iff it is acyclic.</li></ul><h4 id="16-Topological-sort"><a href="#16-Topological-sort" class="headerlink" title="16. Topological sort"></a>16. Topological sort</h4><ul><li><p>Objective: sort vertices in order so that every edge folloes such an order.</p></li><li><p>Input: directed acyclic graph.</p></li><li><p>Output: a <em><strong>linear sequence of vertices</strong></em> such that for any two vertices $u$ and $v$, if there is an edge from $u$ to $v$, then $u$ is before $v$ in the sequence.</p></li><li><p>Algorithm</p><ul><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Create an empty array to store the ordering.While the graph is not emoty, repeat:    Find a vertex with no incoming edges.    Put this vertex in the array.    Delete the vertex from the graph.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>Note that this destructively updates a graph</p><ul><li>Make a copy of the graph first and do topological sort on the copy.</li></ul></li></ul></li></ul><h4 id="17-Cycle-detection-with-topological-sort"><a href="#17-Cycle-detection-with-topological-sort" class="headerlink" title="17. Cycle detection with topological sort"></a>17. Cycle detection with topological sort</h4><ul><li>What happens if we run topological sort on a <em><strong>cyclic</strong></em> graph?</li><li>There will be either no vertex with $0$ prerequisites to begin with, or at some point in the iteration.</li><li>If we run a topological sort on a graph and there are vertices left undeleted, the graph contains a cycle.</li><li>Why does topological sort work?<ul><li>A vertex cannot be removed before all its prerequisites have been removed. (No incomming edges)</li><li>It cannot be inserted in the array before its prerequisite. (So the array is arranged in topological order).</li><li>Cycle detection: in a cycle, a vertex is its own prerequisite. So it can never be removed.</li></ul></li></ul><h4 id="18-Minimal-spanning-tree"><a href="#18-Minimal-spanning-tree" class="headerlink" title="18. Minimal spanning tree"></a>18. Minimal spanning tree</h4><ul><li><p>Input: <em><strong>connected, undirected, weighted</strong></em> graph</p></li><li><p>Output: a tree which connects all vertices in the graph using only the edges present in the graph and is minimal in the sense that <em><strong>the sum of weights of the edges is the smallest</strong></em> possible.</p></li><li><p><em><strong>Spanning tree</strong></em>: a tree contains all vertices of a connected graph.</p></li><li><p><em><strong>Minimal spanning tree</strong></em>: The spanning tree with the smallest total weight.</p></li><li><p>Prim’s algorithm</p><ul><li><p>A <em><strong>greedy</strong></em> algorithm to construct an MST:</p></li><li><pre class="line-numbers language-pseudocode"><code class="language-pseudocode">Pick any vertex M.Choose the shortest edge from M to any other vertex N.Add edge (M, N) to the MST.Continue to add at every step the shortest edge from a vertex in MST to a vertex outside, until all vertices are in MST.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h4 id="19-Euler-path"><a href="#19-Euler-path" class="headerlink" title="19. Euler path"></a>19. Euler path</h4><ul><li>A path that uses every <strong>edge</strong> of a graph exactly once.<ul><li>Start and end at <strong>different</strong> vertices.</li></ul></li><li>The Criterion for Euler Paths<ul><li>The two endpoints of $P$ must be <strong>odd</strong> vertices.</li><li>All vertices other than the two endpoints of $P$ must be <strong>even</strong> vertices.</li></ul></li><li>Explanation Using Degree of Graph<ul><li>A graph has Eular paths iff it has two <strong>odd-degree</strong> nodes, and all other nodes are <strong>even-degree</strong> nodes.</li></ul></li></ul><h4 id="20-Euler-Circuit"><a href="#20-Euler-Circuit" class="headerlink" title="20. Euler Circuit"></a>20. Euler Circuit</h4><ul><li>An Euler circuit is a <strong>circuit</strong> that uses every edge of a graph exactly once.</li><li>An Euler circuit starts and ends at the <strong>same</strong> vertex.</li><li>Explanation Using Degree of Graph<ul><li>A connected graph has Euler circuits iff all nodes are <strong>even-degree</strong> node.</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Binary Search Tree</title>
      <link href="2021/03/11/Binary-Search-Tree/"/>
      <url>2021/03/11/Binary-Search-Tree/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Ordered-Maps"><a href="#1-Ordered-Maps" class="headerlink" title="1. Ordered Maps"></a>1. Ordered Maps</h4><ul><li>Keys are assumed to come from a total order</li><li>Items are stored in order by their keys</li><li>This allows us to support <strong>nearest neighbor queries</strong>:<ul><li>Item with largest key less than or equal to $k$</li><li>Item with smallest key greater than or equal to $k$</li></ul></li></ul><h4 id="2-Sorted-Search-Tables"><a href="#2-Sorted-Search-Tables" class="headerlink" title="2. Sorted Search Tables"></a>2. Sorted Search Tables</h4><ul><li><p>Store the map’s entries in an array list $A$ so that they are in increasing order of their keys.</p></li><li><p>We refer to this implementation as a <em><strong>sorted search table</strong></em>.</p></li><li><p>A search table is an ordered map implemented by means of a sorted sequence</p><ul><li>We store the items in an array-based sequence, sorted by key</li><li>We use an external comparator for the keys</li></ul></li><li><p>Performance:</p><ul><li>Searches take $O(\log n)$ time, using binary search</li><li>Inserting a new item takes $O(n)$ time, since in the worst case we have to shift $n$ items to make room for the new item</li><li>Removing an item takes $O(n)$ time, since in the worst case we have to shift $n$ items to compact the items after the removal</li></ul></li><li><p>The lookup table is efficient only for ordered maps of small size or for maps on which searches are the most common operations, while insertions and removals are rarely performed</p></li><li><p>Performance of a sorted map (implemented using a sorted search table)</p><ul><li><table><thead><tr><th align="center">Method</th><th align="center">Running Time</th></tr></thead><tbody><tr><td align="center">size</td><td align="center">$O(1)$</td></tr><tr><td align="center">get</td><td align="center">$O(\log n)$</td></tr><tr><td align="center">put</td><td align="center">$O(n)$; $O(\log n)$ if map has entry<br>with given key</td></tr><tr><td align="center">remove</td><td align="center">$O(n)$</td></tr><tr><td align="center">firstEntry, lastEntry</td><td align="center">$O(1)$</td></tr><tr><td align="center">ceilingEntry, floorEntry<br>lowerEntry, higherEntry</td><td align="center">$O(\log n)$</td></tr><tr><td align="center">subMap</td><td align="center">$O(s+\log n)$ where $s$ items are<br>reported</td></tr><tr><td align="center">entrySet, keySet, values</td><td align="center">$O(n)$</td></tr></tbody></table></li></ul></li><li><p>Motivation</p><ul><li>Binary search on ordered arrays is efficient: $O(\log_2 n)$</li><li>However insertion or removal os an item in an ordered array is slow: $O(n)$</li><li>Ordered arrays are best suited for static searching, where search space does not change</li><li>Binary search trees can be used for efficient dynamic searching</li></ul></li></ul><h4 id="3-Binary-Search-Trees"><a href="#3-Binary-Search-Trees" class="headerlink" title="3. Binary Search Trees"></a>3. Binary Search Trees</h4><ul><li>A binary search tree is a <em>proper</em> binary tree storing keys (or key-value entries) at its <em>internal nodes</em> and satisfying the following properties:<ul><li>Let $u, v$, and $w$ be three nodes such that $u$ is in the left <em>subtree</em> of $v$ and $w$ is in the right <em>subtree</em> of $v$. We have $key(u)\le key(v)\le key(w)$</li><li>Assuming there are no duplicate keys, we have $key(u)\lt key(v)\lt key(w)$</li></ul></li><li>External nodes do not store items<ul><li>and likely are not actually implemented, but are just null links from the parent</li></ul></li><li>Search<ul><li>To search for a key $k$, we trace a downward path starting at the root</li><li>The next node visited depends on the comparison of $k$ with the key of the current node</li><li>If we reach a leaf, the key is not found</li></ul></li><li>Insertion<ul><li>Have to insert $k$ where a $get(k)$ would find it</li><li>So natural that $put(k, o)$ starts with $get(k)$</li><li>Search for key $k$ (using TreeSearch)</li><li>Assume $k$ is already in the tree then just replace the value</li><li>Otherwise, let $w$ be the leaf reached by the search, we insert $k$ at node $w$ and expand $w$ into an internal node</li></ul></li><li>Deletion<ul><li>To perform operation $remove(k)$, we search for $k$</li><li>Assume key $k$ is in the tree, and let $v$ be the node storing $k$</li><li>If node $v$ has a leaf child $w$, we remove $v$ and $w$ from the tree with operation $removeExternal(w)$, which removes $w$ and its parent</li><li>We consider the case where the key $k$ to be removed is stored at a node $v$ whose children are both internal<ul><li>we find the internal node $w$ that follows $v$ in an <strong>inorder traversal</strong></li><li>we copy $key(w)$ into node $v$</li><li>we remove node $w$ and its left child $z$ (which must be a leaf) by means of operation $removeExternal(z)$</li></ul></li></ul></li><li>Performance<ul><li>Consider an ordered map with $n$ items implemented by means of a binary search tree of height $h$<ul><li>the space used is $O(n)$</li><li>methods <em>get</em>, <em>put</em> and <em>remove</em> take $O(h)$ time</li></ul></li><li>The height $h$ is $n$ in the worst case and $\log n$ in the best case</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Heap</title>
      <link href="2020/11/22/Heap/"/>
      <url>2020/11/22/Heap/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A heap is a binary tree storing keys at its nodes and satisfying the following properties:<ul><li><strong>Heap-Order</strong>: for every internal node $v$ other than the root, $key(v) \ge key(parent(v))$</li><li><strong>Complete Binary Tree</strong>: let $h$ be the height of the heap<ul><li>levels $i = 0, \cdots, h-1$ have the maximal number of nodes, i.e. there are $2^i$ nodes at depth $i$.</li><li>The remaining nodes at level/depth $h$ reside in the leftmost possible positions at that level/depth.</li></ul></li><li>The <strong>last node</strong> of a heap is the rightmost node of maximum depth</li></ul></li></ul><h4 id="2-Insertion-into-a-Heap"><a href="#2-Insertion-into-a-Heap" class="headerlink" title="2. Insertion into a Heap"></a>2. Insertion into a Heap</h4><ul><li><p>Method insertItem of the priority queue ADT corresponds to the insertion of a key $k$ to the heap</p></li><li><p>The insertion algorithm consists os three steps</p><ul><li>Find the insertion node $z$ (the new last node)</li><li>Store $k$ at $z$</li><li>Restore the heap-order property</li></ul></li><li><p><strong>Upheap</strong></p><ul><li><p>After the insertion of a new key $k$, the heap-order property may be violated</p></li><li><p>Algorithm up heap restores the heap-order property by swapping $k$ along an upward path from the insertion node</p></li><li><p>Upheap terminates when the key $k$ reaches the root or a node whose parent has a key smaller than or equal to $k$</p></li><li><p>Since a heap has height $O(\log n)$, up heap runs in $O(\log n)$ time</p></li></ul></li></ul><h4 id="3-Removal-from-a-Heap"><a href="#3-Removal-from-a-Heap" class="headerlink" title="3. Removal from a Heap"></a>3. Removal from a Heap</h4><ul><li>Method removeMin of the priority queue ADT corresponds to the removal of the root key from the heap</li><li>The removal algorithm consists of three steps<ul><li>Replace the root key with the key of the last node $w$</li><li>Remove $w$</li><li>Restore the heap-order property</li></ul></li><li><strong>Downheap</strong><ul><li>After replacing the root key with the key $k$ of the last node, the heap-order property may be violated</li><li>Algorithm downheap restores the heap-order property by swapping key $k$ along a downward path from the root (always swap with the smallest child)</li><li>Downheap terminates when key $k$ reaches a leaf or a node whose children have keys greater than or equal to $k$</li><li>Since a heap has height $O(\log n)$, downheap runs in $O(\log n)$ time</li></ul></li></ul><h4 id="4-Array-besed-Heap-Implementation"><a href="#4-Array-besed-Heap-Implementation" class="headerlink" title="4. Array-besed Heap Implementation"></a>4. Array-besed Heap Implementation</h4><ul><li>We can represent a heap with $n$ keys by means of a vector or ArrayList of length $n+1$</li><li>The cell of at index $0$ is not used</li><li>Links between nodes are not explicitly stored</li><li>For the node at index $i$<ul><li>the left child is at index $2i$</li><li>the right child is at index $2i+1$</li></ul></li><li>Operation insert corresponds to inserting at index $n+1$</li><li>Operation removeMin corresponds to moving index $n$ to index $1$</li></ul><h4 id="5-Implementing-Priority-Queue-with-a-Heap"><a href="#5-Implementing-Priority-Queue-with-a-Heap" class="headerlink" title="5. Implementing Priority Queue with a Heap"></a>5. Implementing Priority Queue with a Heap</h4><ul><li>To create a priority queue, initialise a heap</li><li>To insert in the priority queue, insert in the heap</li><li>To get the value with the minimal key, ask for the value of the root of the heap</li><li>To dequeue the highest priority item, remove the root and return the value stored there.</li></ul><h4 id="6-Heap-Sort"><a href="#6-Heap-Sort" class="headerlink" title="6. Heap-Sort"></a>6. Heap-Sort</h4><ul><li>Using a heap-based priority queue, we can sort a sequence of $n$ elements in $O(n\log n)$ time</li><li>The resulting algorithm is called heap-sort</li><li>Heap-sort is much faster than quadratic sorting algorithms, such as insertion-sort and selection-sort</li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memory Management</title>
      <link href="2020/11/10/Memory-Management/"/>
      <url>2020/11/10/Memory-Management/</url>
      
        <content type="html"><![CDATA[<h4 id="1-内存层次-Memory-Hierarchies"><a href="#1-内存层次-Memory-Hierarchies" class="headerlink" title="1. 内存层次(Memory Hierarchies)"></a>1. 内存层次(Memory Hierarchies)</h4><ol><li>Computers typically have memory hierarchies: <ul><li>Registers, L1/L2/L3 cache (volatile)</li><li>Main memory (volatile)</li><li>Disks (nonvolatile)</li><li><img src="/2020/11/10/Memory-Management/Memory%20Hierarchies.png?lastModify=1630990878" alt="img"></li></ul></li><li><strong>Higher memory</strong> is faster, more expensive and volatile, <strong>lower memory</strong> is slower, cheaper and non-volatile.</li><li>Memory can be seen as one <strong>linear array</strong> of bytes/words.<ul><li><img src="/2020/11/10/Memory-Management/Memory%20Structure.png?lastModify=1630990878" alt="img"></li></ul></li><li><strong>OS responsibilities:</strong><ul><li><strong>Allocate/Deallocate</strong> memory when requested by processes, <strong>keep track of</strong> <strong>used/unused</strong> memory.</li><li><strong>Transparently</strong> move data from <strong>memory</strong> to <strong>disk</strong> and vice versa.</li><li><strong>Distribute memory</strong> between processes and simulate an <strong>“infinitely large”</strong> memory space.</li><li><strong>Control access</strong> when multiprogramming is applied.</li></ul></li></ol><h4 id="2-操作系统的内存管理方式"><a href="#2-操作系统的内存管理方式" class="headerlink" title="2. 操作系统的内存管理方式"></a>2. 操作系统的内存管理方式</h4><ol><li>操作系统中采用的内存管理方式<ul><li>重定位(relocation)</li><li>分段(segmentation)</li><li>分页(paging)</li><li>虚拟存储(virtual memory)</li></ul></li><li>实现高度依赖硬件<ul><li>与计算机存储架构紧耦合</li><li>MMU(内存管理单元): 处理CPU存储访问请求的硬件</li></ul></li></ol><h4 id="3-地址空间和地址生成"><a href="#3-地址空间和地址生成" class="headerlink" title="3. 地址空间和地址生成"></a>3. 地址空间和地址生成</h4><ol><li>地址空间定义:<ul><li>物理地址空间——硬件支持的地址空间<ul><li>起始地址为0，直到MAXsys</li></ul></li><li>逻辑地址空间——在CPU运行的进程看到的地址<ul><li>起始地址为0，直到MAXprog</li></ul></li></ul></li><li>逻辑地址生成<ul><li><img src="/2020/11/10/Memory-Management/Screen%20Shot%202020-12-09%20at%203.54.27%20PM.png?lastModify=1630990878" alt="img"></li></ul></li><li>地址生成时机和限制<ul><li>编译时<ul><li>假设起始地址已知</li><li>如果起始地址改变，必须重新编译</li></ul></li><li>加载时<ul><li>如编译时起始位置未知，编译器需生成可重定位的代码(relocatable code)</li><li>加载时，生成绝对地址</li></ul></li><li>执行时<ul><li>执行时代码可移动</li><li>需地址转换(映射)硬件支持</li></ul></li></ul></li><li>地址生成过程<ul><li>CPU<ul><li>ALU: 需要逻辑地址的内存内容</li><li>MMU: 进行逻辑地址和物理地址的转换</li><li>CPU控制逻辑: 给总线发送物理地址请求</li></ul></li><li>内存<ul><li>发送物理地址的内容给CPU</li><li>或接收CPU数据到物理地址</li></ul></li><li>操作系统<ul><li>建立逻辑地址LA和物理地址PA的映射</li></ul></li></ul></li><li>地址检查<ul><li><img src="/2020/11/10/Memory-Management/Screen%20Shot%202020-12-09%20at%204.34.03%20PM.png?lastModify=1630990878" alt="img"></li></ul></li></ol><h4 id="4-Contiguous-Memory-连续内存"><a href="#4-Contiguous-Memory-连续内存" class="headerlink" title="4. Contiguous Memory (连续内存)"></a>4. Contiguous Memory (连续内存)</h4><ol><li><p>Mono-programming(单道程序)</p><ul><li>Only <strong>one single user process</strong> is in memory/executed at any point in time (no multi-programming)</li><li>A fixed region of memory is allocated to the <strong>OS/kernel</strong>, the remaining memory is reserved for a <strong>single process</strong> (MS-DOS worked this way) </li><li>This process has <strong>direct access</strong> to <strong>physical memory</strong> (i.e. no address translation takes place) </li><li>Every process is allocated <strong>contiguous block of memory</strong>, i.e. it contains no “holes” or “gaps” (⇔ <strong>non-contiguous allocation</strong>)</li><li><strong>One process</strong> is allocated the <strong>entire memory space</strong>, and the process is <strong>always located in the same address space</strong></li><li><strong>No protection</strong> between different user processes required (one process)</li><li><strong>Overlays</strong> enable the programmer to use more memory than available (burden on programmer)</li><li><strong>Shortcomings</strong> of Mono-programming:<ul><li>Since a process has <strong>direct access to the physical memory</strong>, it may have <strong>access to OS</strong> memory</li><li>The operating system can be seen as a process - so we have <strong>two processes</strong> anyway</li><li><strong>Low</strong> <strong>utilisation</strong> of hardware resources (CPU, I/O devices, etc.)</li><li>Mono-programming is unacceptable as <strong>multiprogramming is expected</strong> on modern machines</li></ul></li><li>Direct memory access and mono-programming is common in basic <strong>embedded systems</strong> and <strong>modern consumer electronics</strong>, e.g. washing machines, microwaves, etc.</li></ul></li><li><p>Multi-programming(多道程序): <strong>A Probabilistic Model</strong>(概率模型)</p><ul><li>There are <strong>n independent processes in memory</strong></li><li>A process spends <strong>p percent</strong> of its time <strong>waiting for I/O</strong></li><li><strong>CPU</strong> <strong>Utilisation</strong> is calculated as 1 minus the time that <strong>all processes are waiting for I/O:</strong> e.g., p = 0.9 then CPU utilisation = 1 - 0.9 ⇒ 0.1 =&gt; (1−p)</li><li>The <strong>probability</strong> that all n processes are waiting for I/O (i.e., the CPU is idle) is pn , i.e. p ×p ×p . . . </li><li>The <strong>CPU</strong> <strong>utilisation</strong> is given by 1−pn</li><li>CPU <strong>utilisation goes up</strong> with <strong>the number of processes</strong> and down for <strong>increasing levels of I/O ratio</strong></li><li>Muiti-programming does able to improve resource utilisation</li></ul></li><li><p>Fixed partitions (固定分区)</p><ul><li><p>Divide memory into <strong>static, contiguous and equal sized partitions</strong> that have a <strong>fixed size</strong> and <strong>fixed location</strong> (静态大小相等分区)</p><ul><li>Any process can take up <strong>any</strong> (large enough) <strong>partition</strong></li><li>Allocation of <strong>fixed equal sized partitions</strong> to processes is <strong>trivial</strong></li><li>Very <strong>little overhead</strong> and <strong>simple implementation</strong></li><li>The operating system keeps a track of which partitions are being <strong>used</strong> and which are <strong>free</strong></li></ul></li><li><p><strong>Disadvantages</strong> of static equal-sized partitions</p><ul><li><strong>Low memory utilisation</strong> and <strong>internal fragmentation</strong>: partition may be unnecessarily large</li><li><strong>Overlays</strong> must be used if a program does not fit into a partition (burden on programmer)</li></ul></li><li><p>Divide memory into <strong>static</strong> and <strong>non-equal sized partitions</strong> that have a <strong>fixed size</strong> and <strong>fixed location</strong> (静态大小不等分区)</p><ul><li><strong>Reduces internal fragmentation</strong></li><li>The <strong>allocation</strong> of processes to partitions must be <strong>carefully considered</strong></li></ul></li><li><p>Allocation Methods</p><ol><li><strong>One private queue per partition</strong></li></ol><ul><li>Assigns each process to the <strong>smallest partition</strong> that it would fit in</li><li><strong>Reduces internal fragmentation</strong></li><li>Can <strong>reduce memory utilisation</strong> (e.g., lots of small jobs result in unused large partitions) and result in <strong>starvation</strong></li></ul><ol><li><strong>A single shared queue</strong> for all partitions can allocate small processes to <strong>large partitions</strong> but results in <strong>increased internal fragmentation</strong></li></ol></li><li><p><strong>Cons</strong> of fixed partitioning</p><ul><li>Limits the number of <strong>active processes (overlays)</strong></li><li>Small job not <strong>utilize partition space efficiently</strong> (internal fragmentation)</li><li>Processes’ memory requirement need to be <strong>known beforehand</strong>.</li></ul></li></ul></li><li><p>Dynamic partitioning (动态分区)</p><ul><li><strong>Fixed partitioning</strong> results in <strong>internal fragmentation</strong>:<ul><li>An <strong>exact match</strong> between the requirements of the process and the available partitions <strong>may not exist</strong></li><li>The partition <strong>may not be used entirely</strong></li></ul></li><li>Dynamic partitioning<ul><li>A <strong>variable number of partitions</strong> of which the <strong>size</strong> and <strong>starting address</strong> <strong>can change</strong> over time</li><li>A process is allocated the <strong>exact amount of contiguous memory</strong> it requires, thereby <strong>preventing internal fragmentation</strong></li></ul></li></ul></li></ol><ul><li>Swapping<ul><li><strong>Swapping</strong> holds some of the <strong>processes on the hard disk</strong> and <strong>shuttles processes</strong> between the hard disk and main memory as necessary</li></ul></li><li><strong>Reasons</strong> for swapping:<ul><li>Some <strong>processes</strong> only <strong>run occasionally</strong></li><li>We have <strong>more processes</strong> than <strong>partitions</strong> (assuming fixed partitions)</li><li>A process’s <strong>memory requirements</strong> have <strong>changed</strong>, e.g. increased</li><li>The <strong>total amount of memory that is required</strong> for the processes <strong>exceeds the available memory</strong></li></ul></li><li>Allocation structure: Bitmaps<ul><li>The simplest data structure to <strong>keep track of free memory</strong> is a form of <strong>bitmap</strong></li><li><strong>Memory</strong> is <strong>split into blocks</strong> of say 4 kilobyte size<ul><li>A bit map is set up so that <strong>each bit</strong> is <strong>0</strong> if the <strong>memory block is free</strong> and <strong>1</strong> if the <strong>memory block is used</strong>, e.g.<ul><li>32MB memory = 32 * 220 bytes / 4K blocks =&gt; 8192 bitmap entries</li><li>8192 bits occupy 8192 / 8 = 1K bytes of storage</li></ul></li><li>The size of this bitmap will depend on the <strong>size of the memory</strong> and the <strong>size of the allocation unit</strong></li></ul></li><li>A <strong>trade-off exists</strong> between the <strong>size of the bitmap</strong> and the <strong>size of blocks</strong><ul><li>The <strong>size of bitmaps</strong> can become prohibitive for small blocks and may <strong>make searching</strong> the bitmap <strong>slower</strong></li><li>Larger blocks may increase <strong>internal fragmentation</strong></li></ul></li><li><strong>Bitmaps</strong> are <strong>rarely used</strong> for this reason</li></ul></li><li>Allocation structure: Linked List<ul><li>A more <strong>sophisticated data structure</strong> is required to deal with <strong>a variable number</strong> of <strong>free and used partitions</strong></li><li><strong>A linked list</strong> is one such possible data structure<ul><li>A linked list consists of a <strong>number of entries</strong> (“links”)</li><li>Each link <strong>contains data items</strong>, e.g. <strong>start of memory block</strong>, <strong>size</strong>, free/allocated <strong>flag</strong></li><li>Each link also contains a <strong>pointer to the next</strong> in the chain</li></ul></li><li><img src="/2020/11/10/Memory-Management/Picture1.png?lastModify=1630990878" alt="img"></li></ul></li></ul><ol><li>The operating system is responsible for:<ul><li>Applying strategies to (quickly) <strong>allocate processes</strong> to available memory (“holes”)</li><li>Managing <strong>free space</strong></li></ul></li><li>Allocating Available Memory: Algorithms<ul><li><strong>First fit</strong> starts scanning <strong>from the start</strong> of the linked list until a link is found which represents <strong>free space of sufficient size</strong><ul><li>If requested space is <strong>the exact same size</strong> as the “hole”, all the space is allocated (i.e. no <strong>internal</strong> fragmentation)</li><li>Else, the free link is <strong>split into two</strong>:<ul><li>The first entry is set to the <strong>size requested</strong> and marked <strong>“used”</strong></li><li>The second entry is set to <strong>remaining size</strong> and marked <strong>“free”</strong></li></ul></li></ul></li><li>The <strong>next fit algorithm</strong> maintains a record of where it got to:<ul><li>It <strong>starts its search</strong> from <strong>where it stopped last time</strong></li><li>It gives an <strong>even chance to all memory to get allocated</strong> (first fit concentrates on the start of the list)</li><li>However, simulations have shown that next fit actually gives <strong>worse performance</strong> than first fit</li></ul></li><li>Best fit:<ul><li><strong>First fit</strong> just looks for the <strong>first available hole</strong><ul><li>It doesn’t take into account that there may be <strong>a hole later</strong> in the list that <strong>exactly(-ish)</strong> fits the requested size</li><li>First fit <strong>may break up a big hole</strong> when the right size hole exists later on</li></ul></li><li><strong>Next fit</strong> doesn’t improve that model</li><li>The <strong>best fit algorithm</strong> always <strong>searches the entire linked list</strong> to find the <strong>smallest hole big enough</strong> to satisfy the memory request<ul><li>It is <strong>slower</strong> than first fit</li><li>It also results in <strong>more wasted memory</strong> (memory is more likely to fill up with tiny-useless-holes, first fit generates larger holes on the average)</li></ul></li></ul></li><li>Worst fit:<ul><li><strong>Tiny holes</strong> are created when <strong>best fit</strong> splits an empty partition</li><li>The <strong>worst fit algorithm</strong> finds the <strong>largest available empty partition</strong> and splits it<ul><li>The <strong>left over part will still be large</strong> (and <strong>potentially more useful</strong>)</li><li>Simulations have also shown that worst fit is <strong>not very good either</strong></li></ul></li></ul></li><li>Summary:<ul><li><strong>First fit</strong>: allocate <strong>first block</strong> that is <strong>large enough</strong></li><li><strong>Next fit</strong>: allocate <strong>next block</strong> that is large enough, i.e. <strong>starting from the current location</strong></li><li><strong>Best fit</strong>: choose block that <strong>matches</strong> required size <strong>closet</strong> - O(N) complexity</li><li><strong>Worst fit</strong>: choose the <strong>larget possible block</strong> - O(N) complexity</li></ul></li><li>Quick fit and others:<ul><li><strong>Quick fit</strong> maintains <strong>lists of commonly used sizes</strong><ul><li>For example a separate list for each of 4K, 8K, 12K, 16K, etc. holes</li><li><strong>Odd sizes</strong> can either go into the <strong>nearest size</strong> or into a <strong>special separate list</strong></li></ul></li><li>It is <strong>much faster</strong> to find the required size holes using <strong>quick fit</strong> (table of entries, entries point to head of holes)</li><li>Similar to next fit, it has the problem of creating <strong>many tiny holes</strong></li><li>Finding neighbours for <strong>coalescing</strong> (combining empty partitions) becomes more difficult/time consuming</li></ul></li></ul></li><li>Managing Available Memory: Coalescing<ul><li><strong>Coalescing</strong> (joining together) takes place when two <strong>adjacent entries</strong> in the linked list <strong>become free</strong><ul><li>There may be three adjacent free entries if a used block in-between two free blocks is freed</li></ul></li><li>Both <strong>neighbours</strong> are examined when a <strong>block is freed</strong><ul><li>If either (or both) are <strong>also free</strong></li><li>Then the two (or three) <strong>entries are combined</strong> into one larger block by adding up the sizes<ul><li>The earlier block in the linked list gives the <strong>start point</strong></li><li>The <strong>separate links are deleted</strong> and a single link inserted</li></ul></li></ul></li></ul></li><li>Managing Available Memory: Compacting (紧凑)<ul><li>Even with coalescing happening automatically, <strong>free blocks</strong> may still <strong>distributed across memory</strong><ul><li>=&gt; <strong>Compacting</strong> can be used to join free and used memory (but is <strong>time consuming</strong>)</li></ul></li><li><strong>Compacting is more difficult and time consuming</strong> to implement than coalescing (processes have to be moved)<ul><li>Each process is <strong>swapped out</strong> &amp; <strong>free space coalesced</strong></li><li>Process swapped back in at lowest available location</li></ul></li><li>通过移动分配给进程的内存分区，以合并外碎片</li><li>碎片紧凑的条件<ul><li>所有的应用程序可动态重定位</li></ul></li></ul></li><li>Difficulties of Dynamic Partitioning<ul><li>The exact <strong>memory requirements</strong> may <strong>not be known</strong> in advance (<strong>heap</strong> and <strong>stack</strong> grow dynamically)</li><li><strong>External fragmentation</strong><ul><li><strong>Swapping</strong> a process out of memory will create a <strong>“hole”</strong></li><li>A new process may not use the entire “hole”, leaving a small <strong>unused block</strong></li><li>A new process may be <strong>too large</strong> for a given “hole”</li></ul></li><li>The <strong>overhead</strong> of memory <strong>compaction</strong> to <strong>revocer holes</strong> can be <strong>prohibitive</strong> and requires <strong>dynamic relocation</strong><ul><li>Requires a lot of CPU time</li></ul></li></ul></li><li>Overview and Shortcomings<ul><li>Different contiguous memory allocation schemes have different advantages/disadvantages<ul><li><strong>Mono-programming</strong> is easy but does result in <strong>low resource utilisation</strong></li><li><strong>Fixed partitioning</strong> facilitates <strong>multi-programming</strong> but results in <strong>internal fragmentation</strong></li><li><strong>Dynamic partitioning</strong> facilitates <strong>multi-programming</strong>, reduces <strong>internal fragmentation</strong>, but results in <strong>external fragmentation</strong> (allocation methods, coalescing, and compacting help)</li></ul></li></ul></li></ol><h4 id="5-Code-relocation-and-protection"><a href="#5-Code-relocation-and-protection" class="headerlink" title="5. Code relocation and protection"></a>5. Code relocation and protection</h4><ol><li>Principles:</li></ol><ul><li><strong>Relocation</strong>: when a program is run, it <strong>does not know in advance</strong> which <strong>partition/address</strong> it will occupy<ul><li>The program cannot simply generate <strong>static address</strong> (e.g. jump instructions) that are <strong>absolute</strong></li><li>Addresses should be <strong>relative to where the program has been loaded</strong></li><li><strong>Relocation must be solved</strong> in an operating system that allows processes to run at <strong>changing memory locations (on the fly)</strong></li></ul></li><li><strong>Protection</strong>: once you can have two programs in memory at the same time, protection must be enforced</li></ul><ol><li>Address Types</li></ol><ul><li>A <strong>logical address</strong> is a memory address <strong>seen by the process</strong><ul><li>It is <strong>independent</strong> of the current <strong>physical memory</strong> assignment</li><li>It is, e.g., <strong>relative to the start of the program</strong></li></ul></li><li>A <strong>physical address</strong> refers to an <strong>actual location</strong> in <strong>main memory</strong></li><li>The <strong>logical address space</strong> must be mapped onto the machine’s <strong>physical address space</strong></li></ul><ol><li>Approaches</li></ol><ul><li><strong>Static “relocation” at compile time:</strong> a process has to be located at the same location every single time (impractical)</li><li><strong>Dynamic relocation</strong> at <strong>load time</strong><ul><li>An <strong>offset</strong> is added to every logical address to <strong>account for its physical location</strong> in memory</li><li><strong>Slows down</strong> the loading of a process, does not account for <strong>swapping</strong></li></ul></li><li><strong>Dynamic relocation</strong> at <strong>runtime</strong><ul><li>每个CPU配置2个特殊的硬件寄存器，叫作<strong>基址寄存器</strong>和<strong>界限寄存器</strong>，当一个程序运行时，程序的起始物理地址装载到<strong>基址寄存器</strong>中，程序的长度装载到<strong>界限寄存器</strong>中，每次一个进程访问内存，取一条指令，读或写一个数据字，CPU硬件会在把地址发送到内存总线之前，自动把基址值加到进程发出的地址值上，同时，它检查程序提供的地址是否等于或大于界限寄存器里的值，如果地址超出界限，则产生错误并中止访问</li><li>Two special purpose registers are maintained in the CPU (the <strong>MMU</strong>), containing a <strong>base address</strong> and <strong>limit</strong><ul><li>The <strong>base register</strong> stores the <strong>start address</strong> of the partition</li><li>The <strong>limit register</strong> holds the <strong>size</strong> of the partition (end of program)</li></ul></li><li><strong>At runtime:</strong><ul><li>The <strong>base register</strong> is added to the <strong>logical (relative) address</strong> to generate the <strong>physical address</strong></li><li>The resulting address is <strong>compared</strong> against the <strong>limit address</strong></li></ul></li><li>The Relocation also provides a measure of <strong>protection</strong>: each process image is <strong>isolated</strong> by the contents of the base and bounds registers and <strong>safe from unwanted accesses</strong> by other processes.</li><li>缺点：<ul><li>每次访问内存都需要进行加法和比较运算，加法由于进位传递时间的问题，在没有使用特殊电路的情况下会显得很慢</li></ul></li></ul></li></ul><h4 id="6-Non-Contiguous-Memory-非连续内存"><a href="#6-Non-Contiguous-Memory-非连续内存" class="headerlink" title="6. Non-Contiguous Memory (非连续内存)"></a>6. Non-Contiguous Memory (非连续内存)</h4><ol><li>Paging (分页)</li></ol><ul><li>Principles<ul><li><strong>Paging</strong> uses the principles of <strong>fixed partitioning</strong> and <strong>code re-location</strong> to devise a new <strong>non-contiguous management scheme:</strong><ul><li>Memory is split into much <strong>smaller blocks</strong> and <strong>one or more blocks</strong> are allocated to a process<ul><li>e.g., a 11kb process would take up 3 blocks of 4kb</li></ul></li><li>These blocks <strong>do not have to be contiguous in main memory</strong>, but the process still <strong>perceives them to be contiguous</strong></li></ul></li><li>Benefits compared to contiguous schemes include:<ul><li><strong>Internal fragmentation</strong> is reduced to the <strong>last “block”</strong> only</li><li>There is <strong>no external fragmentation</strong>, since physical blocks are <strong>stacked directly onto each other</strong> in main memory</li></ul></li></ul></li><li>Definitions<ul><li>A <strong>page</strong> is a small block of <strong>contiguous memory</strong> in the <strong>logical address space</strong>, i.e. as seen by the process</li><li>A <strong>page frame (页框)</strong> is a <strong>small contiguous block</strong> in <strong>physical memory</strong></li><li>Pages and frames (usually) have the <strong>same size:</strong><ul><li>The size is usually a power of 2</li><li>Sizes range between 512 bytes and 1Gb</li></ul></li></ul></li><li>Relocation<ul><li><strong>Logical address</strong> (page number, offset within page) needs to be <strong>translated</strong> into a <strong>physical address</strong> (frame number, offset within frame)</li><li><strong>Multiple “base registers”</strong> will be required:<ul><li>Each logical page needs <strong>a separate “base register”</strong> that specifies the start of the associated frame</li><li>i.e., a <strong>set of base registers</strong> has to be maintained for each process</li></ul></li><li>The base registers are stored in the <strong>page table</strong></li></ul></li><li>Relocation: Page Tables<ul><li>The page table can be seen as <strong>a function</strong>, that <strong>maps the page number</strong> of the logical address <strong>onto the frame number</strong> of the physical address<ul><li>frameNumber = f(pageNumber)</li></ul></li><li>The <strong>page number</strong> is used as <strong>index to the page table</strong> that lists the <strong>number of associated frame</strong>, i.e. it contains the location of the frame in memory</li><li>Every process has its <strong>own page table</strong> containing its own “base registers”</li><li>The <strong>operating system</strong> maintains a <strong>list of free frames</strong></li></ul></li><li>Address Translation: Implementation<ul><li>A <strong>logical (physical) address</strong> is relative to the start of the <strong>program (memory)</strong> and consists of two parts:<ul><li>The <strong>right most</strong>  <strong>bits</strong> that represent the <strong>offset within the page (frame)</strong><ul><li>e.g. 12 bits fot the offset, allowing up to 4096 (212) bytes per page (frame)</li></ul></li><li>The <strong>left most</strong>  <strong>bits</strong> that represent the <strong>page (frame) number</strong><ul><li>e.g. 4 bits for the page number allowing 16 (24) pages (frames)</li></ul></li></ul></li><li>The <strong>offset</strong> within the page and frame <strong>remains the same</strong> (they are the same size)</li><li>The page number to frame number mapping is held in the <strong>page table</strong></li><li><img src="/2020/11/10/Memory-Management/Screen%20Shot%202020-12-23%20at%205.13.44%20PM.png?lastModify=1630990878" alt="img"></li></ul></li><li>Relocation: Address Translation<ul><li>Steps in <strong>address translation:</strong><ul><li>1  <strong>Extract the page number</strong> from the logical address</li><li>2  Use page number as an index to <strong>retrieve the frame number</strong> in the page table</li><li>3  <strong>Add the “logical offset within the page”</strong> to the start of the physical frame</li></ul></li><li><strong>Hardware implementation</strong> of address translation<ul><li>1  The CPU’s <strong>memory management unit</strong> (MMU) intercepts logical address</li><li>2  MMU uses a page table as above</li><li>3  The resulting <strong>physical address</strong> is put on the <strong>memory bus</strong></li></ul></li></ul></li></ul><h4 id="7-Virtual-Memory-虚拟内存"><a href="#7-Virtual-Memory-虚拟内存" class="headerlink" title="7. Virtual Memory (虚拟内存)"></a>7. Virtual Memory (虚拟内存)</h4><ul><li>Principle of Locality (局部性原理)<ul><li><strong>Principle of Locality:</strong> the program and data references within a process tend to <strong>cluster</strong><ul><li><strong>localities</strong> constitute <strong>groups of pages</strong> that are <strong>used together</strong>, e.g., related to a function (code, data, etc.)</li><li>Code execution and data manipulation are usually <strong>restricted to a small subset</strong> (i.e. limited number of pages) at any point in time</li><li>I.e. <strong>code</strong> and <strong>data references</strong> within a process are usually <strong>clustered</strong> =&gt; This is called the <strong>principle of locality</strong></li></ul></li><li>程序在执行过程中的一个较短时期，所执行的指令地址和指令的操作数地址，分别局限于一定区域<ul><li>时间局部性<ul><li>一条指令的一次执行和下次执行，一个数据的一次访问和下次访问都集中在一个较短时期内</li><li>即：如果程序中的某条指令一旦执行，则不久之后该指令可能再次被执行；如果某数据被访问，则不久之后该数据可能再次被访问。</li></ul></li><li>空间局部性<ul><li>当前指令和邻近的几条指令，当前访问的数据和邻近的几个数据都集中在一个较小区域内</li><li>即：如果某个位置的信息被访问，那和它相邻的信息也很有可能被访问到。</li></ul></li><li>分支局部性<ul><li>一条跳转指令的两次执行，很可能跳到相同的内存位置</li></ul></li></ul></li><li><strong>Not all pages</strong> have to be <strong>loaded</strong> in memory at the same time =&gt; <strong>virtual memory</strong><ul><li>Loading an entire set of pages for an entire program/data set into memory is <strong>wasteful</strong></li><li>Desired blocks could be <strong>loaded on demand</strong></li></ul></li></ul></li><li>Definition<ul><li>Virtual Memory: A storage allocation scheme in which <strong>secondary memory</strong> can be addressed as though it was part of main memory</li></ul></li><li>Page Faults (缺页)<ul><li>The <strong>resident set (常驻集)</strong> refers to the pages that are loaded in main memory</li><li>A <strong>page fault</strong> is generated if the processor accesses a page that is <strong>not in memory</strong><ul><li>A page fault results in an <strong>interrupt</strong> (process enters <strong>blocked state</strong>)</li><li>An <strong>I/O operation</strong> is started to bring the missing page into main memory</li><li>A <strong>context switch</strong> (may) take place</li><li>An <strong>interrupt signals</strong> that the I/O operation is complete (process enters <strong>ready state</strong>)</li></ul></li></ul></li><li>The Benefits<ul><li>Being able to maintain <strong>more processes</strong> in main memory through the use of virtual memory <strong>improves CPU utilisation</strong><ul><li>Individual processes take up less memory since they are only <strong>partially</strong> loaded</li></ul></li><li>Virtual memory allows the <strong>logical address space</strong> (i.e. processes) to be <strong>larger than physical address space</strong> (i.e. main memory)</li></ul></li><li>基本特征<ul><li>不连续性<ul><li>物理内存分配非连续</li><li>虚拟地址空间使用非连续</li></ul></li><li>大用户空间<ul><li>提供给用户的虚拟内存可大于实际的物理内存</li></ul></li><li>部分交换<ul><li>虚拟存储只对部分虚拟地址空间进行调入和调出</li></ul></li></ul></li><li>Contents of a Page Entry<ul><li>A <strong>“present/absent bit”</strong> that is set if the page/frame is in memory (page fault)</li><li>A <strong>“modified bit”</strong> that is set if the page/frame has been modified (only modified pages have to be written back to disk when evicted) (page usage)</li><li>A <strong>“referenced bit”</strong> that is set if the page is in usage (page usage)</li><li><strong>Protection and sharing bits:</strong> read, write, execute or combinations thereof</li></ul></li><li>Dealing with Large Page Tables<ul><li>Solution: Page the page table!</li><li>We keep tree-like structures to hold page tables</li><li>Divide the page number into<ul><li>An index to a page table of second level</li><li>A page within a second level page table</li></ul></li><li>No need to keep all page tables in memory all time</li></ul></li><li>Multi-level Page Tables<ul><li><img src="/2020/11/10/Memory-Management/Screen%20Shot%202020-12-24%20at%205.05.59%20PM.png?lastModify=1630990878" alt="img"></li><li><img src="file:///Users/apple/Desktop/Y3/OSC/diagrams/Screen%20Shot%202020-12-24%20at%205.06.20%20PM.png?lastModify=1630990878" alt="img"></li></ul></li><li>Access Speed<ul><li><strong>Memory organisation</strong> of multi-level page tables:<ul><li>The <strong>root page table</strong> is always maintained in memory</li><li>Page tables themselves are maintained in <strong>virtual memory</strong> due to their size</li><li>Page table size is proportional to that of the virtual address space</li></ul></li><li>Assume that a <strong>fetch</strong> from main memory (=&gt; memory access) takes T nano seconds<ul><li>With a <strong>single page table level</strong>, access is 2T (1. 访问内存中的页表；2. 访问目标内存单元)</li><li>With <strong>two page table levels</strong>, access is 3T (1. 访问内存中的顶级页表（页目录表）；2. 访问内存中的二级页表；3. 访问目标内存单元 )</li></ul></li></ul></li><li>Translation Look Aside Buffers (TLBs, 简称为快表)<ul><li><strong>Translation look aside buffer</strong> (TLBs) are (usually) located inside the memory management unit<ul><li>They <strong>cache</strong> the most <strong>frequently</strong> used page table entries</li><li>They can be searched <strong>in parallel</strong></li></ul></li><li>The principle behind TLBs is similar to other types of <strong>caching in operating systems</strong></li><li><img src="/2020/11/10/Memory-Management/Screen%20Shot%202020-12-24%20at%205.56.16%20PM.png?lastModify=1630990878" alt="img"></li><li>Memory access with TLBs:<ul><li>Assume a 20ns associative <strong>TLB lookup</strong></li><li>Assume a 100ns <strong>memory access time</strong> and with a single level page table<ul><li><strong>TLB Hit</strong> -&gt; </li><li><strong>TLB Miss</strong> -&gt; </li></ul></li></ul></li><li>Performance evaluation of TLBs:<ul><li>For an 80% hit rate, the estimated access time is:<ul><li> (i.e. 40% slowdown)</li></ul></li><li>For a 98% hit rate, the estimated access time is:<ul><li> (i.e. 22% slowdown)</li></ul></li></ul></li><li>Note that <strong>page tables</strong> can be <strong>held in virtual memory</strong> -&gt; further (initial) slow down due to page faults</li></ul></li><li>Inverted Page Tables (反置页表)<ul><li>A <strong>“normal” page table’s</strong> size is proportional to the number of pages in the virtual address space (drawback to multi-level/single-level page table)</li><li>An <strong>“inverted” page table’s</strong> size is proportional to the size of main memory<ul><li>The inverted table contains one <strong>entry for every frame</strong> (i.e. not for every page)</li><li>A <strong>hash function</strong> based on the page number is used to index the inverted page table</li><li>The inverted table <strong>indexes entries by frame number</strong>, not by page number</li></ul></li><li>The OS maintains a <strong>single inverted page table</strong> for all processes</li><li><img src="/2020/11/10/Memory-Management/Screen%20Shot%202020-12-24%20at%208.56.49%20PM.png?lastModify=1630990878" alt="img"></li><li>Advantages:<ul><li>The OS maintains a <strong>single inverted page table</strong> for all processes</li><li>It saves lots of space (especially when the virtual address space is much larger than the physical memory)</li></ul></li><li>Disadvantages:<ul><li>Virtual-to-physical <strong>translations becomes much harder</strong>. We need to use hash tables to avoid searching the whole inverted table (be aware of potential collisions)</li></ul></li><li>It is used in combination with TLBs to speed up the search.</li></ul></li><li>Demand Paging (按需调页)<ul><li><strong>Demand paging</strong> starts the process with <strong>no pages in memory</strong><ul><li>The first instruction will immediately cause <strong>a page fault</strong></li><li><strong>More page faults</strong> will follow, but they will <strong>stabilise over time</strong> until moving to the <strong>next locality</strong></li><li>The set of pages that is currently being used is called its <strong>working set</strong> (resident set)</li></ul></li><li>Pages are only <strong>loaded when needed</strong>, i.e. following <strong>page faults</strong></li></ul></li><li>Pre-Paging (预调页)<ul><li>When the process is started, all pages <strong>expected</strong> to be used (i.e. the working set) could be <strong>brought into memory at once</strong><ul><li>This can drastically <strong>reduce the page fault rate</strong></li><li>Retrieving multiple (<strong>contiguously stored</strong>) pages <strong>reduces transfer time</strong> (seek time, rotational latency, etc.)</li></ul></li><li><strong>Pre-paging</strong> loads pages (as much as possible) <strong>before page faults are generated</strong> (-&gt; a similar mechanism is used when processes are <strong>swapped out/in</strong>)</li></ul></li><li>Implementation Details<ul><li>Avoid <strong>unnecessary pages</strong> and <strong>page replacement</strong> is important</li><li>Let ma, p, and pft denote the <strong>memory access time</strong> (10-200ns), <strong>page fault rate</strong>, and <strong>page fault time</strong>, the <strong>page access time</strong> is then given by: </li><li>The expected access time is <strong>proportional to page fault rate</strong> when keeping page faults into account</li></ul></li><li>Page Replacement<ul><li>The OS must choose a <strong>page to remove</strong> when a new <strong>one is loaded</strong> (and all are occupied)</li><li><strong>Objective</strong> of replacement: the page that is removed should be the page <strong>least likely</strong> to be referenced in the <strong>near future</strong>. (reduce page fault rate)</li><li>This choice is made by <strong>page replacement algorithms</strong> and <strong>takes into account</strong><ul><li>When the page is <strong>last used/expected to be used</strong> again</li><li>Whether the <strong>page has been modified</strong> (only modified pages need to be written)</li></ul></li><li>Replacement choices have to be <strong>made intelligently</strong> (&lt;=&gt; random) to <strong>save time</strong>/avoid <strong>thrashing</strong></li></ul></li><li>Optimal Page Replacement (最佳置换算法)<ul><li>In an <strong>ideal/optimal</strong> world<ul><li>Each page is labeled with the <strong>number of instructions</strong> that will be executed/length of time before it is <strong>used again</strong></li><li>The page which will be <strong>not referenced</strong> for the <strong>longest time</strong> is the optimal one to remove</li></ul></li><li>The <strong>optimal approach</strong> is <strong>not possible to implement</strong><ul><li>It can be used for <strong>post-execution analysis</strong> -&gt; what would have been the minimum number of page faults</li><li>It provides a <strong>lower bound</strong> on the <strong>number of page faults</strong> (used for comparison with other algorithms)</li></ul></li></ul></li><li>First-In, First-Out (FIFO) (先进先出置换算法)<ul><li>FIFO maintains a <strong>linked list</strong> and <strong>new pages</strong> are added at the end of the list</li><li>The <strong>oldest page</strong> at the <strong>head of the list</strong> is evicted when a page faults occurs</li><li>The <strong>(dis-)advantages</strong> of FIFO include:<ul><li>It is <strong>easy</strong> to understand/implement</li><li>It <strong>performs poorly</strong> =&gt; heavily used pages are just as likely to be evicted as a lightly used pages</li></ul></li></ul></li><li>Second Chance FIFO (第二次机会置换算法)<ul><li>Second chance is a <strong>modification of FIFO</strong>:<ul><li>If a page at the front of the list has <strong>not been referenced</strong> it is <strong>evicted</strong></li><li>If the reference bit is set, the page is <strong>placed at the end</strong> of list and its <strong>reference bit reset</strong></li></ul></li><li>The <strong>(dis-)advantages</strong> of second chance FIFO include:<ul><li>It <strong>works better</strong> than standard FIFO</li><li>The algorithm is <strong>relatively simple</strong>, but it is <strong>costly to implement</strong> because the list is constantly changing (pages have to be added to the end of the list again)</li><li>The algorithm <strong>can degrade to FIFO</strong> if all pages were initially referenced</li></ul></li></ul></li><li>Not Recently Used (NRU) (最近未使用置换算法)<ul><li><strong>Referenced</strong> and <strong>modified</strong> bits are kept in the page table<ul><li>Referenced bits are clear at the start, and <strong>nulled at regular intervals</strong> (e.g. system clock interrupt)</li></ul></li><li>Four different <strong>page “types”</strong> exist<ul><li>class 0: not referenced, not modified</li><li>class 1: not referenced, modified</li><li>class 2: referenced, not modified</li><li>class 3: referenced, modified</li></ul></li><li><strong>Page table entries</strong> are inspected upon every <strong>page fault</strong> -&gt; a page from the <strong>lowest numbered non-empty class</strong> is removed (can be implemented as a clock)</li><li>The NRU algorithm provides a <strong>reasonable performance</strong> and is easy to understand and implement</li></ul></li><li>Least-Recently-Used (最近最久未使用置换算法)<ul><li>Least recently used <strong>evicts the page</strong> that has <strong>not been used the longest</strong><ul><li>The OS must <strong>keep track</strong> of when a page was <strong>last used</strong></li><li>Every <strong>page table entry</strong> contains a <strong>field for the counter</strong></li><li>This is <strong>not cheap</strong> to implement as we need to maintain a <strong>list of pages</strong> which are <strong>sorted</strong> in the order in which they have been used (or search for the page)</li></ul></li><li>The algorithm can be <strong>implemented in hardware</strong> using a <strong>counter</strong> that is incremented after each instruction</li></ul></li><li>Resident Set (常驻集)<ul><li>How many pages should be allocated to individual processes:<ul><li><strong>Small resident sets</strong> enable to store <strong>more processes</strong> in memory -&gt; improve CPU utilisation</li><li><strong>Small resident sets</strong> may result in <strong>more page faults</strong></li><li><strong>Large resident sets</strong> may <strong>no longer reduce</strong> the <strong>page fault rate</strong></li></ul></li><li>A trade-off exists between the <strong>sizes of the resident sets</strong> and <strong>system utilisation</strong></li><li>Resident set sizes may be <strong>fixed</strong> or <strong>variable</strong> (i.e. adjusted at runtime)</li><li>For <strong>variable sized</strong> resident sets, <strong>replacement policies</strong> can be:<ul><li><strong>Local scope:</strong> a page of the same process is replaced</li><li><strong>Global scope:</strong> a page can be taken away from a different process</li></ul></li><li>Variable sized sets require <strong>careful evaluation of their size</strong> when a <strong>local scope</strong> is used (often based on the <strong>working set</strong> or the <strong>page fault frequency</strong>)</li><li><strong>Global replacement policies (global scope)</strong> can select frames from the entire set, i.e., they can be “taken” from other processes<ul><li>Frames are <strong>allocated dynamically</strong> to processes</li><li>Processes cannot control their own page fault frequency, i.e., the <strong>PFF</strong> of one process is <strong>influenced by other processes</strong></li></ul></li><li><strong>Local replacement policies (local scope)</strong> can only select frames that are allocated to the current process<ul><li>Every process has a <strong>fixed fraction of memory</strong></li><li>The locally <strong>“oldest page”</strong> is not necessarily the globally “oldest page”</li></ul></li><li>Windows uses a <strong>variable approach</strong> with <strong>local replacement</strong></li></ul></li><li>Working Sets (工作集/驻留集)<ul><li>The <strong>resident set</strong> comprises the set of pages of the process that are in memory</li><li>The <strong>working set</strong> W(t, k) comprises the set referenced pages in the last k virtual time units for the process at time t</li><li>k can be defined as <strong>memory references</strong><ul><li>The set of most recently used pages</li><li>The set of pages used within a pre-specified time interval</li></ul></li><li>The <strong>working set size</strong> can be used as a guide for the number frames that should be allocated to a process</li><li>The working set is a <strong>function of time</strong> t:<ul><li>Processes <strong>move between localities</strong>, hence, the pages that are included in the working set <strong>change over time</strong></li></ul></li><li>Choosing the right value of k is paramount:<ul><li>Too <strong>small</strong>: inaccurate, pages are missing</li><li>Too <strong>large</strong>: too many unused pages present</li><li><strong>Infinity</strong>: all pages of the process are in the working set</li></ul></li><li>Working sets can be used to guide the <strong>size of the resident sets</strong><ul><li>Monitor the working set</li><li>Remove pages from the resident set that are not in the working set (LRU)</li></ul></li><li>The working set is costly to maintain -&gt; <strong>page fault frequency</strong> can be used as an approximation<ul><li>If the PFF is increased -&gt; we need to increase k</li><li>If the PFF is reduced -&gt; we may try to decrease k</li></ul></li></ul></li><li>Paging Daemon (分页守护进程): Pre-cleaning (demand-cleaning)<ul><li>如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，而且被修改过的话，再换入一个新页面时，旧页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统有一个称为分页守护进程（paging daemon）的后台进程，它在大多数时候睡眠，但定期被唤醒以检查内存的状态。如果空闲页框过少，分页守护进程通过预定的页面置换算法选择页面换出内存。如果这些页面装入内存后被修改过，则将它们写回磁盘。</li><li>It is more efficient to <strong>proactively (主动地)</strong> keep a number of <strong>free pages</strong> for <strong>future page faults</strong><ul><li>If not, we may have to <strong>find a page</strong> to evict and we <strong>write it to the drive</strong> (modified) first when a page fault occurs</li></ul></li><li>Many systems have a background process called a <strong>paging daemon</strong><ul><li>This process <strong>runs at periodic intervals</strong></li><li>It inspect the state of the frames and, if too few pages are free, it <strong>selects pages to evict</strong> (using page replacement algorithms)</li></ul></li><li>Paging daemons can be combined with <strong>buffering</strong> (free and modifies lists) write the modified pages but keep them in main memory when possible</li><li>在任何情况下，页面中原先的内容都被记录下来。当需要使用一个已被淘汰的页面时，如果该页框还没有被覆盖，将其从空闲页框缓冲池中移出即可恢复该页面。保存一定数目的页框供给比使用所有内存并在需要时搜索一个页框有更好的性能。分页守护进程至少保证了所有的空闲页框是“干净”的，所以空闲页框在被分配时不必再急着写回磁盘。</li></ul></li><li>Thrashing (抖动)<ul><li>Assume <strong>all available pages are in active use</strong> and a new page needs to be loaded:<ul><li>The page that will be <strong>evicted</strong> will have to be <strong>reloaded soon afterwards</strong>, i.e., it is still active</li></ul></li><li><strong>Thrashing</strong> occurs when pieces are swapped out and loaded again immediately</li><li>CPU utilisation is too low -&gt; scheduler (medium term scheduler) <strong>increases degree of multi-programming</strong><ul><li>-&gt; Frames are allocated to new processes and <strong>taken away from existing processes</strong><ul><li>-&gt; I/O <strong>repuests are queued</strong> up as a consequence of page faults</li></ul></li></ul></li><li>CPU <strong>utilisation drops further</strong> -&gt; scheduler increases degree of multi-programming</li><li><img src="/2020/11/10/Memory-Management/20200610182224879.png"></li><li><strong>Causes</strong> of thrashing include:<ul><li>The degree of multi-programming is too high, i.e., the total <strong>demand</strong> (i.e., the sum of all <strong>working set</strong> sizes) <strong>exceeds supply</strong> (i.e. the available frames)</li><li>An individual process is allocated <strong>too few pages</strong></li></ul></li><li>This can be <strong>prevented</strong> by, e.g., using good <strong>page replacement policies</strong>, reducing the <strong>degree of multi-programming</strong> (medium term scheduler), or adding more memory</li><li>The <strong>page fault frequency</strong> can be used to detect that a system is thrashing</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deadlock</title>
      <link href="2020/10/20/Deadlock/"/>
      <url>2020/10/20/Deadlock/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>A set of processes is deadlocked if <strong>each process</strong> in the set is waiting for <strong>an event</strong> that only <strong>the other process</strong> in the set can cause<ul><li>Each <strong>deadlocked process</strong> is <strong>waiting for</strong> a resource held by <strong>another deadlocked process</strong> (which cannot run and hence cannot release the resources)</li><li>This can happen between <strong>any number of processes</strong> and for <strong>any number of resources</strong></li></ul></li></ul><h4 id="2-How-do-they-occur"><a href="#2-How-do-they-occur" class="headerlink" title="2. How do they occur?"></a>2. How do they occur?</h4><ul><li>On occasions, <strong>multiple processes</strong> will <strong>require access</strong> to <strong>multiple mutually exclusive resources</strong></li><li>Process A and B request the resources in <strong>opposite orders</strong> and end up in deadlock<ul><li>Deadlocks can occur on the <strong>same machine</strong> or between <strong>mutiple machines</strong> (e.g. resources are requested over the network) and <strong>any number of resources</strong></li></ul></li><li>A resource (e.g. a devide, a data record, file, semaphore) can be <strong>acquired, used,</strong> and <strong>released</strong><ul><li>A resource can be <strong>preemptable</strong>, i.e., it can be forcefully taken away from the process without permanent adverse effect</li><li>A resource can be <strong>non-preemptable</strong>, i.e., it cannot be taken away from a process without permanent adverse effect</li></ul></li><li><strong>Deadlocks only occur</strong> for <strong>non-preemptable resources</strong> since preemptable resources can be temporarily taken away to recover from the deadlock</li><li>If a non-preemptable resource is requested but not available, the <strong>process is made to wait</strong></li></ul><h4 id="3-Minimum-Conditions"><a href="#3-Minimum-Conditions" class="headerlink" title="3. Minimum Conditions"></a>3. Minimum Conditions</h4><ul><li><strong>Four conditions</strong> must hold for deadlocks to occur:<ul><li><strong>Mutual exclusion</strong>: a resource can be assigned to at most one process at a time</li><li><strong>Hold and wait condition</strong>: a resource can be held whilst requesting new resources</li><li><strong>No preemption</strong>: resources cannot be forcefully taken away from a process</li><li><strong>Circular wait</strong>: there is a circular chain of two or more processes, waiting for a resource held by the other processes</li></ul></li><li><strong>No deadlocks</strong> can occur if one of the conditions is <strong>not satisfied</strong></li></ul><h4 id="4-Detecting-Deadlocks-A-Graph-Based-Approach"><a href="#4-Detecting-Deadlocks-A-Graph-Based-Approach" class="headerlink" title="4. Detecting Deadlocks (A Graph Based Approach)"></a>4. Detecting Deadlocks (A Graph Based Approach)</h4><ul><li>Deadlocks can be modeled using <strong>directed graphs</strong>:<ul><li><strong>Resources</strong> are represented by <strong>squares</strong> and <strong>processes</strong> are represented by <strong>circles</strong></li><li>A directed arc from a <strong>square</strong> (resource) and a <strong>circle</strong> (process) means that the resource was <strong>requested and granted</strong>, i.e. is allocated to the process</li><li>A directed arc from a <strong>circle</strong> (process) to a <strong>square</strong> (resource) indecates that the process has requested the resource and is <strong>waiting to obtain it</strong></li></ul></li><li>A <strong>cycle</strong> in the graph means that a <strong>deadlock</strong> occurs for the respective resources and processes</li></ul><h4 id="5-Matrix-approach-detection"><a href="#5-Matrix-approach-detection" class="headerlink" title="5. Matrix approach (detection)"></a>5. Matrix approach (detection)</h4><ul><li>A <strong>matrix based algorithm</strong> is used when <strong>multiple “copies” of</strong> the same <strong>resource</strong> exist</li><li>Let $P_1$ to $P_n$ denote $n$ <strong>processes</strong></li><li>Let there exist $m$ <strong>resources</strong> classes<ul><li>Let $E_i$ denote the “existing” resources of type $i$</li><li>Let $A_i$ denote the number of allocated and available resources ($A_i$) of type $i$ is equal to $E_i$</li></ul></li><li>The algorithm uses <strong>two vectors</strong> describing the existing and available resources ($E$ and $A$), and <strong>two matrices</strong> to describe the <strong>current</strong> and <strong>requested</strong> resources per process ($C$ and $R$)</li><li>Graph Approach (cycle detection algorithm)<ul><li>Select a process for which the requested resource allocation can be satisfied<ul><li>$R_{ij}\leq A_i$ for all j in [1..m]</li><li>Run the process and ‘mark’ it as completed</li></ul></li><li>Reclaim the process’s resources by adding row $C_i$ to $A_i$</li><li>Repeat the above and terminate the algorithm if no ‘unmarked’ or ‘completable’ process can be found</li></ul></li></ul><h4 id="6-Recover-from-Deadlocks"><a href="#6-Recover-from-Deadlocks" class="headerlink" title="6. Recover from Deadlocks"></a>6. Recover from Deadlocks</h4><ul><li><strong>Preemption (抢占)</strong>: the resource is forcefully removed from one of the processes (this is likely to have an ill effect)<ul><li>Take a resource away from a process, have another process use it, and then give it back. (difficult/impossible)</li></ul></li><li><strong>Rollback mechanisms (回滚)</strong>: build in check points that allow the process to be restarted (periodically)<ul><li>The check points contain the <strong>memory image</strong> and <strong>resource states</strong></li><li>Multiple checkpoints should be maintained (not overwrite)</li><li>The process that owns the “deadlocked” resources is rolled back</li></ul></li><li><strong>Kill</strong> a strategically chosen <strong>process</strong> to release its resources<ul><li>The process should be easy to restart</li><li>The process can be re-run from the beginning with no ill effect</li></ul></li></ul><h4 id="7-Avoidance"><a href="#7-Avoidance" class="headerlink" title="7. Avoidance"></a>7. Avoidance</h4><ul><li>Deadlock detection approaches are <strong>reactive</strong>, i.e., they only detect and recover from deadlocks, but do not <strong>prevent</strong> them</li><li><strong>Deadlocks can be avoided</strong> by carefully considering resource allocation</li><li>A <strong>safe state</strong> is one where there is <strong>a feasible order</strong> for the processes to finish <strong>without deadlock</strong></li><li>A state might <strong>not</strong> be <strong>in deadlocked</strong>, but <strong>still</strong> be <strong>unsafe</strong></li><li>The “<strong>bankers algorithm</strong>“ (Dijkstra) can be used to avoid deadlocks<ul><li>Find customer closet to max resource allocation</li><li>Check whether remaining resources can satisfy the maximum request</li><li>Return the resources from this process to the set of available resources</li><li>Find next process closest to its max resource allocation</li></ul></li><li>The state is safe <em>iff</em> all processes can finish, even when they request all their resources at the same time</li><li>The bankers algorithm for <strong>multiple resources</strong> is a genrealisation of the bankers algorithm for a single resource<ul><li><strong>Two matrices</strong> are maintained, one for the allocated resources $C$, one for the remaining resources that are required $R$</li><li><strong>Three vectors</strong> are maintained: the existing resources, the possessed resources, the available resources</li><li>Find a row in $R$ for which all corresponding resource requirements are less or equal to $A$ (selection order has no influence)<ul><li>If no row satisfies this, the state is <strong>unsafe</strong></li><li>else ‘run the process’ and add its resources to $A$</li></ul></li><li>Repeat the above until all processes have finished<ul><li>The state is <strong>safe</strong> if <strong>all processes</strong> complete successfully</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Concurrency</title>
      <link href="2020/10/16/Concurrency/"/>
      <url>2020/10/16/Concurrency/</url>
      
        <content type="html"><![CDATA[<h5 id="竞争条件-race-conditions-多个线程或者进程在读写一个共享数据时结果依赖于它们执行的相对时间的情形。"><a href="#竞争条件-race-conditions-多个线程或者进程在读写一个共享数据时结果依赖于它们执行的相对时间的情形。" class="headerlink" title="竞争条件(race conditions): 多个线程或者进程在读写一个共享数据时结果依赖于它们执行的相对时间的情形。"></a>竞争条件(race conditions): 多个线程或者进程在读写一个共享数据时结果依赖于它们执行的相对时间的情形。</h5><ul><li>A <strong>race condition occurs</strong> when multiple threads/processes <strong>access shared data</strong> and the result is dependent on <strong>the order in which the instructions are interleaved</strong></li></ul><h5 id="防止竞争条件-确保一次只有一个进程可以操作变量，即进程需要进行同步。"><a href="#防止竞争条件-确保一次只有一个进程可以操作变量，即进程需要进行同步。" class="headerlink" title="防止竞争条件: 确保一次只有一个进程可以操作变量，即进程需要进行同步。"></a>防止竞争条件: 确保一次只有一个进程可以操作变量，即进程需要进行同步。</h5><h4 id="1-临界区-critical-section-critical-region"><a href="#1-临界区-critical-section-critical-region" class="headerlink" title="1. 临界区(critical section/critical region):"></a>1. 临界区(critical section/critical region):</h4><p>当一个进程在临界区内时，其他进程不允许进入临界区执行。</p><h5 id="临界区问题的解决方案需满足三个要求："><a href="#临界区问题的解决方案需满足三个要求：" class="headerlink" title="临界区问题的解决方案需满足三个要求："></a>临界区问题的解决方案需满足三个要求：</h5><ol><li>互斥访问(Mutual exclusion)：如果一个进程在临界区执行，其他进程不能进入临界区<ul><li>only one process can be in its critical section at any one point in time</li></ul></li><li>空闲让进/进步(Progress)：如果没有进程在临界区，但有些进程需要进入临界区，不能无限期地延长下一个要进入临界区的等待时间。即不在临界区的进程不能阻止另一个进程进入临界区。<ul><li>any process must be able to enter its critical section at some point in time</li></ul></li><li>有限等待(Bounded waiting)：当一个进程提出要进入临界区请求后，只需要等待临界区被使用有上限的次数后，该进程就可以进入临界区。即进程不应该饿死在临界区入口处。<ul><li>processes cannot be made to wait indefinitely</li></ul></li></ol><h4 id="2-禁止中断-disabling-interrupts"><a href="#2-禁止中断-disabling-interrupts" class="headerlink" title="2. 禁止中断(disabling interrupts)"></a>2. 禁止中断(disabling interrupts)</h4><ol><li>在单处理机中并发进程不能重叠执行，它们只能被插入，而且进程将继续执行直到它调用操作系统服务或被中断，所以，为保证互斥，禁止进程被中断就已足够。</li><li>因为临界点不能被中断，互斥就能得到保证。</li><li>缺点：<ul><li>代价高，执行效率降低，因为处理机受到不能插入的限制。</li><li>不适用于多处理机系统。</li></ul></li></ol><h4 id="3-忙等待-busy-waiting"><a href="#3-忙等待-busy-waiting" class="headerlink" title="3. 忙等待(busy waiting)"></a>3. 忙等待(busy waiting)</h4><ol><li>持续测试某个变量直到该变量变为特定值。</li><li>缺点：<ul><li>浪费CPU时间。</li></ul></li></ol><h4 id="4-自旋锁-spin-lock"><a href="#4-自旋锁-spin-lock" class="headerlink" title="4. 自旋锁(spin lock)"></a>4. 自旋锁(spin lock)</h4><ol><li>利用了忙等待（即自旋）的锁机制称为自旋锁，或者说自旋锁就是忙等待。</li><li>下面提到的利用test_and_set方法和compare_and_swap方法的两个例子以及Peterson算法，都可称为自旋锁。</li><li>缺点：<ul><li>造成死锁</li><li>过多占用cpu资源</li></ul></li><li>优点：<ul><li>防止上下文切换</li><li>较适用于锁使用者保持锁时间比较短的情况。</li></ul></li></ol><h4 id="5-互斥锁-mutex"><a href="#5-互斥锁-mutex" class="headerlink" title="5. 互斥锁(mutex)"></a>5. 互斥锁(mutex)</h4><ol><li><p>互斥锁的实现方式之一就是自旋锁。</p></li><li><p>一个互斥锁就是一个可共享的变量，有两种状态：“锁定”(locked)和“非锁定”(unlocked)</p></li><li><p>两个原子函数来操作互斥锁：</p><ul><li><p>acquire()：在进程进入临界区前调用，将bool值设置为false</p><pre class="line-numbers language-c"><code class="language-c"><span class="token function">acquire</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>  <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token operator">!</span>available<span class="token punctuation">)</span> <span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// busy wait</span>  available <span class="token operator">=</span> false<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>release()：在某进程退出临界区后调用，将bool值设为true</p><pre class="line-numbers language-c"><code class="language-c"><span class="token function">release</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>  available <span class="token operator">=</span> true<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>available</p></li><li><p>这两个函数必须是原子操作</p></li><li><p>这种互斥锁也称为自旋锁，因为调用者会进入忙等待</p></li><li><p>优缺点同自旋锁</p></li></ul></li><li><p>真正的互斥锁与上述不同的是，调用者在锁没释放之前会进入阻塞或睡眠状态，而不是进入忙等待，无限循环地去测试锁是否释放。</p></li></ol><pre class="line-numbers language-c"><code class="language-c"><span class="token function">acquire</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>  <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token operator">!</span>available<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 进入睡眠或阻塞</span>    <span class="token comment" spellcheck="true">// 直到锁释放了再唤醒</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="5"><li><p>必须是同一个进程上锁和解锁（与之后的信号量比较）</p></li><li><p>缺点：</p><ul><li>需要涉及上下文切换，开销比自旋锁大</li></ul></li></ol><h4 id="6-死锁-deadlock"><a href="#6-死锁-deadlock" class="headerlink" title="6. 死锁(deadlock)"></a>6. 死锁(deadlock)</h4><ol><li>是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。</li><li>规范定义：集合中的每一个进程都在等待只能由本集合中的其他进程才能引发的事件，那么该组进程是死锁的。</li><li>必要条件：<ul><li>互斥条件：指一段时间内某资源只由一个进程占用，即一个资源只能被一个进程使用</li><li>请求与保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。</li><li>不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。</li><li>环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源。</li></ul></li><li>预防，解决方法，排除方法（还没讲&#x1F605;）</li></ol><h4 id="7-Peterson算法-软件实现"><a href="#7-Peterson算法-软件实现" class="headerlink" title="7. Peterson算法(软件实现)"></a>7. Peterson算法(软件实现)</h4><ol><li><p>假设LOAD和STORE两个指令都是原子的；也就是说，不能被打断</p></li><li><p>两个进程Pi, Pj共享两个变量：</p></li></ol><ul><li><strong>int</strong> turn: 表明哪个进程是下一个进入临界区的</li><li><strong>boolean</strong> flag[2]: 表明某个进程准备好进入临界区，<strong>flag[i]</strong> = true表示进程Pi已经准备好进入临界区</li></ul><ol start="3"><li><p>进程Pi和Pj的结构</p><pre class="line-numbers language-c"><code class="language-c"><span class="token comment" spellcheck="true">// 进程Pi的结构</span><span class="token keyword">do</span> <span class="token punctuation">{</span>    flag<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> true<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// i wants to enter critical section</span>    turn <span class="token operator">=</span> j<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// allow j to access first</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>flag<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&amp;&amp;</span> turn <span class="token operator">==</span> j<span class="token punctuation">)</span> <span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// busy waiting</span>    <span class="token comment" spellcheck="true">// 临界区</span>    flag<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> false<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 剩余区</span><span class="token punctuation">}</span><span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 进程Pj的结构</span><span class="token keyword">do</span> <span class="token punctuation">{</span>    flag<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> true<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// j wants to enter critical section</span>    turn <span class="token operator">=</span> i<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// allow i to access first</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>flag<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&amp;&amp;</span> turn <span class="token operator">==</span> i<span class="token punctuation">)</span> <span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// busy waiting</span>    <span class="token comment" spellcheck="true">// 临界区</span>    flag<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> false<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 剩余区</span><span class="token punctuation">}</span><span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>该算法满足解决临界区问题的三个必须标准：互斥访问，进入（即不死锁），有限等待（即不饿死）</p><ul><li><p>互斥访问：变量turn只能有一个值</p><ul><li>flag[i]和flag[j]都为true当Pi和Pj都想进入临界区</li><li>turn只能等于i或j其中一个值</li><li>while (flag[i] &amp;&amp; turn == i)和while (flag[j] &amp;&amp; turn == j)中只有一个为true，为true的那个进程进入忙等待(busy waiting)，而另一个进程可以进入临界区</li><li>因此，最多只有一个进程可以进入临界区</li></ul></li><li><p>空闲让进：</p><ul><li><p><strong>情况一</strong>：</p></li><li><p>假设Pj正在临界区，Pi正在忙等待</p></li><li><p>当Pj退出临界区后（即没有进程在临界区），此时flag[j]变为false</p></li><li><p>while (flag[j] &amp;&amp; turn == j) 将会停止Pi的忙等待，此时Pi就可以进入临界区</p></li><li><p><strong>情况二</strong>：</p></li><li><p>假设Pi和Pj都想进入临界区，即flag[i] = flag[j] = true</p></li><li><p>turn等于i或j =&gt; 假设turn = i</p></li><li><p>while (flag[j] &amp;&amp; turn == j)终止，Pi进入临界区，Pj进入忙等待</p></li><li><p>Pi退出临界区，flag[i] = false</p></li><li><p>while (flag[i] &amp;&amp; turn == i)终止，Pj进入临界区</p></li></ul></li><li><p>有限等待：Peterson算法显然让进程等待不超过1次的临界区使用，即可获得权限进入临界区。</p><ul><li>由上述情况可知，一个进程最多在另一个进程进入临界区一次后就能进入</li></ul></li></ul></li><li><p>分析（“谦让式”）</p><ul><li>首先，如果是进程i第一次开始执行，那它可以顺利进入临界区，因为flag[j] = false，进程j还不想进入临界区。</li><li>其次，如果进程i和进程j已经在并发执行了，它们的调度顺序是未知的，假设每个进程每次执行一行代码，交替执行。那先执行的进程就“赚了”，比如进程i先执行，那么它会先将turn“谦让”地设置为j，但接下来轮到进程j执行了，它也“谦让”地将turn设置为i。这时又轮到了进程i执行了，而且我们可以发现，while中第二个条件已经不满足了！这时进程i就进入了临界区！然后我们把情况一般化，不再假设每个进程交替地执行一行代码，只要一个进程后执行了turn = i;(或turn = j;)这条语句，那么另一个进程就可以进入临界区。（分析的时候重点关注一点：另一个进程到底想不想进入临界区？）</li></ul></li></ol><h4 id="8-硬件同步方法"><a href="#8-硬件同步方法" class="headerlink" title="8. 硬件同步方法"></a>8. 硬件同步方法</h4><ol><li><p>test_and_set()</p><pre class="line-numbers language-c"><code class="language-c"><span class="token comment" spellcheck="true">// test and set method</span>bool <span class="token function">test_and_set</span><span class="token punctuation">(</span>boolean<span class="token operator">*</span> lock<span class="token punctuation">)</span> <span class="token punctuation">{</span>  bool rv <span class="token operator">=</span> <span class="token operator">*</span>lock<span class="token punctuation">;</span>  <span class="token operator">*</span>lock <span class="token operator">=</span> true<span class="token punctuation">;</span>  <span class="token keyword">return</span> rv<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// Example of using test and set method</span><span class="token keyword">do</span> <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// while the lock is in use (i.e. true)</span>  <span class="token comment" spellcheck="true">// apply busy waiting</span>  <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">test_and_set</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>lock<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// lock from false to true, the loop terminates</span>    <span class="token comment" spellcheck="true">//critical section</span>  lock <span class="token operator">=</span> false<span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">//remainder section</span><span class="token punctuation">}</span><span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>compare_and_swap()</p><pre class="line-numbers language-c"><code class="language-c"><span class="token comment" spellcheck="true">// compare and swap method (version1)</span><span class="token comment" spellcheck="true">// 总是返回旧值(expected)，可在cas操作之后对其进行测试，以查看是否匹配旧值</span><span class="token comment" spellcheck="true">// 其他version返回bool值，来判断是否成功更新</span><span class="token keyword">int</span> <span class="token function">compare_and_swap</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>lock<span class="token punctuation">,</span> <span class="token keyword">int</span> expected<span class="token punctuation">,</span> <span class="token keyword">int</span> new_value<span class="token punctuation">)</span> <span class="token punctuation">{</span>  <span class="token keyword">int</span> temp <span class="token operator">=</span> <span class="token operator">*</span>lock<span class="token punctuation">;</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">*</span>lock <span class="token operator">==</span> expected<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token operator">*</span>lock <span class="token operator">=</span> new_value<span class="token punctuation">;</span>  <span class="token punctuation">}</span>  <span class="token keyword">return</span> temp<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// Example of using compare and swap method</span><span class="token keyword">do</span> <span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// while the lock is in use (i.e. true or 1), applying busy waiting</span>  <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">compare_and_swap</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>lock<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">//lock from false to true or from 0 to 1, the loop terminates</span>    <span class="token comment" spellcheck="true">//critical section</span>  lock <span class="token operator">=</span> false<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// lock = 0</span>  <span class="token comment" spellcheck="true">//remainder section</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>上述两种方法都是由硬件保证同步的，即硬件保证这三条语句必须原子执行，中间不发生任何中断。如果中断，则会有竞争条件发生。</p></li><li><p>如果上述两种方法被同时调用，则按顺序执行。</p></li><li><p>缺点：</p><ul><li>利用了忙等待(busy waiting)</li><li>可能造成死锁</li></ul></li></ol><h4 id="9-信号量-Semaphores"><a href="#9-信号量-Semaphores" class="headerlink" title="9. 信号量(Semaphores)"></a>9. 信号量(Semaphores)</h4><ol><li><p>实现由操作系统提供</p></li><li><p>相比互斥量只能取0和1两个值，信号量可以为0-N，用来实现更加复杂的同步，互斥量可看作是信号量取只取0和1时的特殊情况</p></li><li><p>信号量通过一个计数器控制对共享资源的访问，信号量的值是一个非负整数，所有通过它的线程都会将该整数减一。如果计数器大于0，则访问被允许，计数器减1；如果为0，则访问被禁止，所有试图通过它的线程都将处于等待状态。</p></li><li><p>信号量定义：</p><pre class="line-numbers language-c"><code class="language-c"><span class="token keyword">typedef</span> <span class="token keyword">struct</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> value<span class="token punctuation">;</span>    <span class="token keyword">struct</span> process <span class="token operator">*</span> list<span class="token punctuation">;</span><span class="token punctuation">}</span> semaphore<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>两个原子函数用来操作信号量</p><ul><li><p>wait()被调用当一个资源被需要时</p><pre><code>wait(semaphore * S) &#123;    S -&gt; value--;    if (S -&gt; value &lt; 0) &#123;        add process to S -&gt; list        block(); // system call    &#125;&#125;</code></pre></li><li><p>signal()/post()被调用当一个资源释放</p><pre><code>signal(semaphore * S) &#123;    S -&gt; value++;    if (S -&gt; value &lt;= 0) &#123;        remove a process P from S -&gt; list        wakeup(P); // system call    &#125;&#125;</code></pre></li></ul></li><li><p>调用wait()：当计数器小于0时，会阻塞进程</p><ul><li>进程进入<strong>阻塞队列</strong></li><li>进程状态由<strong>运行</strong>变为<strong>阻塞</strong></li><li>进程控制交由<strong>进程调度器</strong></li></ul></li><li><p>调用signal()/post()：当计数器小于等于0时，从阻塞队列中移除进程</p><ul><li>进程状态由<strong>阻塞</strong>变为<strong>就绪</strong></li></ul></li><li><p>负信号值代表有几个进程正在等资源</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thread</title>
      <link href="2020/10/14/Thread/"/>
      <url>2020/10/14/Thread/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Thread-Usage"><a href="#1-Thread-Usage" class="headerlink" title="1. Thread Usage"></a>1. Thread Usage</h4><ul><li>Thread can be viewed as <strong>miniprocesses</strong> within a process</li><li>Why do we need multiple threads in a process?<ul><li>Multiple <strong>related activities</strong> apply to the <strong>same resources</strong>, these resources should be accessible/shared (share address space and data)</li><li>Easy to create and destroy.<ul><li>10 - 100 times faster than creating a process</li></ul></li></ul></li></ul><h4 id="2-User-Threads-Many-to-One"><a href="#2-User-Threads-Many-to-One" class="headerlink" title="2. User Threads (Many-to-One)"></a>2. User Threads (Many-to-One)</h4><ul><li><p><strong>Thread management</strong> (creating, destroying, scheduling, thread control block manipulation) is carried out <strong>in user space</strong> with the help of a <strong>user library</strong></p></li><li><p>The process maintains a <strong>thread table</strong> managed by the <strong>runtime system</strong> without the <strong>kernel’s knowledge</strong></p><ul><li>Similar to <strong>process table</strong></li><li>Used for <strong>thread switching</strong></li><li>Tracks thread related information</li></ul></li><li><p>They can be implemented on <strong>OS</strong> that <strong>does not support multithreading</strong></p></li><li><img src="/2020/10/14/Thread/Screen Shot 2020-12-21 at 9.15.22 PM.png" style="zoom:50%;"></li><li><p>Advantages:</p><ul><li>Threads are in user space (i.e., <strong>no mode switches</strong> required)</li><li><strong>Full control</strong> over the thread scheduler (e.g. website server)</li><li><strong>OS independent</strong> (threads can run on OS that do not support them)</li><li>The runtime system can <strong>switch local blocking threads</strong> in user space (e.g. wait for another thread to complete)</li></ul></li><li><p>Disadvantages:</p><ul><li><strong>Blocking system calls</strong> suspend the entire process (user threads are mapped onto a single process, managed by the kernel)</li><li><strong>Page fault</strong> result in blocking the process</li><li><strong>No true parallelism</strong> (a process is scheduled on a single CPU)\</li></ul></li></ul><h4 id="3-Kernel-Threads-One-to-One"><a href="#3-Kernel-Threads-One-to-One" class="headerlink" title="3. Kernel Threads (One-to-One)"></a>3. Kernel Threads (One-to-One)</h4><ul><li><img src="/2020/10/14/Thread/Screen Shot 2020-12-21 at 9.41.55 PM.png" style="zoom:50%;"></li><li>The <strong>kernel manages</strong> the threads, user application accesses threading facilities through <strong>API</strong> and <strong>System calls</strong><ul><li><strong>Thread table</strong> is in the kernel, containing thread control blocks (subset of process control blocks)</li><li>If a <strong>thread blocks</strong>, the kernel chooses thread from same or different process</li></ul></li><li>Advantages:<ul><li><strong>True parallelism</strong> can be achieved</li><li>No run-time system needed</li></ul></li><li>Frequent <strong>mode switches</strong> take place, resulting in lower performance</li></ul><h4 id="4-Hybrid-Implementation"><a href="#4-Hybrid-Implementation" class="headerlink" title="4. Hybrid Implementation"></a>4. Hybrid Implementation</h4><ul><li>User threads are <strong>multiplexed</strong> onto kernel threads</li><li>Kernel sees and schedules the kernel threads (a limited number)</li><li>User application sees user threads and creates/schedules these (an “unrestricted” number)</li><li><img src="/2020/10/14/Thread/Screen Shot 2020-12-22 at 2.14.12 PM.png" style="zoom:50%;"></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Process Scheduling</title>
      <link href="2020/10/13/Process-scheduling/"/>
      <url>2020/10/13/Process-scheduling/</url>
      
        <content type="html"><![CDATA[<h4 id="1-CPU密集型进程"><a href="#1-CPU密集型进程" class="headerlink" title="1. CPU密集型进程"></a>1. CPU密集型进程</h4><ol><li>CPU密集型也叫计算密集型，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading很高。</li><li>在多重程序系统中，大部份时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部份时间用在三角函数和开根号的计算，便是属于CPU bound的程序。</li><li>CPU bound的程序一般而言CPU占用率相当高。这可能是因为任务本身不太需要访问I/O设备，也可能是因为程序是多线程实现因此屏蔽掉了等待I/O的时间。</li></ol><h4 id="2-IO密集型进程"><a href="#2-IO密集型进程" class="headerlink" title="2. IO密集型进程"></a>2. IO密集型进程</h4><ol><li>IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。</li><li>I/O bound的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而pipeline做得不是很好，没有充分利用处理器能力。</li></ol><h4 id="3-CPU密集型-vs-IO密集型"><a href="#3-CPU密集型-vs-IO密集型" class="headerlink" title="3. CPU密集型 vs IO密集型"></a>3. CPU密集型 vs IO密集型</h4><ol><li><p>计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，<strong>所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。</strong></p><p>计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。</p></li><li><p>第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。</p><p>IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。</p></li></ol><h4 id="4-进程调度"><a href="#4-进程调度" class="headerlink" title="4. 进程调度"></a>4. 进程调度</h4><ol><li><strong>Classification by Time Horizon</strong></li></ol><ul><li><strong>Long term</strong>: applies to new processes and controls the <strong>degree of multiprogramming</strong> by deciding which processes to admit to the system when a good <strong>mix</strong> of <strong>CPU</strong> and <strong>I/O bound processes</strong> is favorable to keep all resources as busy as possible.</li><li><strong>Medium term</strong>: controls <strong>swapping</strong> and the <strong>degree of multi-programming</strong> (Memory management).</li><li><strong>Short term (dispatcher)</strong>: decide which process to run next.</li></ul><ol start="2"><li><strong>Classification by Approach</strong></li></ol><ul><li><strong>Non-preemptive(非抢占式或非剥夺式)</strong>: processes are only interrupted voluntarily</li><li><strong>Preemptive(抢占式或剥夺式)</strong>: processes can be <strong>interrupted forcefully</strong> or <strong>voluntarily</strong><ul><li>E.g. preemptive scheduling algorithm picks a process and lets it run for a maximum of some fixed time. (clock interrupt)</li><li>This requires context switches, which generates <strong>overhead</strong></li><li>Prevents processes from <strong>monopolizing(垄断，独占) the CPU</strong></li><li><strong>Most popular</strong> modern operation systems are preemptive</li></ul></li></ul><h4 id="5-性能评估"><a href="#5-性能评估" class="headerlink" title="5. 性能评估"></a>5. 性能评估</h4><ol><li><strong>User oriented criteria</strong>: </li></ol><ul><li><p><strong>Response time(响应时间)</strong>: minimize the time between creating the job and its first</p><p>execution</p></li><li><p><strong>Turnaround time(周转时间)</strong>: minimize the time between creating the job and finishing it</p></li></ul><ol start="2"><li><strong>System oriented criteria</strong>:</li></ol><ul><li><strong>Throughput(吞吐量)</strong>: maximize the number of jobs processed per hour</li><li><strong>Fairness(公平)</strong>: <ul><li>Are processing power/waiting time equally distributed?</li><li>Is there any process with excessively long waiting time? (<strong>starvation</strong>)</li></ul></li></ul><h4 id="6-调度算法"><a href="#6-调度算法" class="headerlink" title="6. 调度算法"></a>6. 调度算法</h4><ol><li>Overview:</li></ol><ul><li><strong>Algorithms</strong>:<ul><li>First Come First Served (FCFS) / First In First Out (FIFO) (<strong>Batch System</strong>)</li><li>Shortest job first (<strong>Batch System</strong>)</li><li>Round Robin (<strong>Interactive System</strong>)</li><li>Priority Queue (<strong>Interactive System</strong>)</li></ul></li><li>Performance measured used:<ul><li><strong>Average response time(平均响应时间)</strong>: the average of the time taken for all the processes to start</li><li><strong>Average turnaround time(平均周转时间)</strong>: the average time taken for all the processes to finish</li></ul></li></ul><ol start="2"><li><strong>First Come First Served</strong>:</li></ol><ul><li>Concept: a <strong>non-preemptive algorithm</strong> that operates as a <strong>strict queueing mechanism</strong> and schedules the processes in the same order that they were added to the queue</li><li>Advantages:<ul><li><strong>positional fairness</strong> and easy to implement</li></ul></li><li>Disadvantages:<ul><li><strong>Favours long processes</strong> over short ones (think of the supermarket checkout!)</li><li>Could <strong>compromise resource utilisation</strong>, i.e., CPU vs. I/O devices</li></ul></li></ul><ol start="3"><li><strong>Shortest Job First:</strong></li></ol><ul><li>Concept: A <strong>non-preemptive algorithm</strong> that starts processes in order of <strong>ascending processing time</strong> using a provided/known estimate of the processing</li><li>Advantages: results in the <strong>optimal turn around time</strong></li><li>Disadvantages:<ul><li><strong>Starvation</strong> might occur</li><li><strong>Fairness</strong> is compromised</li><li><strong>Processing times have to be known</strong> beforehand or estimated by</li></ul></li></ul><ol start="4"><li><strong>Round Robin:</strong></li></ol><ul><li>Concept: a <strong>preemptive version of FCFS</strong> that forces <strong>context switches</strong> at <strong>periodic intervals</strong> or <strong>time slices (Time quantum)</strong><ul><li>Processes run in the order that they were added to the queue</li><li>Processes are forcefully <strong>interrupted by the timer</strong></li></ul></li><li>Advantages:<ul><li>Improved <strong>response time</strong></li><li>Effective for general purpose <strong>time sharing systems</strong></li></ul></li><li>Disadvantages:<ul><li>Increased <strong>context switching</strong> and thus overhead</li><li>Can <strong>reduce to FCFS</strong> (只要时间片足够长，就是FCFS算法)</li></ul></li></ul><ol start="5"><li><strong>Priority Queues:</strong></li></ol><ul><li>Concept: A <strong>preemptive algorithm</strong> that schedules processes by priority (high to low)<ul><li>The process priority is saved in the <strong>process control block</strong></li></ul></li><li>Advantages:<ul><li>can <strong>prioritise I/O bound jobs</strong></li></ul></li><li>Disadvantages:<ul><li>low priority processes may suffer from <strong>starvation</strong> (with static priorities)</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Process</title>
      <link href="2020/10/10/Process/"/>
      <url>2020/10/10/Process/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h4><ul><li>The simplified definition: “a process is a <strong>running instance</strong> of a program”<ul><li>A program is <strong>passive</strong> entity and “sits” on a disk, not doing anything.</li><li>A process has <strong>control structures</strong> associated with it, may be <strong>active</strong>, and may have <strong>resources</strong> assigned to it (e.g. I/O devices, memory, processor). It has a program, input, output and a state.</li><li>A single <strong>processor</strong> may be shared among several <strong>processes</strong>.</li></ul></li><li>A process is registered with the OS using its <strong>“control structures”</strong>: i.e. an entry in the OS’s <strong>process table</strong> to a <strong>process control blocks</strong> (PCB)</li><li>The <strong>process control block</strong> contains all information necessary to <strong>administer the process</strong> and is <strong>essential</strong> for <strong>context switching</strong> in <strong>multiprogramming systems</strong></li></ul><h4 id="2-Menory-Image-of-Processes"><a href="#2-Menory-Image-of-Processes" class="headerlink" title="2. Menory Image of Processes"></a>2. Menory Image of Processes</h4><ul><li>A <strong>process’ memory image</strong> contains:<ul><li>The program <strong>code</strong> (could be shared between multiple processes running the same code)</li><li>A <strong>data</strong> segment: process-specified data (input and output, global variable)</li><li>call <strong>stack</strong>: keep track of active subroutines (function parameters, local variables)</li><li><strong>Heap</strong>: hold intermediate computation data generated during run time</li></ul></li><li>Every process has its own <strong>logical address space</strong>, in which the <strong>stack</strong> and <strong>heap</strong> are placed at <strong>opposite sides</strong> to allow them to grow</li></ul><h4 id="3-Process-States-and-Transitions"><a href="#3-Process-States-and-Transitions" class="headerlink" title="3. Process States and Transitions"></a>3. Process States and Transitions</h4><ul><li><p>Diagram</p><img src="/2020/10/10/Process/Screen Shot 2020-12-21 at 1.50.28 PM.png" style="zoom:50%;"></li><li><p>State transitions include:</p><ul><li>1  <strong>New -&gt; ready</strong>: admit the process and commit to execution</li><li>2  <strong>Running -&gt; blocked</strong>: e.g. process is waiting for input or carried out a system call</li><li>3  <strong>Ready -&gt; running</strong>: the process is selected by the <strong>process sceduler</strong></li><li>4  <strong>Blocked -&gt; ready</strong>: event happens, e.g. I/O operation has finished</li><li>5  <strong>Running -&gt; ready</strong>: the process is preempted, e.g., by a <strong>timer interrupt</strong> or by <strong>pause</strong></li><li>6  <strong>Running -&gt; exit</strong>: process has finished, e.g. program ended or exception encountered</li></ul></li></ul><h4 id="4-Context-Switching-Multiprogramming"><a href="#4-Context-Switching-Multiprogramming" class="headerlink" title="4. Context Switching (Multiprogramming)"></a>4. Context Switching (Multiprogramming)</h4><ul><li><p>Modern computers are <strong>multiprogramming</strong> systems:</p></li><li><p>Assuming a <strong>single processor system</strong>, the instructions of individual processes are executed <strong>sequentially</strong></p><ul><li>CPU’s rapid switching back and forth from process to process is called <strong>multiprogramming</strong>.</li><li>Multiprogramming is achieved by <strong>alternating</strong> processes and <strong>context switching</strong></li><li><strong>True parallelism</strong> requires <strong>mutiple processors</strong></li></ul></li><li><p>When a <strong>context switch</strong> takes place, the system <strong>saves the state</strong> of the old process and <strong>loads the state</strong> of the new process (created <strong>overhead</strong>)</p><ul><li><strong>Saved</strong> =&gt; the process control block is <strong>updated</strong></li><li><strong>(Re-)started</strong> =&gt; the process control block <strong>read</strong></li></ul></li><li><p>A <strong>trade-off</strong> exists between <strong>“responsiveness”</strong> and <strong>“overhead”</strong></p><ul><li><strong>Short time slices</strong> result in <strong>good response time</strong> but <strong>low effective “utilisation”</strong></li><li><strong>Long time slices</strong> result in <strong>poor response time</strong> but <strong>better effective “utilisation”</strong></li></ul></li><li><p>The OS uses <strong>process control block</strong> and a <strong>process table</strong> to manage processes and maintain their information</p></li><li><p>A <strong>process control block</strong> contains three types of <strong>attributes</strong>:</p><ul><li><strong>Process identification</strong> (PID, UID, Parent PID)</li><li><strong>Process state information</strong> (user registers, program counters, stack pointer, program status word, memory management information, files, etc.)</li><li><strong>Process control information</strong> (process state, scheduling information, etc.)</li></ul></li><li><p><strong>Process control blocks</strong> are <strong>kernel data structures</strong>, i.e. they are <strong>protected</strong> and only accessible in <strong>kernel mode!</strong></p><ul><li>Allowing user applications to access them directly could <strong>compromise their integrity</strong></li><li>The <strong>operating system manages</strong> them on the user’s behalf through <strong>system calls</strong></li></ul></li><li><p>Switching Processes</p><ul><li><img src="/2020/10/10/Process/Screen Shot 2020-12-21 at 3.42.30 PM.png" style="zoom:50%;"></li><li><p>1  Save process state (program counter, registers)</p></li><li><p>2  Update PCB (running -&gt; ready)</p></li><li><p>3  Move PCB to appropriate queue (ready/blocked)</p></li><li><p>4  Run scheduler, select new process</p></li><li><p>5  Update to running state in PCB</p></li><li><p>6  Update memory structures</p></li><li><p>7  Restore process</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> class notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operating System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树有关算法</title>
      <link href="2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/"/>
      <url>2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h4 id="1-二叉树定义-java"><a href="#1-二叉树定义-java" class="headerlink" title="1. 二叉树定义(java)"></a>1. 二叉树定义(java)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">// Definition for a binary tree node.</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TreeNode</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> val<span class="token punctuation">;</span>  TreeNode left<span class="token punctuation">;</span>  TreeNode right<span class="token punctuation">;</span>  <span class="token function">TreeNode</span><span class="token punctuation">(</span><span class="token keyword">int</span> x<span class="token punctuation">)</span> <span class="token punctuation">{</span> val <span class="token operator">=</span> x<span class="token punctuation">;</span> <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-二叉树翻转"><a href="#2-二叉树翻转" class="headerlink" title="2. 二叉树翻转"></a>2. 二叉树翻转</h4><img src="/2020/09/16/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%89%E5%85%B3%E7%AE%97%E6%B3%95/截屏2020-09-16 下午4.04.19.png" style="zoom:50%;"><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//简单递归即可实现</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> TreeNode <span class="token function">invertTree</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>root <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token keyword">return</span> null<span class="token punctuation">;</span>    TreeNode left <span class="token operator">=</span> <span class="token function">invertTree</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>left<span class="token punctuation">)</span><span class="token punctuation">;</span>    TreeNode right <span class="token operator">=</span> <span class="token function">invertTree</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>right<span class="token punctuation">)</span><span class="token punctuation">;</span>    root<span class="token punctuation">.</span>right <span class="token operator">=</span> left<span class="token punctuation">;</span>    root<span class="token punctuation">.</span>left <span class="token operator">=</span> right<span class="token punctuation">;</span>    <span class="token keyword">return</span> root<span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 当然还可以用四种遍历来解决这道题</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 未完待续 </tag>
            
            <tag> Java </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序方法汇总</title>
      <link href="2020/09/15/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/"/>
      <url>2020/09/15/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h4 id="1-冒泡排序"><a href="#1-冒泡排序" class="headerlink" title="1. 冒泡排序"></a>1. 冒泡排序</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 整数数组升序</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> i<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>          nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>          nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环，通过依次比较相邻元素，若前一个大于后一个则交换位置，循环结束后，数组内最大值将位于数组最后一个位置；第二次循环，也依次比较相邻元素，除了最后一对（因为最后一个是最大值，前一个肯定比后一个小）,循环结束数组内倒数第二大的值将位于数组倒数第二个位置；……直到没有一对数字需要比较，此时数组以升序排列。</p><p>冒泡排序的时间复杂度为O(n<sup>2</sup>)，空间复杂度为O(1)，计算方法请参见“<a href="https://scycy2.github.io/2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/">算法复杂度</a>”一文。</p><h4 id="2-选择排序"><a href="#2-选择排序" class="headerlink" title="2. 选择排序"></a>2. 选择排序</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">SelectionSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> minIndex <span class="token operator">=</span> i<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 记录最小值的下标</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&lt;</span> nums<span class="token punctuation">[</span>minIndex<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>          minIndex <span class="token operator">=</span> j<span class="token punctuation">;</span>        <span class="token punctuation">}</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// 交换最小值和当前位置</span>      <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>      nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>minIndex<span class="token punctuation">]</span><span class="token punctuation">;</span>      nums<span class="token punctuation">[</span>minIndex<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环找出当前数组最小值的下标，并放到数组开头；第二次循环找出除了数组第一个值的数组最小值，并交换到第二个位置；……直到数组最后两个值比较（并交换），此时数组按从小到大顺序排列。</p><p>选择排序的时间复杂度为O(n<sup>2</sup>)，空间复杂度为O(1)。</p><h4><span id="3">3. 直接插入排序</span></h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">InsertionSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 当前值与前面已排好序的值比较</span>      <span class="token keyword">int</span> j<span class="token punctuation">;</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span>j <span class="token operator">=</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> j <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> temp<span class="token punctuation">;</span> <span class="token operator">--</span>j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        nums<span class="token punctuation">[</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 如果当前值小于前面的值，则将前面的值向后移</span>      <span class="token punctuation">}</span>      nums<span class="token punctuation">[</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 直到当前值大于等于前面的某个值或者前面没有值的时候，将该值插入</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环，获取当前值temp=nums[1]，将temp与nums[0]比较，若小于nums[0]，将nums[0]的值往后移变为nums[1]，然后temp插入到nums[1]前即nums[0]的位置；第二次循环（前两个值已排好序），获取temp=nums[2]，先与nums[1]比较，若大于则不变，小于则将nums[1]的值往后移变为nums[2]，再比较temp与nums[0]并重复之前的过程；……最终数组以升序排序。</p><p>插入排序的空间复杂度为O(1)，时间复杂度则与原数组排列顺序有关，如果原数组已按升序排序，则该算法的时间复杂度为O(n)（最好的情况）；若按降序排列，则时间复杂度为O(n<sup>2</sup>)（最差的情况）。</p><h4 id="4-快速排序"><a href="#4-快速排序" class="headerlink" title="4. 快速排序"></a>4. 快速排序</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// start为待排序数组开始的下标，end为结束下标</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token function">quickSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">,</span> <span class="token keyword">int</span> start<span class="token punctuation">,</span> <span class="token keyword">int</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> pivot <span class="token operator">=</span> nums<span class="token punctuation">[</span>start<span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 取数组某个值(此处选第一个值)作为参照，比它小的放在它左边，大的放在右边</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> start<span class="token punctuation">;</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> end<span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> pivot<span class="token punctuation">)</span> <span class="token punctuation">{</span>        j<span class="token operator">--</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 从右往左，本就比参照大的数则不动</span>      <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> j <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;</span> pivot<span class="token punctuation">)</span> <span class="token punctuation">{</span>        i<span class="token operator">++</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 从左往右，本就比参照小的数不动</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">&amp;&amp;</span> i <span class="token operator">&lt;</span> j<span class="token punctuation">)</span> <span class="token punctuation">{</span>        i<span class="token operator">++</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 交换</span>        <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">></span> start<span class="token punctuation">)</span> nums <span class="token operator">=</span> <span class="token function">quickSort</span><span class="token punctuation">(</span>nums<span class="token punctuation">,</span> start<span class="token punctuation">,</span> i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">if</span> <span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">&lt;</span> start<span class="token punctuation">)</span> nums <span class="token operator">=</span> <span class="token function">quickSort</span><span class="token punctuation">(</span>nums<span class="token punctuation">,</span> j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> nums<span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一趟排序将数组分为两部分，一部分小于参照值，另一部分则大于参照值；再分别对两部分进行排序（递归）。当两部分都有序时，则整个数组都为有序状态。</p><p>快速排序的空间复杂度为O(1)，其时间复杂度与选取的参照有关，若每次选取的参照可使数组等分，则经过log<sub>2</sub>n躺划分，可完成排序，此时时间复杂度为O(nlog<sub>2</sub>n)；若每次选取的参照为最大值或最小值，则需要经过n躺划分，可完成排序，此时时间复杂度为O(n<sup>2</sup>)。</p><h4 id="5-希尔排序"><a href="#5-希尔排序" class="headerlink" title="5. 希尔排序"></a>5. 希尔排序</h4><p>希尔排序是插入排序的一种，又称“缩小增量排序”。</p><p>希尔排序就是将数组根据下标的一定增量分组，然后对每一组进行直接插入排序；随后增量减少，每次增量不同，都进行一次直接插入排序，直到增量为1，此时就是上面所提到的<a href="#3">直接插入排序</a>。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">shellSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> n <span class="token operator">=</span> nums<span class="token punctuation">.</span>length<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> step <span class="token operator">=</span> n <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> step <span class="token operator">>=</span> <span class="token number">1</span><span class="token punctuation">;</span> step <span class="token operator">/=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 步长</span>      <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> step<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 内循环两层为插入排序</span>        <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> j <span class="token operator">=</span> i <span class="token operator">-</span> step<span class="token punctuation">;</span>          <span class="token keyword">while</span> <span class="token punctuation">(</span>j <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> temp<span class="token punctuation">)</span> <span class="token punctuation">{</span>            nums<span class="token punctuation">[</span>j <span class="token operator">+</span> step<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>          j <span class="token operator">-=</span> step<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        nums<span class="token punctuation">[</span>j <span class="token operator">+</span> step<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>      <span class="token punctuation">}</span>       <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一次循环增量（步长）step为数组长度n的一半，此时根据增量将数组分为n/2组，分别为(nums[0], nums[step])，(nums[1], nums[1+step])，……(nums[step-1], nums[2*step-1])，然后对每组进行直接插入排序；第二次循环增量再减半，根据增量数组可分为多组，再对各组进行直接插入排序；……最后增量为1，即普通的直接插入排序。</p><p>希尔排序的空间复杂度为O(1)，但时间复杂度很难计算，查阅资料后可得希尔排序的平均时间复杂度为O(n<sup>3/2</sup>)（注意这里是<strong>平均</strong>)。</p><p>希尔排序较直接插入排序快是因为当增量大时，进行直接插入排序的元素少，速度快；当增量逐渐减少，此时数组已基本有序，此时直接插入排序对基本有序的序列排序效率很高。</p><p>这里有一个例子来自维基百科（个人认为维基百科将此过程通过列来表达更加清晰易懂）</p><p>此处待补充。。。。。。。。。。。。。。。。。。。。。。。</p><h4>6. 归并排序</h4><p>归并排序是建立在归并操作上的一种有效的、稳定的排序算法，该算法采用了分治法(Divide and Conquer)。</p><p>将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。</p><pre class="line-numbers language-java"><code class="language-java">Public <span class="token keyword">class</span> <span class="token class-name">Sort</span><span class="token punctuation">{</span>  <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">mergeSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arr<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> temp<span class="token punctuation">,</span> <span class="token keyword">int</span> start<span class="token punctuation">,</span> <span class="token keyword">int</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>start <span class="token operator">&lt;</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">int</span> mid <span class="token operator">=</span> <span class="token punctuation">(</span>start <span class="token operator">+</span> end<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>      <span class="token function">mergeSort</span><span class="token punctuation">(</span>arr<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> start<span class="token punctuation">,</span> mid<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token function">mergeSort</span><span class="token punctuation">(</span>arr<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> mid <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">merge</span><span class="token punctuation">(</span>arr<span class="token punctuation">,</span> temp<span class="token punctuation">,</span> start<span class="token punctuation">,</span> mid<span class="token punctuation">,</span> end<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">merge</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arr<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> temp<span class="token punctuation">,</span> <span class="token keyword">int</span> start<span class="token punctuation">,</span> <span class="token keyword">int</span> mid<span class="token punctuation">,</span> <span class="token keyword">int</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> i <span class="token operator">=</span> start<span class="token punctuation">;</span>    <span class="token keyword">int</span> j <span class="token operator">=</span> mid <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> mid <span class="token operator">&amp;&amp;</span> j <span class="token operator">&lt;=</span> mid<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token operator">++</span>i<span class="token punctuation">;</span>      <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>        temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token operator">++</span>j<span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token operator">++</span>k<span class="token punctuation">;</span>    <span class="token punctuation">}</span>        <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> mid<span class="token punctuation">)</span> <span class="token punctuation">{</span>      temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token operator">++</span>i<span class="token punctuation">;</span>      <span class="token operator">++</span>k<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>j <span class="token operator">&lt;=</span> end<span class="token punctuation">)</span> <span class="token punctuation">{</span>      temp<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> arr<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token operator">++</span>j<span class="token punctuation">;</span>      <span class="token operator">++</span>k<span class="token punctuation">;</span>    <span class="token punctuation">}</span>        System<span class="token punctuation">.</span><span class="token function">arraycopy</span><span class="token punctuation">(</span>temp<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> arr<span class="token punctuation">,</span> start <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 未完待续 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树四种遍历方式</title>
      <link href="2020/09/13/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/"/>
      <url>2020/09/13/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h4 id="1-二叉树定义-java"><a href="#1-二叉树定义-java" class="headerlink" title="1. 二叉树定义(java)"></a>1. 二叉树定义(java)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">// Definition for a binary tree node.</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TreeNode</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> val<span class="token punctuation">;</span>  TreeNode left<span class="token punctuation">;</span>  TreeNode right<span class="token punctuation">;</span>  <span class="token function">TreeNode</span><span class="token punctuation">(</span><span class="token keyword">int</span> x<span class="token punctuation">)</span> <span class="token punctuation">{</span> val <span class="token operator">=</span> x<span class="token punctuation">;</span> <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-前、中、后序遍历-递归版"><a href="#2-前、中、后序遍历-递归版" class="headerlink" title="2. 前、中、后序遍历(递归版)"></a>2. 前、中、后序遍历(递归版)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>  <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> <span class="token function">preorderTraversal</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">)</span> <span class="token punctuation">{</span>    List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> res <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">helper</span><span class="token punctuation">(</span>root<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> res<span class="token punctuation">;</span>  <span class="token punctuation">}</span>  <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">helper</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">,</span> List <span class="token operator">&lt;</span>Integer<span class="token operator">></span> res<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>root <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token comment" spellcheck="true">// res.add(root.val); // 前序遍历，先将root的值加入List中</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>root<span class="token punctuation">.</span>left <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">helper</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>left<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// res.add(root.val); // 中序遍历，先将左边节点的值加入List中，再加入root的值</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>root<span class="token punctuation">.</span>right <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>          <span class="token function">helper</span><span class="token punctuation">(</span>root<span class="token punctuation">.</span>right<span class="token punctuation">,</span> res<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token comment" spellcheck="true">// res.add(root.val); // 后序遍历，先将左右节点的值加入List中，最后加入root的值</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-前、中、后序遍历-迭代版"><a href="#3-前、中、后序遍历-迭代版" class="headerlink" title="3. 前、中、后序遍历(迭代版)"></a>3. 前、中、后序遍历(迭代版)</h4><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">//前序</span><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> <span class="token function">preorderTraversal</span><span class="token punctuation">(</span>TreeNode root<span class="token punctuation">)</span> <span class="token punctuation">{</span>      List<span class="token operator">&lt;</span>Integer<span class="token operator">></span> ans <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LinkedList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>root <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    LinkedList<span class="token operator">&lt;</span>TreeNode<span class="token operator">></span> stack <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">LinkedList</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 需要不断的增删，因此用LinkedList</span>    stack<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token operator">!</span>nodes<span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      TreeNode n <span class="token operator">=</span> stack<span class="token punctuation">.</span><span class="token function">pollLast</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//根据stack先进后出的原则，先从上到下获取所有左节点的值，再往右获取所有右节点的值</span>      ans<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>val<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>n<span class="token punctuation">.</span>right <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        stack<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>right<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>n<span class="token punctuation">.</span>left <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>        stack<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>left<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>        <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 中序</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="4-层次遍历"><a href="#4-层次遍历" class="headerlink" title="4. 层次遍历"></a>4. 层次遍历</h4>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 未完待续 </tag>
            
            <tag> Java </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法复杂度</title>
      <link href="2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"/>
      <url>2020/07/29/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h3 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1. 时间复杂度"></a>1. 时间复杂度</h3><h4 id="1-1-大O表示法："><a href="#1-1-大O表示法：" class="headerlink" title="1.1 大O表示法："></a>1.1 大O表示法：</h4><p>用O(n)来体现算法的时间复杂度。</p><p>大O表示法O(f(n))中f(n)可以是1、n<sup>2</sup>、logn等，接下来看看如何推倒大O阶。</p><h4 id="1-2-推导大O阶规则："><a href="#1-2-推导大O阶规则：" class="headerlink" title="1.2 推导大O阶规则："></a>1.2 推导大O阶规则：</h4><p>a. <strong>用1来代替运行时间中的所有加法常数</strong></p><p>b. <strong>f(n)若是多项式，只保留最高阶项即可</strong></p><p>c. <strong>去掉最高阶项系数</strong></p><h4 id="1-3"><a href="#1-3" class="headerlink" title="1.3"></a>1.3</h4><ul><li><p>常数阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">;</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"O"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">--</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//每一句都只执行一次</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码执行次数为4次，根据规则a，其时间复杂度为O(1)。</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    i<span class="token operator">++</span><span class="token punctuation">;</span><span class="token punctuation">}</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">Println</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码虽然循环了100次，但它的时间复杂度依然为O(1)，因为它的执行次数只是一个较大的常数.</p></li><li><p>线性阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>此段代码为for循环，其中<code>int i = 0</code>只执行一次，<code>i &lt; n</code>执行n次，<code>i++</code>执行n次，打印语句也执行了n次，因此这段代码执行次数为3n+1，根据规则，T(n) = O(n)。</p></li><li><p>对数阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//执行一次</span><span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>     i <span class="token operator">*=</span> <span class="token number">2</span><span class="token punctuation">;</span><span class="token punctuation">}</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//执行一次</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>中间循环体<code>i &lt; n</code>以及<code>i *= 2</code>都分别执行了log<sub>2</sub>n次，因为i每次翻倍增加，通过计算可知中间循环体循环了log<sub>2</sub>n次，一次这段代码执行次数为1+2log<sub>2</sub>n，根据规则，T(n) = O(logn)。</p></li><li><p>平方阶</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// for (int j = 0; j &lt; i + 1; j++)</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> j<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最内层循环体执行次数为xn<sup>2</sup>(x为某常数)，可得该段代码执行次数最高阶项为xn<sup>2</sup>，根据规则，T(n) = O(n<sup>2</sup>)。</p><p>同样可得立方阶，四次方阶……</p></li><li><p>碰到的较复杂的算法复杂度分析</p><pre class="line-numbers language-java"><code class="language-java"><span class="token comment" spellcheck="true">/** * Definition for singly-linked list. * public class ListNode { *     int val; *     ListNode next; *     ListNode(int x) { val = x; } * } */</span><span class="token comment" spellcheck="true">//合并K个有序链表</span><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> ListNode <span class="token function">mergeKLists</span><span class="token punctuation">(</span>ListNode<span class="token punctuation">[</span><span class="token punctuation">]</span> lists<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>lists<span class="token punctuation">.</span>length <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> null<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        ListNode ans <span class="token operator">=</span> lists<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> lists<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>lists<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token keyword">continue</span><span class="token punctuation">;</span>            ans <span class="token operator">=</span> <span class="token function">mergeTwoLists</span><span class="token punctuation">(</span>ans<span class="token punctuation">,</span> lists<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> ans<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> ListNode <span class="token function">mergeTwoLists</span><span class="token punctuation">(</span>ListNode l1<span class="token punctuation">,</span> ListNode l2<span class="token punctuation">)</span> <span class="token punctuation">{</span>        ListNode head <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ListNode</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        ListNode curr <span class="token operator">=</span> head<span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span>l1 <span class="token operator">!=</span> null <span class="token operator">&amp;&amp;</span> l2 <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>l1<span class="token punctuation">.</span>val <span class="token operator">&lt;=</span> l2<span class="token punctuation">.</span>val<span class="token punctuation">)</span> <span class="token punctuation">{</span>                curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l1<span class="token punctuation">;</span>                curr <span class="token operator">=</span> curr<span class="token punctuation">.</span>next<span class="token punctuation">;</span>                l1 <span class="token operator">=</span> l1<span class="token punctuation">.</span>next<span class="token punctuation">;</span>            <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>                curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l2<span class="token punctuation">;</span>                curr <span class="token operator">=</span> curr<span class="token punctuation">.</span>next<span class="token punctuation">;</span>                l2 <span class="token operator">=</span> l2<span class="token punctuation">.</span>next<span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>l1 <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l2<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>l2 <span class="token operator">==</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>            curr<span class="token punctuation">.</span>next <span class="token operator">=</span> l1<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> head<span class="token punctuation">.</span>next<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>首先分析<code>mergeTwoLists</code>函数的时间复杂度，假设每个链表的长度为n，可简单地得出该函数的T(o) = O(n + n) = O(n) (为简便，只计算循环体执行次数)。再分析<code>mergeKLists</code>函数的时间复杂度，同样的每个链表长度都为n，第一次合并前两个链表，此时<code>ans</code> 的长度都为2n，第x次合并后，长度变为xn，第x次的时间代价为O(n + (x - 1) $\times$ n) = O(x $\times$ n)，此时总的时间代价可用求和公式计算O($\sum_{i=1}^k(i\times n)$) = O($\frac{(1+k)\times k}{2}\times n$) = O(k<sup>2</sup>$\times $n)，所以时间复杂度为O(k<sup>2</sup> $\times$ n)。</p></li></ul><h3 id="2-空间复杂度"><a href="#2-空间复杂度" class="headerlink" title="2. 空间复杂度"></a>2. 空间复杂度</h3><h4 id="2-1-空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S-n-来定义。"><a href="#2-1-空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S-n-来定义。" class="headerlink" title="2.1 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S(n)来定义。"></a>2.1 空间复杂度是对一个算法在运行过程中临时占用存储空间大小的一个量度，与时间复杂度一样反应的是一个趋势，用S(n)来定义。</h4><h4 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h4><p>空间复杂度常用的有O(1), O(n), O(n<sup>2</sup>)</p><h4 id="2-3"><a href="#2-3" class="headerlink" title="2.3"></a>2.3</h4><ul><li><p>O(1)</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BubbleSort</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> nums<span class="token punctuation">)</span> <span class="token punctuation">{</span>  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> i<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>      <span class="token keyword">if</span> <span class="token punctuation">(</span>nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> temp <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        nums<span class="token punctuation">[</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>      <span class="token punctuation">}</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>经典的冒泡排序以及插入排序、选择排序等都是空间复杂度为O(1)的算法，因为它没有临时占用额外的存储空间，只在自己的数组里进行交换。</p></li><li><p>O(n)</p><pre class="line-numbers language-java"><code class="language-java"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">climbStairs</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span><span class="token punctuation">[</span><span class="token punctuation">]</span> dp <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">int</span><span class="token punctuation">[</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        dp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        dp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> dp<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        dp<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>n <span class="token operator">>=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> dp<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是leetcode上一道算法题，需要你计算爬到n阶且每次只能爬1或2阶有多少种不同的方法。很显然这是一道动态规划的题目，因此我们可以new一个数组来保存爬1～n阶各有多少种方法，而到n阶的方法就是到n-1阶和到n-2阶的方法和。这里我们new了一个长度为n+1的数组，用了额外的空间来存储我们需要的数据，因此空间复杂度为O(n+1) = O(n)。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自学笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 算法复杂度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo-theme-matery主题部分优化</title>
      <link href="2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/"/>
      <url>2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>hexo-theme-matery下载与配置: <a href="https://github.com/blinkfox/hexo-theme-matery">https://github.com/blinkfox/hexo-theme-matery</a></p><h3 id="1-关于个人信息的添加及修改："><a href="#1-关于个人信息的添加及修改：" class="headerlink" title="1. 关于个人信息的添加及修改："></a>1. 关于个人信息的添加及修改：</h3><p>以自己的个人信息作说明：</p><p>原：<img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/yuan.png" style="zoom:50%;"></p><p>现：</p><img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/PersonalInfo.png" style="zoom:50%;"><p>首先找到<code>/hexo-theme-matery/layout/about.ejs</code> ，并找到代码如下：</p><pre class="line-numbers language-js"><code class="language-js"><span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"author"</span><span class="token operator">></span> <span class="token comment" spellcheck="true">//作者信息</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"post-statis hide-on-large-only"</span> data<span class="token operator">-</span>aos<span class="token operator">=</span><span class="token string">"zoom-in-right"</span><span class="token operator">></span>        <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/post-statis'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//引用另一个文件，表示的是左边文章、分类、标签以及他们的数量</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"title"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> config<span class="token punctuation">.</span>author <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//作者名字，在blog/_config.yml中配置</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"career"</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> theme<span class="token punctuation">.</span>profile<span class="token punctuation">.</span>career <span class="token operator">%</span><span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//作者职业，在/hexo-theme-matery/layout/about.ejs中配置</span>    <span class="token operator">&lt;</span>div <span class="token keyword">class</span><span class="token operator">=</span><span class="token string">"social-link hide-on-large-only"</span> data<span class="token operator">-</span>aos<span class="token operator">=</span><span class="token string">"zoom-in-left"</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">%</span><span class="token operator">-</span> <span class="token function">partial</span><span class="token punctuation">(</span><span class="token string">'_partial/social-link'</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span> <span class="token comment" spellcheck="true">//右边github、email、QQ等的链接，也可自己减少或增加</span><span class="token operator">&lt;</span><span class="token operator">/</span>div<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因此想要修改或增加自己的信息，需要在相应的配置文件里进行修改或增加。</p><p>比如我自己做的修改，首先打开<code>hexo-theme-matery</code>下的<code>_config.yml</code>文件，并找到<code>profile</code>属性，</p><pre class="line-numbers language-yml"><code class="language-yml">profile:  avatar: /medias/头像.jpg  career: Undergraduate  school: University of Nottingham Ningbo China (UNNC)  major: Computer Science with Artificial Intelligence (4+0)  introduction: If you wish to succeed, you should use persistence as your good friend, experience as your reference, prudence as your brother and hope as your sentry.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在<code>profile</code>里修改了头像，职业，以及新增了学校和专业。</p><p>那么该如何将这些特性在网页里显示出来呢？我们需要在上面说的<code>about.ejs</code>这个文件里做相应的修改。</p><pre><code>&lt;div class=&quot;title&quot;&gt;&lt;%- config.author %&gt;&lt;/div&gt;&lt;div class=&quot;career&quot;&gt;&lt;%- theme.profile.career %&gt;&lt;/div&gt;&lt;div class=&quot;major&quot;&gt;&lt;%- theme.profile.major %&gt;&lt;/div&gt;&lt;div class=&quot;school&quot;&gt;&lt;%- theme.profile.school %&gt;&lt;/div&gt;</code></pre><p>只要加几个<code>div</code>块元素，就能显示你的其他信息。</p><h3 id="2-Post封面图片的修改"><a href="#2-Post封面图片的修改" class="headerlink" title="2. Post封面图片的修改"></a>2. Post封面图片的修改</h3><p>先找到<code>/hexo-theme/matery/layout/_partial/post_cover.ejs</code>中的代码</p><pre class="line-numbers language-js"><code class="language-js"><span class="token keyword">if</span> <span class="token punctuation">(</span>page<span class="token punctuation">.</span>img<span class="token punctuation">)</span> <span class="token punctuation">{</span>    featureimg <span class="token operator">=</span> <span class="token function">url_for</span><span class="token punctuation">(</span>page<span class="token punctuation">.</span>img<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 如果设置了img属性，那么封面图片就是你设置的图片</span><span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> hashCode <span class="token operator">=</span> <span class="token keyword">function</span> <span class="token punctuation">(</span>str<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>str <span class="token operator">&amp;&amp;</span> str<span class="token punctuation">.</span>length <span class="token operator">===</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">var</span> hash <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">var</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> len <span class="token operator">=</span> str<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> len<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            hash <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>hash <span class="token operator">&lt;</span><span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">-</span> hash<span class="token punctuation">)</span> <span class="token operator">+</span> str<span class="token punctuation">.</span><span class="token function">charCodeAt</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>            hash <span class="token operator">|</span><span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> hash<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> len <span class="token operator">=</span> theme<span class="token punctuation">.</span>featureImages<span class="token punctuation">.</span>length<span class="token punctuation">;</span>    <span class="token keyword">var</span> num <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">abs</span><span class="token punctuation">(</span><span class="token function">hashCode</span><span class="token punctuation">(</span>page<span class="token punctuation">.</span>title<span class="token punctuation">)</span> <span class="token operator">%</span> len<span class="token punctuation">)</span><span class="token punctuation">;</span>    featureimg <span class="token operator">=</span> theme<span class="token punctuation">.</span>jsDelivr<span class="token punctuation">.</span>url            <span class="token operator">?</span> theme<span class="token punctuation">.</span>jsDelivr<span class="token punctuation">.</span>url <span class="token operator">+</span> <span class="token function">url_for</span><span class="token punctuation">(</span>theme<span class="token punctuation">.</span>featureImages<span class="token punctuation">[</span>num<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token punctuation">:</span> <span class="token function">url_for</span><span class="token punctuation">(</span>theme<span class="token punctuation">.</span>featureImages<span class="token punctuation">[</span>num<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span> <span class="token comment" spellcheck="true">// 否则将会采用作者的算法，根据title的长度来计算hashcode，再根据hashcode从默认的图片中选出一张。如果两篇文章的title都是四个字，那这两篇的封面就是同一张图片。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>根据代码可知，想要一篇post用特定的图片，就必须设置这篇post的img属性。如下</p><img src="/2020/07/18/hexo-theme-matery%E4%B8%BB%E9%A2%98%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96/屏幕快照 2020-07-20 下午2.41.07.png" style="zoom:50%;"><p>可将图片移至<code>hexo-theme-matery/source/medias/featureimages</code>下，并将img属性设置为<code>/medias/featureimages/‘图片名’</code>即可。</p>]]></content>
      
      
      <categories>
          
          <category> 博客优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/07/16/hello-world/"/>
      <url>2020/07/16/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 初始内容 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
