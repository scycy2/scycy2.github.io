---
title: Formal Languages
date: 2021-05-15 13:50:30
tags: ['Language and Computation']
categories: ['class notes']
cover:
summary: Formal Languages (Automata, Regular Languages, Pumping Lemma, Context-Free Languages, Turing Machine)
img: /medias/featureimages/17.jpg
mathjax: true
---

#### 1. Basic Concepts

* **Alphabet**: a finite, nonempty set $\Sigma$ of symbols.
* **String**: a finite sequence of symbols from $\Sigma$
* substring, prefix and suffix

* The empty string has length 0 and is written as $\lambda$
  * It is a prefix, substring and suffix of any given string
  * Operations (suppose **v = aba** and **w = abaaa**):
    * Concatenation: **vw = abaabaaa**
    * Reverse: **w<sup>R</sup> = aaaba**
    * Repetition: **v<sup>2</sup> = abaaba** and **v<sup>0</sup> = $\lambda$**
* If $\Sigma$ is an alphabet, then we use $\Sigma^{*}$ to denote the set of strings obtained by concatenating zero or more symbols from $\Sigma$. The set $\Sigma^{*}$ always contains $\lambda$.

  * $\Sigma^{*}$ = the set of **all strings** formed by concatenating zero or more symbols in $\Sigma$
  * $\Sigma^{+}$ = the set of all **non-empty** strings formed by concatenating symbols in $\Sigma$
  * $\Sigma^{+} = \Sigma^{*} - \{\lambda\}$
* A **formal language L** is any subset of $\Sigma^{*}$

  * The **complement** of **L** is: $\overline{L} = \Sigma^{*} - L$
  * Reverse: $L^{R} = \{w^{R}, w\in L\}$
  * Concatenation: $L_{1}L_{2} = \{xy: x\in L_{1}, y \in L_{2}\}$
  * $L^{n}$ as $L$ concatenated with itself $n$ times
    * $L^{0} = \{\lambda\}$
    * $L^{1} = L$
  * **Star-closure**: $L^{*} = L^{0}\bigcup L^{1}\bigcup L^{2}\cdots$
  * **Positive-closure**: $L^{+} = L^{1}\bigcup L^2\cdots$
  * 空集和$\{\lambda\}$都是一种语言
* A grammar G is defined as a quadruple $G = (V, T, S, P)$ (the sets $V$ and $T$ are non-empty and disjoint):
  * $V$: a finite set of objects called **variables** or non-terminal symbols
  * $T$: a finite set of objects called **terminal symbols**
  * $S\in V$: a special symbol called the **start** symbol
  * $P$: a finite set of **productions**
  * Production rules are of the form: $x\rightarrow y$, where $x$ is an element of $(V\cup T)^+$ and $y$ is in $(V\cup T)^*$
* For a given grammar G, the language generated by G, L(G), is the set of all strings derived from the start  symbol.
  * $L(G) = \{w\in T^*: S \stackrel*\Rightarrow w\}$
  * If $w\in L(G)$, then the sequence $S\Rightarrow w_1 \Rightarrow w_2 \Rightarrow \cdots \Rightarrow w_n \Rightarrow w$ is a **derivation** of the sentence $w$. The strings $S, w_1, w_2, \cdots, w_n$, which contain variables as well as terminals, are called **sentential forms** of the derivation
* Automata
  * An automaton is an abstract model of a digital computer
  * An automaton consists of
    * An input mechanism
    * A control unit
    * Possibly, a storage mechanism
    * Possibly, an output mechanism

#### 2. Finite Automata

##### 2.1 Deterministic Finite Acceptors

* A **deterministic finite acceptor** or **dfa** is defined by the quintuple
  $$
  M = (Q, \Sigma, \delta, q_0, F)
  $$
  where

  ​		$Q$ is a finite set of **internal states**,

  ​		$\Sigma$ is a finite set of symbols called the **input alphabet**,

  ​		$\delta : Q\times \Sigma \rightarrow Q$ is a total function called the **transition function**,

  ​		$q_0 \in Q$ is the **initial state**,

  ​		$F\subseteq Q$ is a set of **finite states**.

* Extended Transition Function: $\delta^*$
  $$
  \delta^*: Q \times \Sigma^* \rightarrow Q
  $$
  For any given $q \in Q, w \in \Sigma^*, a \in \Sigma$:

  ​	$\delta^*(q,\lambda) = q$

  ​	$\delta^*(q, wa) = \delta(\delta^*(q, w), a)$

* The language accepted by a DFA $M = (Q, \Sigma, \delta, q_0, F)$ is the set of all strings on $\Sigma$ accepted by $M$, i.e. the set of all strings $w$ such that $\delta^*(q_0, w)$ results in a final state:
  $$
  L(M) = \{w \in \Sigma^* : \delta^*(q_0, w) \in F\}
  $$

* **Theorem 2.1**

  * Let $M = (Q, \Sigma, \delta, q_0, F)$ be a deterministic finite accepter, and let $G_M$ be its associated transition graph. Then for every $q_i, q_j \in Q$, and $w \in \Sigma^+$, $\delta^*(q_i, w) = q_j$ if and only if there is in $G_M$ a walk with label $w$ from $q_i$ to $q_j$

* Notes

  * Finite accepters are characterized by having no temporary storage
  * Since an input file cannot be rewritten, a finite automaton is severely limited in its capacity to "remember" things during the computation
  * A finite amount of information can be retained in the control unit by placing the unit into a specific state
  * But since the number of such states is finite, a finite automaton can only deal with situations in which the information to be stored at any time is strictly bounded

* Regular Languages

  * A language $L$ is **regular** if and only if there is a DFA that accepts $L$

##### 2.2 Nondeterministic Finite Accepter (NFA)

* A **nondeterministic finite accepter** or **nfa** is defined by the quintuple
  $$
  M = (Q, \Sigma, \delta, q_0, F)
  $$
  where $Q, \Sigma, q_0, F$ are defined as for deterministic finite accepter, but
  $$
  \delta : Q \times (\Sigma \cup \{\lambda\}) \rightarrow 2^Q
  $$
  where $2^Q$ is the set of all subsets of $Q$ (i.e., the power set of $Q$)

* The basic differences between deterministic and nondeterministic finite automata are:

  * In an NFA, a (state, symbol) combination may lead to several states <u>simultaneously</u>
  * An NFA may have <u>undefined transitions</u>
  * If a transition is labeled with the empty string as its input symbol, the NFA may change states <u>without consuming input</u>

* The language $L$ accepted by an nfa $M = (Q, \Sigma, \delta, q_0, F)$ is defined as the set of all strings accepted in the above sense. Formally,
  $$
  L(M) = \{w \in \Sigma^* : \delta^*(q_0, w) \cap F \neq \emptyset\}
  $$

##### 2.3 Equivalence of Deterministic and Nondeterministic Finite Accepters

* **Theorem 2.2**
  * Let $L$ be the language accepted by a nondeterministic finite accepter $M_N = (Q_N, \Sigma, \delta_N, q_0, F_N)$. Then there exists a deterministic finite accepter $M_D = (Q_D, \Sigma, \delta_N, \{q_0\}, F_D)$ such that $L = L(M_D)$
  * For <u>any</u> nondeterministic finite accepter, there is an equivalent deterministic finite accepter.
  * Therefore, *every language accepted by a nondeterministic finite accepter is also regular.*
* Procedure: NFA-to-DFA Conversion
  1. Beginning with the start state, define input transitions for the DFA as follows:
     * If the NFA input transition leads to a single state, replicate for the DFA.
     * If the NFA input transition leads to more than one state, create a new state in the DFA labeled $\{q_i, \cdots, q_j\}$, where $q_i, \cdots, q_j$ are all the states the NFA transition can lead to.
     * If the NFA input transition is not defined, the corresponding DFA transition should lead to a trap state.
  2. Repeat step 1 for all newly created DFA states, until no new states are created.
  3. Any DFA state containing an NFA final state in its label should be labeled as final.
  4. If the NFA accepts the empty string, label the start DFA state a final state.

#### 3. Regular Languages and Regular Grammars

##### 3.1 Regular Expressions

* Regular Expressions are defined recursively. For any alphabet $\Sigma$:
  1. Primitive regular expressions:
     * the empty set $\emptyset$
     * the empty string $\lambda$
     * any symbol of the alphabet $a \in \Sigma$
  2. If $r_1$ and $r_2$ are regular expressions, then so are:
     * the union $r_1 + r_2$
     * concatenation $r_1 \cdot r_2$ or $r_1r_2$
     * star closure $r_1^*$
  3. Any string resulting from a *finite* number of these operations on primitive regular expressions is also a regular expression.

* Languages Associated with Regular Expressions
  * A regular expressions $r$ denotes a language $L(r)$
  * Assuming that $r_1$ and $r_2$ are regular expressions:
    1. The regular expression $\emptyset$ denotes the empty set
    2. The regular expression $\lambda$ denotes the set $\{\lambda\}$
    3. For any $a$ in the alphabet $\Sigma$, the regular expression $a$ denotes the set $\{a\}$
    4. The regular expression $r_1 + r_2$ denotes $L(r_1) \cup L(r_2)$
    5. The regular expression $r_1 \cdot r_2$ denotes $L(r_1)L(r_2)$
    6. The regular expression $(r_1)$ denotes $L(r_1)$
    7. The regular expression $r_1^*$ denotes $(L(r_1))^*$

##### 3.2 Regular Expressions and Regular Languages

* **Theorem 3.1**

  * For any regular expression $r$, there is a nondeterministic finite automaton that accepts the language denoted by $r$.

* Since nondeterministic and deterministic accepters are equivalent, for any regular expression $r$, the language $L(r)$ is also regular.

* **Primitive regular expressions**

  * We can construct simple automata that accept the languages associated with:
    * the empty set
    * the empty string
    * any individual symbol $a \in \Sigma$

  <img src="Formal-Languages/Screen Shot 2021-03-23 at 11.56.19 AM.png" style="zoom:50%;" />

  * <img src="Formal-Languages/Screen Shot 2021-03-23 at 12.03.27 PM.png" style="zoom:50%;" />

  * <img src="Formal-Languages/Screen Shot 2021-03-23 at 12.04.22 PM.png" style="zoom:50%;" />

  * <img src="Formal-Languages/Screen Shot 2021-03-23 at 12.04.52 PM.png" style="zoom:50%;" />

  * <img src="Formal-Languages/Screen Shot 2021-03-23 at 12.05.25 PM.png" style="zoom:50%;" />

* Regular Expressions for Regular Languages

  * **Theorem 3.2**
    * For every regular language $L$, it is possible to construct a corresponding regular expression $r$ satisfying $L = L(r)$

##### 3.3 Regular grammars

* A grammar $G = (V, T, S, P)$ is said to be **right-linear** if all productions are of the form
  $$
  A \rightarrow xB,\\
  A \rightarrow x,
  $$
  where $A, B \in V$, and $x \in T^*$. A grammar is said to be **left-linear** if all productions are of the form
  $$
  A \rightarrow Bx,\\
  A \rightarrow x.
  $$

* In a *<u>right-linear grammar</u>*, at most one variable symbol appears on the right side of any production. If it occurs, it is the ***rightmost*** symbol.

* In a *<u>left-linear grammar</u>*, at most one variable symbol appears on the right side of any production. If it occurs, it is the ***leftmost*** symbol.

* A **regular grammar** is one that is either right-linear or left-linear.

* Right-Linear grammars Generate Regular Languages

  * **Theorem 3.3**
    * For any given right-linear grammar $G$, the language $L(G)$ is regular
  * The algorithm for constructing an NFA to accept the language generated by a given right-linear grammar $G$:
    * Label the NFA start state with $S$ and a final state $V_f$
    * For every variable symbol $V_i$ in $G$, create an NFA state and label it $V_i$
    * For each production of the form $A \rightarrow aB$, label a transition from state $A$ to $B$ with symbol $a$
    * For each production of the form $A \rightarrow a$, label a transition from state $A$ to $V_f$ with symbol a:
      * You need to add intermediate states for productions with more than one terminal on the right hand side.

* Right-Linear grammars for Regular Languages

  * **Theorem 3.4**
    * If $L$ is a regular language on the alphabet $\Sigma$, then there exists a right-linear grammar $G = (V, \Sigma, S, P)$ such that $L = L(G)$
  * There is an algorithm that, given any DFA $M$ accepting a regular language $L$, constructs a right-linear grammar $G$ which generates the same language:
    * Each state in the DFA corresponds to a variable symbol in $G$
    * For each DFA transition from state $A$ to state $B$ labeled with symbol $a$, there is a production of the form $A \rightarrow aB$ in $G$
    * For each final state $F_i $ in the DFA, there is a corresponding production $F_i \rightarrow \lambda$ in $G$

* Equivalence of Regular Languages and Regular grammars

  * <img src="Formal-Languages/Screen Shot 2021-03-27 at 12.46.52 PM.png" style="zoom:50%;" />

#### 4. Properties of Regular Languages

* Closure Properties
  * **Theorem 4.1**
    * If $L_1$ and $L_2$ are regular languages, so are the languages that result from the following operations:
      * $L_1 \cup L_2$
      * $L_1 \cap L_2$
      * $L_1L_2$
      * $\overline{L_1}$
      * $L_1^*$
    * In other words, the family of regular language is **closed** under union, intersection, concatenation, complementation, and star-closure.

* Closure under Reversal
  * **Theorem 4.2**
    * If $L$ is a regular language, so is $L^R$

* A Membership Algorithm for Regular Languages

  * We say that a regular language is given in a **standard representation** if and only if it is described by one of the following:
    * a finite automaton,
    * a regular expression,
    * or a regular grammar
  * **Theorem 4.5** confirms the existence of a **membership** algorithm for reguklar languages
  * To determine if an arbitrary string $w$ is in a regular language $L$, we assume we are given a standard representation of $L$m which we then convert to a DFA that accepts $L$
  * Simulate the operation of the DFA while processing $w$ as the input string
  * If the machine halts in a final state after processing $w$, then $w\in L$, otherwise, $w\notin L$.

* Determining whether a regular language is empty, finite or infinite

  * **Theorem 4.6** confirms the existence of an ***algorithm*** to determine if a regular language is ***empty, finite, or infinite***
  * Given the ***transition graph*** of a DFA that accepts $L$,
    * If there is a simple path from the start state to any final state, $L$ is not empty (since it contains, at least, the corresponding string)
    * If a path form the start state to a final state includes a vertex which is the base of some cycle, $L$ is infinite (otherwise, $L$ is finite)

* Determining whether two regular languages are equal

  * **Theorem 4.7** confirms the existence of an algorithm to determine if two regular languages $L_1$ and $L_2$ are equal:

    * Define the language $L = (L_1 \cap \overline{L_2}) \cup (\overline{L_1} \cap L_2)$
    * By closure properties, $L$ is regular
    * So we can construct a DFA $M$ to accept it, and by Theorem 4.6, we can determine whether $L$ is emoty or not.

    * $L_1$ and $L_2$ are equal if and only if $L$ is empty

* Identifying Non-regular Languages

  * **Pigeonhole Principle:** If we put $n$ objects into $m$ boxes (pigeonholes), and if $n > m$, then at least one box must have more than one item in it.
  * This simple principle is the basis of most of the methods for proving non-regularity of languages.

  * **Basic observation:** Although regular languages can be *infinite*, their associated automata have *finite* memory.

* The Pumping Lemma
  * Suppose that $M = (Q, \Sigma, \delta, q_0, F)$ is a DFA with $n$ states that accepts a language $L$:
    * If it accepts a string $x$ such that $|x| \ge n$, then by the time $n$ symbols have been read, $M$ must have entered some state more than once;
    * In other words, there must be two different prefixes $u$ and $uv$ such that $\delta^*(q_0, u) = \delta^*(q_0, uv)$
    * <img src="Formal-Languages/Screen Shot 2021-05-03 at 4.16.49 PM.png" style="zoom:50%;" />
  * This implies that there are many more strings in $L$, because we can traverse the loop $v$ any number of times (including leaving it out altogether).
  * In other words, all of the string $uv^iw$ for $i \ge 0$ are in $L$
  * This fact is known as the ***Pumping Lemma for Regular Languages***.
  * **Theorem (*pumping property*):** Suppose that $L$ is a language over $\Sigma$. If $L$ is accepted by the DFA $M = (Q, \Sigma, \delta, q_0, F)$, then there is an integer $n$ so that for every $x$ in $L$ satisfying $|x| \ge n$, there are three strings $u, v$, and $w$ such that $x = uvw$ and:
    * $|uv| \le n$;
    * $v > 0 (i.e. v \ne \lambda) $
    * For every $i \ge 0$, the string $uv^iw$ belongs to $L$
  * The way we found $n$ was to take the number of states in an FA accepting $L$.
  * In many applications we don't need to know this, only that there is such an $n$.
  * The statement of the Pumping lemma is as follows:
    * If $L$ is regular $\Rightarrow$ the pumping property holds
  * In practice, we use the contrapositive:
    * If the pumping property does not hold $\Rightarrow$ the language $L$ is not regular
  * Thus, the most common application of the pumping lemma is to show that a language is not regular
  * The proof is by contradiction. We suppose that the language can be accepted by an FA, and we let $n$ be the integer in the pumping lemma
  * Then we choose a string $x$ with $|x| \ge n$ to which we can apply the lemma so as to get a contradiction.
  * Note that the Pumping lemma only provides a ***necessary*** condition, but not a sufficient one.
    * If $L$ is regular $\Rightarrow$ the pumping property holds
  * In other words, it does not say anything about the pumping property when L is not regular
  * In fact, ***there are languages that do satisfy the pumping lemma, but are not regular***, e.g.:
    * $L = \{a^ib^jc^j\mid i\ge 1\ and\ j \ge 0\}\cup\{b^jc^k\mid j\ge 0\ and\ k\ge 0\}$

#### 5. Context Free Languages

##### 5.1 Context-Free Grammars

* Definition:

  * We call $G = (V, \Sigma, S, P)$ a ***context-free grammar* (CFG)** if all productions in $P$ have the form:

    * $A\rightarrow x$

    in which $A\in V$, and $x\in (V\cup\Sigma)^*$.

  * We say that $L$ is a ***context-free language* (CFG)** if and only if there is a context-free grammar $G$ such that $L = L(G)$

    * In other words, a language is context-free if it is generated by a context-free grammar.

* No Restriction on the Right Side

  * A context-free grammar has *no restrictions* on the right side of its productions.
  * Therefore, the class of CFLs includes the class of regular languages as a proper subset.

* Restriction on the Left Side

  * Note that, a context-free grammar does impose a restriction on the *left side* of productions:
    * **the left side must be a single variable**

##### 5.2 Leftmost and Rightmost Derivations

* In a *leftmost derivation* (LMD), at each step, the leftmost variable in a sentential form is replaced.
* In a *rightmost derivation* (RMD), at each step, the rightmost variable in a sentential form is replaced.

##### 5.3 Derivation Trees

* In a *derivation tree* or *parse tree*,
  * the root is labeled $S$
  * internal nodes are labeled with a variable occurring on the left side of a production
  * the children of a node contain the symbols on the corresponding right side of a production
* In a full derivation tree, the root node represents the start variable $S$.
* Any interior node and its children represent a production $A\rightarrow x$ used in the derivation;
  * the node represents $A$, and the children, from left to right, represent the symbols in $x$.
* Each leaf node represents a symbol or $\lambda$.
* The string derived is read off from left to right, ignoring $\lambda'$s.
  * The ***yield*** of a derivation tree is the string of terminals produced by a leftmost depth-first traversal of the tree.

##### 5.4 Sentential Forms and Derivation Trees

* **Theorem 5.1** states that:
  * Given a context-free grammar $G$, for every string $w\in L(G)$, there exists a derivation tree whose yield is $w$.
  * The converse is also true: the yield of any derivation tree formed with productions from $G$ is in $L(G)$
  * **Note:** Derivation trees show which productions are used in obtaining a sentence, but *do not give the order of their application*.

##### 5.5 Parsing and Membership

* **The *parsing* problem**: given a grammar $G$ and a string $w$, find a sequence of derivations using the productions in $G$ to produce $w$
* To ensure that the problem may ne solved (regardless of efficiency) we need to require grammars to be given in a suitable shape
* Exhausive parsing is guaranteed to yield all strings eventually, but many fail to stop for strings not in $L(G)$, unless we restrict the productions in the grammar.
* Questions about the strings generated by a CFg are sometimes easier to answer if the productions have a retricted form:
  * For example, if we know that a grammar has:
    * no $\lambda$-productions ($A\rightarrow \lambda$),
    * and no unit productions ($A\rightarrow B$),
  * we can deduce that no derivation of a string $x$ can take more than $2\vert x\vert - 1$.
  * We could then, in principle, determine whether $x$ can be derived by considering derivations no longer than $2\vert x\vert - 1$.
* There exists an algorithm which, for every CFG $G = (V, \Sigma, S, P)$, produces a CFG $G_1 = (V, \Sigma, S, P_1)$ which has no $\lambda$-productions, no unit productions of the form $A\rightarrow B$, and for which:
  * $L(G_1) = L(G)-\{\lambda\}$
* With this conversion, the parsing problem may be solved in an exhaustive, top-down, but not very efficient fashion:
* **<u>Theorem 5.2</u>**: Exhaustive parsing is guaranteed to yield all strings eventually, but may fail to stop for strings not in $L(G)$, *unless if the grammar has no $\lambda$-productions or unit productions*.

##### 5.6 Parsing and Ambiguity

* If $G$ is a CFG, then for any $x\in L(G)$ these three statements are equivalent:
  * $x$ has more than one derivation tree.
  * $x$ has more than one LMD.
  * $x$ has more than one RMD.
* Thus, a CFG $G$ is ***ambiguous*** if and only if, for at least one $x\in L(G)$, $x$ has more than one LMD (or RMD).

##### 5.7 Derivation Trees and Ambiguity

* A classic example of ambiguity is **the dangling *else***.

* In C, an if-statement can be defined by:

  * $S\rightarrow if\ (E)\ S\ \mid\ if\ (E)\ S\ else\ S\ \mid\ OS$  (Where $OS$ stands for "other statement").

* Consider the statement

  $if\ (e1)\ if\ (e2)\ f();\ else\ g();$

  * In C, the *else* should belong to the second $if$, but this grammar does not rule out interpreting the statement with $else$ belonging to the first $if$.

* Clearly the grammar given is ambiguous, *but there are equivalent grammars that allow only the correct interpretation*. For example:

  $S\rightarrow S_1\ \mid\ S_2$

  $S_1\rightarrow \ if\ (E)\ S_1\ else\ S_1\ \mid\ OS$

  $S_2\rightarrow\ if\ (E)\ S\ \mid\ if\ (E)\ S_1\ else\ S_2$

* These rules generate the same strings as the original ones and are unambiguous.

##### 5.8 Ambiguous Languages

* For some languages, it is always possible to find an unambiguous grammar.
* However, there are ***inherently ambiguous*** languages, for which every possible grammar is ambiguous.

##### 5.9 Pumping lemma for CFGs

* For every CFL $L$ there exists a natural number $n$ such that for any string $u\in L$ with $|u|\ge n$, there are $v, w, x, y, z$ such that $u = vwxyz$ and:
  1. $|wxy|\le n$
  2. $|wy|\gt 0$
  3. $\forall i\in N : vw^ixy^iz\in L$

#### 6. Pushdown Automata

##### 6.1 Introduction

* CFLs do not have anything similar to regular expressions.
* But they do have a machine model, i.e., pushdown automata.

* A pushdown automaton is essentially a finite automaton with a ***stack*** added as storage.
* As there is no limit on the size of the stack, pushdown automata do not have bounded memory limitation of finite automata.
* Pushdown automata are equivalent to CFGs, as long as we allow them to be nondeterministic.
* The language family associated with deterministic pushdown automata is a proper subset of the context-free languages.
* There are CFLs (such as $A^nB^n$) that cannot be recognized by finite automata
* This is because the memory of a finite automata is restricted to a finite set of states, whereas the recognition of a CFL may require storing an unbounded amount of information.
* Consider the following languages:
  * $A^nB^m = \{a^nb^m\mid n, m \ge 0\}$, which is regular;
  * $A^nB^n = \{a^nb^n\mid n\ge 0\}$, which is not regular.
* To recognize strings in $A^nB^m$, all we need to do is to check that $a$'s appear before $b$'s, a task which can be done by a DFA.
* In constrast, for $A^nB^n$, we must not only check that all $a$'s precede the first $b$, we must also ***count*** the number of $a$'s.
* Since $n$ is unbounded, this counting cannot be done with a finite memory.
* As the example of $A^nB^n$ shows, for the machine model of CFLs, we need a machine that can ***count without limit***.
* But, that is not enough. For instance, consider $WW^R = \{ww^R\mid w\in \{a, b\}^*$, which is a CFL.
* $WW^R$ shows that we need more than unlimited counting ability:
  * We need the ability to store and match a sequence of symbols in ***reverse*** order.
* This suggests that we might try a ***stack*** as a storage mechanism, allowing unbounded storage that is restricted to operating like a stack.
* This gives us a class of machines called pushdown automata (PDA), which we use as a model of computation to process context-free languages.

##### 6.2 Nondeterministic Pushdown Automata

* Each move of the control unit:

  * reads a symbol from the input file;
  * changes the contents of the stack (*through the usual stack operations*).

* Each move of the control unit is determined by:

  * the current input symbol
  * and the symbol currently on top of the stack.

* The result of the move is a new state of the control unit and a change in the top of the stack.

* A *nondeterministic pushdown accepter* (NPDA) $M = (Q, \Sigma, \Gamma, \delta, q_0, z, F)$ is defined by:

  * $Q$: the finite set of internal states of the control unit

  * $\Sigma$: the finite set of input alphabet

  * $\Gamma$: the finite set of stack alphabet

  * $\delta$: the transition function with the type signature:
    $$
    \delta: Q\times (\Sigma \cup \{\lambda\})\times \Gamma\rightarrow P_f(Q\times \Gamma^*)
    $$
    $P_f(Q\times \Gamma^*)$ is the set of *finite* subsets of $(Q\times \Gamma^*)$

  * $q_0\in Q$: the initial state of the control unit

  * $z\in \Gamma$: the stack start symbol

  * $F\subseteq Q$: the set of final states

  * The arguments of $\delta$ are:

    * the current state of the control unit
    * the current input symbol
    * and the current symbol on *top* of the stack

  * The result is a finite set of pairs $(q, x)$, where:

    * $q$ is the next state of the control unit
    * and $x$ is a ***string*** that is put on top of the stack ***in place of the single symbol*** there before.

  * $\lambda$-transition: When the second argument of $\delta$ is $\lambda$, i.e., a move that does not consume an input symbol.

  * $\delta$ Always needs a stack symbol; i.e., no move is possible if the stack is empty.

  * Finally, the requirement that the elements of the range of $\delta$ be a ***finite*** subset is necessary because:

    * $Q\times \Gamma^*$ is an infinite set and therefore has infinite subsets.
    * While an NPDA may have several choices for its moves, *this choice must be restricted to a finite set of possibilities*.

  * **Note**: If a particular transition is not defined, the corresponding (state, symbol, stack top) configuration represents a *dead* state.

* Transition Graphs

  * Label the edges of the graph with three things:
    * the current input symbol
    * the symbol at the top of the stack
    * and the string that replaces the top of the stack

* Instantaneous Descriptions

  * While transition graphs are convenient for *describing* NPDAs, they are not so suitable for formal reasoning.

  * To trace the operation of an NPDA, we must keep track of:

    * the current state of the control unit,
    * the unread part of the input string,
    * and the stack contents.

  * **<u>Instantaneous Description</u>**: The triplet $(q, w, u)$ in which:

    * $q$ is the state of the control unit,
    * $w$ is the unread part of the input string,
    * and $u$ is the stack contents:
      * with the leftmost symbol indicating the top of the stack

    is called an instantaneous description of a pushdown automaton.

  * A move from one instantaneous description to another will be denoted by the symbol $\vdash$

  * Thus $(q_1, aw, bx)\vdash (q_2, w, yx)$ is possible if and only if:
    $$
    (q_2, y)\in \delta (q_1, a, b)
    $$

  * Moves involving an arbitrary number of steps will be denoted by $\vdash^*$.

##### 6.3 The Language Accepted by an NPDA

* The language accepted by an NPDA is the set of all strings that cause the NPDA to halt in a final state, after starting in $q_0$ with only the stack symbol $z$ on the stack.

* *The final contents of the stack are irrelevant*.

* As was the case with nondeterministic finite automata, the string is accepted if *at least one* of the computations cause the NPDA to halt in a final state.

* In other words, it does not require all the possible traces to end up in a final state.

* **<u>Definition 7.2</u>**: Let $M = (Q, \Sigma, \Gamma, \delta, q_0, z, F)$ be an NPDA. The language accepted by $M$ is the set:
  $$
  L(M) = \{w\in \Sigma^*\mid (q_0, w, z) \vdash^* (p, \lambda, u), p\in F, u \in \Gamma^*\}
  $$

##### 6.4 NPDAs and CFLs

* A context-free grammar is in *Greibach Normal Form* if, in all of its productions, the right side consists of a single terminal follows by any number of variables

* **<u>Definition 6.5</u>**: A context-free grammar $G = (V, T, S, P)$ is said to be in ***Greibach normal form*** if all productions have the form
  $$
  A\rightarrow ax
  $$
  where $a\in T$ and $x\in V^*$.

* **<u>Theorem 6.7</u>**: For every context-free grammar $G$ there exists a grammar $\hat{G}$ in Greibach normal form satisfying:
  $$
  L(\hat{G}) = L(G) - \{\lambda\}
  $$

##### 6.5 NPDAs for CFLs

* **<u>Theorem 7.1</u>** For any given $\lambda$-free context-free language $L$, there exists an NPDA $M$ such that
  $$
  L(M) = L
  $$

* The constructive proof of Theorem 7.1 provides an algorithm that can be used to build the corresponding NPDA, for any language specified by a grammar $G$ in Greibach normal form.

* The resulting NPDA simulates grammar derivations by:

  * keeping variables on the stack
  * while making sure that the input symbol matches the terminal on the right side of the production.

##### 6.6 Construction of an NPDA from a Grammar in Greibach Normal Form

* The NPDA $M = (\{q_0, q_1, q_f\}, T, V\cup\{z\}, \delta, q_0,z \{q_f\})$ has:
  * three states $Q = \{q_0, q_1, q_f\}$, with $q_0$ as the initial state, and $q_f$ as the only final state
  * input alphabet equal to the grammar terminal symbols $T$,
  * and stack alphabet equal to the grammar variable $V$, plus $z$, with the assumption that $z\notin V$
* The transition function contains the following:
  * A rule that pushes $S$ on the stack and switches control to $q_1$ without consuming input:
    * $\delta (q_0, \lambda, z) = \{(q_1, Sz)\}$
  * For every production of the form $A\rightarrow au$, we have
    * $(q_1, u)\in \delta (q_1, a, A)$
  * A rule that switches the control unit to the final state $q_f$ when there is no more input, and the stack is empty:
    * $\delta(q_1, \lambda, z) = \{(q_f, z)\}$
  * If $\lambda \in L$, we add the transition $\delta(q_0, \lambda, z) = \{(q_f, z)\}$

##### 6.7 CFGs for PDAs

* **<u>Theorem 7.2</u>**: If $L = L(M)$ for some NPDA $M$, then there exists a context-free grammar $G$ such that $L(G) = L(M)$. In other words, $L$ is a context-free language.
* Proof: The basic idea behind the proof is to reverse the process in the proof of Theorem 7.1
* The aim is to construct a grammar that simulates the moves of the NPDA $M$.
* In particular:
  * the content of the stack should be reflected in the variable part of sentential forms in derivations,
  * while the processed input is the terminal prefix of the sentential forms.
* Theorem 7.1 and 7.2 show that the following are equivalent, in the sense that they generate the same class of languages, i.e., context-free languages:
  * Context-free grammars
  * NPDAs
* Which specification one chooses depends on the purpose:
  * For specifying programming language constructs, grammars are more appropriate, as they are easier to understand by human beings.
  * For conputational purposes (e.g., compilation of a program) the machine model, i.e., NPDA, is more appropriate.

##### 6.8 Deterministic Pushdown Automata (DPDA)

A *deterministic pushdown accepter* (DPDA) never has more than one choice in its moves.

* For every $q\in Q, a\in \Sigma \cup \{\lambda\}$ and $b\in \Gamma$:
  * $\delta (q, a, b)$ contains at most one element;
  * If $\delta(q, \lambda, b)$ is not empty, then $\delta(q, c, b)$ must be empty for every input symbol $c\in \Sigma$.
    * when a $\lambda$-move is possible for some configuration, no input-consuming alternative is available.
* Unlike the case for finite automata, a $\lambda$-transition does not necessarily mean the automaton is nondeterministic:
  * Since the top of the stack plays a role in determining the next move, the presence of $\lambda$-transitions does not automatically imply nondeterminism.
* Also, some transitions of a DPDA may be to the empty set, that is, undefined, so there may be dead configurations.
* This does not affect the definition either:
  * The only criterion for determinism is that, at all times, *at most* one possible move exists.

##### 6.9 Deterministic Context-Free Languages (DCFLs)

* A context-free language $L$ is *deterministic* if there is a DPDA $M$ such that:
  $$
  L = L(M)
  $$

* There are, however, languages which are context-free, but not DCFL.

  * This shows that, ***deterministic and nondeterministic pushdown automata are not equivalent.***

##### 6.10 Importance of DCFLs

* The importance of deterministic context-free languages lies in the fact that they can be ***parsed efficiently***.
* Think of the pushdown automaton as a parsing device:
  * Since there is no backtracking involved, we can easily write a computer program for it, and we may expect that it will work efficiently.
  * Since there may be $\lambda$-transitions involved, we cannot immediately claim that this will yield a linear-time parser, but it puts us on the right track, nevertheless.

#### 7. Turing Machines

##### 7.1 The Standard Turing Machine

* A standard Turing machine has unlimited storage in the form of a tape:
  * The tape consists of an infinite number of cells,
  * with each cell capable of storing one symbol.
* The read-write head can travel in ***both*** directions on the tape, processing one symbol per move.
* In a standard Turing machine, the tape acts as the:
  * input,
  * output,
  * and storage medium.
* A control function causes the machine to change states and possibly overwrite the tape contents:
  * Deterministic and non-deterministic Turing machines turn out to be as powerful as each other.
* The input string is surrounded by blanks, so the input alphabet is considered a proper subset of the tape alphabet.

##### 7.2 Definition of a Turing Machine

* **<u>Definition 9.1</u>**: A *Turing Machine* $M = (Q, \Sigma, \Gamma, \delta, q_0, \Box, F)$ is defined by:
  * $Q$: a finite set of internal states
  * $\Sigma$: the input alphabet
  * $\Gamma$: the tape alphabet
  * $\delta$: $Q\times \Gamma\rightarrow Q\times \Gamma \times \{L, R\}$: the transition function
  * $\Box \in \Gamma$: a special symbol called the blank
  * $q_0 \in Q$: the initial state
  * $F \subseteq Q$: the set of final states
* In the definition of a Turing machine, we assume that: $\Sigma \subseteq \Gamma - \{\Box\}$.
* In other words, the input alphabet is a subset of the tape alphabet, and the blank symbol is not in the input alphabet.
* Transition Function $\delta$
  * $\delta: Q\times \Gamma \rightarrow Q \times \Gamma \times \{L, R\}$
  * Input to the transition function $\delta$ consists of:
    * the current state of the control unit
    * and the current tape symbol
  * Output of $\delta$ consists of:
    * a new state,
    * a new tape symbol,
    * and location of the next symbol to be read (left or right)
  * In general, $\delta$ is a partial function, so that some (state, symbol) input combinations may be undefined.

##### 7.3 Transition Graphs for Turing Machines

* Label the edges of the graph with three items:
  * the current tape symbol,
  * the symbol that replaces it,
  * and the direction in which the read-write head is to move.

##### 7.4 A Turing machine that Never Halts

* It is possible for a Turing machine to never halt on certain inputs.
* The machine runs forever with the read-write head moving alternately right and left but making no modifications to the tape.
* In fact, it should be clear that this particular machine will run forever, *regardless of the initial information on its tape*, with the read-write head moving alternately right then left but making no modifications to the tape.
* In the analogy with programming terminology, we say that the Turing machine is in an ***infinite loop***.

##### 7.5 Instantaneous Description

* The most convenient way to exhibit a sequence of configurations of a Turing machine uses the idea of an *instantaneous description*.

* Any configuration is completely determined by:

  * the current state of the control unit,
  * the contents of the tape,
  * and the position of the read-write head.

* Use the notation in which $x_1qx_2$ or $a_1a_2\cdots a_{k-1}qa_ka_{k+1}\cdots a_n$ is the instantaneous description of a machine in state $q$ with the tape depicted in the figure below.

  <img src="Formal-Languages/Screen Shot 2021-05-06 at 5.25.49 PM.png" style="zoom:50%;" />

* This convention is chosen so that the position of the read-write head is over the cell containing the symbol immediately following $q$.

* The instantaneous description gives only a finite amount of information to the right and left of the read-write head.

* The unspecified part of the tape is assumed to contain all blanks.

* Normally, such blanks are irrelevant and are not shown explicitly in the instantaneous description.

* If the position of blanks is relevant to the discussion, however, the blank symbol may appear in the instantaneous description.

##### 7.6 The Language Accepted by a Turing Machine

* **<u>Definition 9.3</u>**: Let $M = (Q, \Sigma, \Gamma, \delta, q_0, \Box, F)$ be a Turing machine. Then the language accepted by $M$ is:
  $$
  L(M) = \{w\in \Sigma^+\mid q_0w\vdash^* x_1q_fx_2,\ for\ some\ q_f\in F, x_1, x_2\in \Gamma^*\}
  $$

* The language accepted by a Turing machine is the set of all strings which cause the machine to halt in a final state, when started in its standard initial configuration.

* A string is rejected if

  * The machine halts in a nonfinal state, or
  * ***The machine never halts***

* $M$ is said to halt starting from some initial configuration $x_1q_ix_2$ if
  $$
  x_1q_ix_2\vdash^* y_1q_jay_2
  $$
  for any $q_j$ and $a$, for which $\delta(q_j, a)$ is undefined.

* Turing machines are more powerful than the previous classes of automata that we have studied (i.e., FAs, and PDAs).

##### 7.7 Turing Machines as Transducers

* Turing machines provide an abstract model for digital computers, acting as a transducer that transforms input into output.

* A *Turing machine transducer* implements a function that treats the original contents of the tape as its input and the final contents of the tape as its output.

* A function is *Turing-computable* if it can be carried out by a Turing machine capable of processing all values in the function domain.

* **<u>Definition 9.4</u>**: A function $f$ with domain $D$ is said to be Turing-computable if there exists some Turing machine $M = (Q, \Sigma, \Gamma, \delta, q_0, \Box, F)$ such that, for all $w \in D$:
  $$
  q_0w\vdash^*q_ff(w)
  $$
  for some final state $q_f\in F$, ***and for any input $w\notin D$, the machine $M$ does not halt in a final state.***

* Turing machines are the most powerful model of computation as transducers as well.

* In fact, all the common mathematical functions are Turing-computable, e.g.:

  * Arithmetic operators, Exponentiation, Integer logarithm;
  * Comparison;
  * String manipulation;
  * Etc.

##### 7.8 Combining Turing Machines

* By combining Turing Machines that perform simple tasks, complex algorithms can be implemented.

##### 7.9 Universal Turing Machines

* The TMs we have studied so far have been special-purpose computers capable of executing a single algorithm.
* We can consider a "universal" Turing machine, which can execute a program stored in its memory:
  * It receives an input string that specifies both:
    * the algorithm it is to execute
    * and the input that is to be provided to the algorithm

##### 7.10 The Church-Turing Thesis

* To say that the TM is a general model of computation implies that:
  * *"any algorithmic procedure that can be carried out at all, by a human computer or a team of humans or an electronic computer, can be carried out by a TM"*.
* An acceptance of the Church-Turing Thesis leads to a ***definition of an algorithm***:
  * An *algorithm* for a function $f: D\rightarrow R$ is a Turing machine $M$, which given any $d\in D$ on its tape, eventually halts with the correct answer $f(d)\in R$ on its tape.
* The nature of the model makes it seems that a TM can execute any algorithm a human can.
* Apparent enhancements to the TM have been shown not to increase its power.
* Other models of computation proposed have either been less powerful or equivalent to Turing machines.
* No one has ever suggested any kind of ***effective*** computation that cannot be implemented on a TM
* From now on, we will consider that, by definition, ***an "algorithmic procedure" is what a Turing machine can do***.

##### 7.11 The Chomsky Hierarchy

* <img src="Formal-Languages/Screen Shot 2021-05-14 at 12.41.00 PM.png" style="zoom:50%;" />

* <img src="Formal-Languages/Screen Shot 2021-05-14 at 12.41.10 PM.png" style="zoom:50%;" />

#### 8. Easy wrong topic

* Is every formal language regular? No. $L = {a^nb^n : n \ge 0}$ is not regular.

* $L = \emptyset$ is regular language.

* $G = (V, T, S, P)$, $S \rightarrow aSb \mid S$, $L(G)$ is regular, because the production cannot halt, the language is empty.

* Every finite language is regular. Therefore, every non-regular language is infinite.
* Context Free contains regular languages, regular languages contain finite language
